{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c334db8",
   "metadata": {},
   "source": [
    "Source:\n",
    "    https://pytorch.org/blog/int8-quantization/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e3c5af",
   "metadata": {},
   "source": [
    " By reducing the precision of the modelâ€™s weights and activations from 32-bit floating-point (FP32) to 8-bit integer (INT8), INT8 quantization can significantly improve the inference speed and reduce memory requirements without sacrificing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff07ac31",
   "metadata": {},
   "source": [
    "Step by step work:\n",
    " 1. Create a simple model \n",
    "    2. Use any random dataset as the input\n",
    "    3. Train the model\n",
    "    4. Convert the model into tflite\n",
    "    5. Create a representative dataset \n",
    "    6. Perform INt8 quantization\n",
    "    7. Save the quantizated model\n",
    "    8. Run the inference\n",
    "    \n",
    "Question to answer:\n",
    "1. How does INT8 quatnization is working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "722a47b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 09:54:23.367087: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e68cbfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model=models.Sequential([\n",
    "        layers.Input(shape=(10,)),\n",
    "        layers.Dense(5,activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1cc5d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random input data and labels\n",
    "np.random.seed(0)\n",
    "num_samples = 1000\n",
    "input_data = np.random.randint(2001, size=(num_samples, 10)) \n",
    "labels = np.random.randint(2, size=(num_samples, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc62bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile model\n",
    "model = create_model()\n",
    "model.compile(optimizer='adam',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9570bb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "32/32 [==============================] - 1s 2ms/step - loss: 24.8520 - accuracy: 0.4980\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 12.2576 - accuracy: 0.4960\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7.5347 - accuracy: 0.5030\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 5.2731 - accuracy: 0.5060\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 3.9009 - accuracy: 0.5180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff8d4743610>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(input_data, labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eac54232",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/sft.cern.ch/lcg/views/LCG_105/x86_64-el9-gcc13-opt/lib/python3.9/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save('random_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d63d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('random_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a66f5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 5)                 55        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61 (244.00 Byte)\n",
      "Trainable params: 61 (244.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f36cd042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/sraj/tmpxg_5bung/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/sraj/tmpxg_5bung/assets\n",
      "2024-02-08 09:54:45.952555: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2024-02-08 09:54:45.952599: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2024-02-08 09:54:45.953046: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/sraj/tmpxg_5bung\n",
      "2024-02-08 09:54:45.953980: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2024-02-08 09:54:45.954004: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/sraj/tmpxg_5bung\n",
      "2024-02-08 09:54:45.956897: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2024-02-08 09:54:45.957793: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2024-02-08 09:54:45.999000: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/sraj/tmpxg_5bung\n",
      "2024-02-08 09:54:46.011391: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 58338 microseconds.\n",
      "2024-02-08 09:54:46.035643: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4cbc8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a representative dataset\n",
    "representative_data = input_data[:100]  # Use a subset of the input data as the representative dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d07e3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a generator function to provide representative data\n",
    "def representative_dataset_generator():\n",
    "    for data in representative_data:\n",
    "        yield [data.reshape(1, -1).astype(np.float32)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab2091b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "converter.representative_dataset = representative_dataset_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14e449bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the quantized model to a file\n",
    "with open('random_model_quantized.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3da166e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b0f235e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/step - loss: 3.3887 - accuracy: 0.5050\n"
     ]
    }
   ],
   "source": [
    "# Load the unquantized model\n",
    "unquantized_model = tf.keras.models.load_model('random_model.h5')\n",
    "unquantized_loss, unquantized_accuracy = unquantized_model.evaluate(input_data, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc19ed87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Load the quantized model\n",
    "interpreter = tf.lite.Interpreter(model_path='random_model_quantized.tflite')\n",
    "interpreter.allocate_tensors()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2f56c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare he input and output tensors\n",
    "input_index = interpreter.get_input_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e008b613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'serving_default_input_1:0',\n",
       "  'index': 0,\n",
       "  'shape': array([ 1, 10], dtype=int32),\n",
       "  'shape_signature': array([-1, 10], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f65669a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input data with the correct shape\n",
    "input_data = input_data.astype(np.float32)\n",
    "input_data = np.expand_dims(input_data, axis=0)  # Add a batch dimension\n",
    "output_index = interpreter.get_output_details()[0]['index']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fff7b91f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SetTensor(): incompatible function arguments. The following argument types are supported:\n    1. (self: tensorflow.lite.python.interpreter_wrapper._pywrap_tensorflow_interpreter_wrapper.InterpreterWrapper, i: int, value: handle, subgraph_index: int = 0) -> object\n\nInvoked with: <tensorflow.lite.python.interpreter_wrapper._pywrap_tensorflow_interpreter_wrapper.InterpreterWrapper object at 0x7ff8cc629270>, [{'name': 'serving_default_input_1:0', 'index': 0, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([-1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}], array([[ 684.,  559., 1653., ..., 1383., 1033., 1747.],\n       [ 277., 1778., 1828., ..., 1420.,  314.,  705.],\n       [1510.,  551.,   87., ...,  537.,  845.,   72.],\n       ...,\n       [ 481., 1163., 1169., ...,  364., 1060., 1288.],\n       [ 303., 1797.,  542., ...,  514.,  212.,  856.],\n       [1516.,  909.,  702., ..., 1104., 1299., 1700.]], dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/sraj/ipykernel_1604879/514509199.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105/x86_64-el9-gcc13-opt/lib/python3.9/site-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36mset_tensor\u001b[0;34m(self, tensor_index, value)\u001b[0m\n\u001b[1;32m    718\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minterpreter\u001b[0m \u001b[0mcould\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mset\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m     \"\"\"\n\u001b[0;32m--> 720\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mresize_tensor_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: SetTensor(): incompatible function arguments. The following argument types are supported:\n    1. (self: tensorflow.lite.python.interpreter_wrapper._pywrap_tensorflow_interpreter_wrapper.InterpreterWrapper, i: int, value: handle, subgraph_index: int = 0) -> object\n\nInvoked with: <tensorflow.lite.python.interpreter_wrapper._pywrap_tensorflow_interpreter_wrapper.InterpreterWrapper object at 0x7ff8cc629270>, [{'name': 'serving_default_input_1:0', 'index': 0, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([-1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}], array([[ 684.,  559., 1653., ..., 1383., 1033., 1747.],\n       [ 277., 1778., 1828., ..., 1420.,  314.,  705.],\n       [1510.,  551.,   87., ...,  537.,  845.,   72.],\n       ...,\n       [ 481., 1163., 1169., ...,  364., 1060., 1288.],\n       [ 303., 1797.,  542., ...,  514.,  212.,  856.],\n       [1516.,  909.,  702., ..., 1104., 1299., 1700.]], dtype=float32)"
     ]
    }
   ],
   "source": [
    "# Run inference on the quantized model and compute accuracy\n",
    "num_correct = 0\n",
    "for input_data, label in zip(input_data, labels):\n",
    "    input_data = input_data.astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, input_data)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_index)\n",
    "    predicted_label = np.argmax(output)\n",
    "    if predicted_label == label:\n",
    "        num_correct += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d4db70",
   "metadata": {},
   "source": [
    "![image](https://pytorch.org/assets/images/int8/pytorch_quant_x86_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416d3a4a",
   "metadata": {},
   "source": [
    "![image_s](https://pytorch.org/assets/images/int8/pytorch_quant_x86_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad7034f",
   "metadata": {},
   "source": [
    "https://pytorch.org/blog/int8-quantization/\n",
    "https://www.intel.com/content/www/us/en/developer/articles/technical/int8-quantization-for-x86-cpu-in-pytorch.html#:~:text=INT8%20Quantization%20for%20x86%20CPU%20in%20PyTorch*,-Overview&text=By%20reducing%20the%20precision%20of,memory%20requirements%20without%20sacrificing%20accuracy.\n",
    "https://www.tensorflow.org/lite/performance/post_training_integer_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8194b12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/sft.cern.ch/lcg/views/LCG_105/x86_64-el9-gcc13-opt/lib/python3.9/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(M, self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.conv = torch.nn.Conv2d(1, 1, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "model_fp32 = M()\n",
    "\n",
    "model_fp32.eval()\n",
    "\n",
    "\n",
    "model_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "\n",
    "model_fp32_fused = torch.quantization.fuse_modules(model_fp32, [['conv', 'relu']])\n",
    "\n",
    "\n",
    "model_fp32_prepared = torch.quantization.prepare(model_fp32_fused)\n",
    "\n",
    "# calibrate the prepared model to determine quantization parameters for activations\n",
    "# in a real world setting, the calibration would be done with a representative dataset\n",
    "input_fp32 = torch.randn(4, 1, 4, 4)\n",
    "model_fp32_prepared(input_fp32)\n",
    "\n",
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, and replaces key operators with quantized\n",
    "# implementations.\n",
    "model_int8 = torch.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "res = model_int8(input_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6e3e437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (quant): QuantStub()\n",
       "  (conv): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "baf61745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (quant): QuantStub()\n",
       "  (conv): ConvReLU2d(\n",
       "    (0): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (relu): Identity()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19aaf7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): HistogramObserver(min_val=-2.759754180908203, max_val=3.021393299102783)\n",
       "  )\n",
       "  (conv): ConvReLU2d(\n",
       "    (0): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (activation_post_process): HistogramObserver(min_val=0.19379568099975586, max_val=1.7738090753555298)\n",
       "  )\n",
       "  (relu): Identity()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fp32_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bddb429c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.6188, -1.3656,  0.1770,  0.1060],\n",
       "          [-0.1552, -0.3424, -0.6614, -0.1574],\n",
       "          [-0.0652, -0.6068,  0.8175, -0.8872],\n",
       "          [ 0.0636, -0.8041, -1.1435,  0.0300]]],\n",
       "\n",
       "\n",
       "        [[[ 1.7816, -0.3857, -2.7598,  1.2740],\n",
       "          [ 0.2323, -0.2197, -0.3202, -1.2060],\n",
       "          [ 0.4900,  0.3746, -0.8379, -0.3279],\n",
       "          [-0.5364, -1.5201,  3.0214, -0.8101]]],\n",
       "\n",
       "\n",
       "        [[[-1.4131, -1.5440, -0.8876,  0.9838],\n",
       "          [ 2.3838,  0.8122, -0.7225, -0.2188],\n",
       "          [-1.1641,  0.1184, -1.5926,  0.6182],\n",
       "          [-0.7598,  0.1797, -0.8831,  0.1931]]],\n",
       "\n",
       "\n",
       "        [[[-0.3019, -1.3952,  0.5295, -0.2410],\n",
       "          [-0.9327,  2.2946, -0.8485, -0.3899],\n",
       "          [ 0.0132,  1.2915, -0.4745,  0.4994],\n",
       "          [ 0.5789, -0.1150,  0.7701,  1.1317]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa573511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M(\n",
       "  (quant): Quantize(scale=tensor([0.0455]), zero_point=tensor([61]), dtype=torch.quint8)\n",
       "  (conv): QuantizedConvReLU2d(1, 1, kernel_size=(1, 1), stride=(1, 1), scale=0.013960925862193108, zero_point=0)\n",
       "  (relu): Identity()\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0dec3f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.1169, 0.5724, 0.9912, 0.9773],\n",
       "          [0.9075, 0.8516, 0.7679, 0.9075],\n",
       "          [0.9354, 0.7818, 1.1727, 0.7120],\n",
       "          [0.9633, 0.7260, 0.6422, 0.9633]]],\n",
       "\n",
       "\n",
       "        [[[1.4380, 0.8516, 0.1955, 1.2984],\n",
       "          [1.0052, 0.8795, 0.8656, 0.6143],\n",
       "          [1.0890, 1.0471, 0.7260, 0.8656],\n",
       "          [0.7958, 0.5445, 1.7591, 0.7260]]],\n",
       "\n",
       "\n",
       "        [[[0.5584, 0.5305, 0.6980, 1.2146],\n",
       "          [1.5915, 1.1727, 0.7539, 0.8795],\n",
       "          [0.6282, 0.9912, 0.5166, 1.1169],\n",
       "          [0.7399, 0.9912, 0.7120, 0.9912]]],\n",
       "\n",
       "\n",
       "        [[[0.8656, 0.5584, 1.1029, 0.8795],\n",
       "          [0.6980, 1.5636, 0.7120, 0.8377],\n",
       "          [0.9493, 1.2984, 0.8237, 1.0890],\n",
       "          [1.1029, 0.9075, 1.1588, 1.2565]]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
