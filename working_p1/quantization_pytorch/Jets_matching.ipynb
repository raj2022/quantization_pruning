{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65fd9818",
   "metadata": {},
   "source": [
    "Task:\n",
    "* correct jet matching, 1000 events, response distributions, simple code \n",
    "\n",
    "Issues:\n",
    "* Check the code\n",
    "* Check the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "373371fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.30/02\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import ROOT\n",
    "\n",
    "import vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d7820e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastjet\n",
    "import awkward as ak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bc4625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import tensorflow_datasets as tfds\n",
    "import torch_geometric\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27812db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a44b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "data_dir = \"../../mlpf/tensorflow_datasets/\"\n",
    "dataset = \"clic_edm_ttbar_pf\"\n",
    "\n",
    "# Load dataset\n",
    "builder = tfds.builder(dataset, data_dir=data_dir)\n",
    "ds_train = builder.as_data_source(split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6bb5f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_FEATURES_TRK = [\n",
    "    \"elemtype\",\n",
    "    \"pt\",\n",
    "    \"eta\",\n",
    "    \"sin_phi\",\n",
    "    \"cos_phi\",\n",
    "    \"p\",\n",
    "    \"chi2\",\n",
    "    \"ndf\",\n",
    "    \"dEdx\",\n",
    "    \"dEdxError\",\n",
    "    \"radiusOfInnermostHit\",\n",
    "    \"tanLambda\",\n",
    "    \"D0\",\n",
    "    \"omega\",\n",
    "    \"Z0\",\n",
    "    \"time\",\n",
    "]\n",
    "X_FEATURES_CL = [\n",
    "    \"elemtype\",\n",
    "    \"et\",\n",
    "    \"eta\",\n",
    "    \"sin_phi\",\n",
    "    \"cos_phi\",\n",
    "    \"energy\",\n",
    "    \"position.x\",\n",
    "    \"position.y\",\n",
    "    \"position.z\",\n",
    "    \"iTheta\",\n",
    "    \"energy_ecal\",\n",
    "    \"energy_hcal\",\n",
    "    \"energy_other\",\n",
    "    \"num_hits\",\n",
    "    \"sigma_x\",\n",
    "    \"sigma_y\",\n",
    "    \"sigma_z\",\n",
    "]\n",
    "Y_FEATURES = [\"cls_id\", \"charge\", \"pt\", \"eta\", \"sin_phi\", \"cos_phi\", \"energy\"]\n",
    "Y_CLASSES = [0, 211, 130, 22, 11, 13]\n",
    "\n",
    "INPUT_DIM = max(len(X_FEATURES_TRK), len(X_FEATURES_CL))\n",
    "NUM_CLASSES = len(Y_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2f04b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JP 2024-02-29: currently torch int8 onnx export does not work with MultiheadAttention because of the following:\n",
    "# - it uses q_scaling_product.mul_scalar which is not supported in ONNX opset 17: the fix is to just remove the q_scaling_product\n",
    "# - somehow, the \"need_weights\" option confuses the ONNX exporter because the multiheaded attention layer then returns a tuple: the fix is to make the MHA always return just the attended values only\n",
    "# I lifted these two modules directly from the pytorch code and made the modifications here.\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as nnF\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class QuantizeableMultiheadAttention(nn.MultiheadAttention):\n",
    "    _FLOAT_MODULE = nn.MultiheadAttention\n",
    "\n",
    "    r\"\"\"Quantizable implementation of the MultiheadAttention.\n",
    "\n",
    "    Note::\n",
    "        Please, refer to :class:`~torch.nn.MultiheadAttention` for more\n",
    "        information\n",
    "\n",
    "    Allows the model to jointly attend to information from different\n",
    "    representation subspaces.\n",
    "    See reference: Attention Is All You Need\n",
    "\n",
    "    The original MHA module is not quantizable.\n",
    "    This reimplements it by explicitly instantiating the linear layers.\n",
    "\n",
    "    .. math::\n",
    "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
    "        \\text{where} head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "\n",
    "    Args:\n",
    "        embed_dim: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n",
    "        bias: add bias as module parameter. Default: True.\n",
    "        add_bias_kv: add bias to the key and value sequences at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        kdim: total number of features in key. Default: None.\n",
    "        vdim: total number of features in value. Default: None.\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
    "\n",
    "    Note that if :attr:`kdim` and :attr:`vdim` are None, they will be set\n",
    "    to :attr:`embed_dim` such that query, key, and value have the same\n",
    "    number of features.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> import torch.ao.nn.quantizable as nnqa\n",
    "        >>> multihead_attn = nnqa.MultiheadAttention(embed_dim, num_heads)\n",
    "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "\n",
    "    Note::\n",
    "        Please, follow the quantization flow to convert the quantizable MHA.\n",
    "    \"\"\"\n",
    "    __constants__ = ['batch_first']\n",
    "\n",
    "    def __init__(self, embed_dim: int, num_heads: int,\n",
    "                 dropout: float = 0., bias: bool = True,\n",
    "                 add_bias_kv: bool = False, add_zero_attn: bool = False,\n",
    "                 kdim: Optional[int] = None, vdim: Optional[int] = None, batch_first: bool = False,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, num_heads, dropout,\n",
    "                         bias, add_bias_kv,\n",
    "                         add_zero_attn, kdim, vdim, batch_first,\n",
    "                         **factory_kwargs)\n",
    "        self.linear_Q = nn.Linear(self.embed_dim, self.embed_dim, bias=bias, **factory_kwargs)\n",
    "        self.linear_K = nn.Linear(self.kdim, self.embed_dim, bias=bias, **factory_kwargs)\n",
    "        self.linear_V = nn.Linear(self.vdim, self.embed_dim, bias=bias, **factory_kwargs)\n",
    "        # for the type: ignore, see https://github.com/pytorch/pytorch/issues/58969\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=bias, **factory_kwargs)  # type: ignore[assignment]\n",
    "\n",
    "        # Functionals\n",
    "        # self.q_scaling_product = torch.ao.nn.quantized.FloatFunctional()\n",
    "        # note: importing torch.ao.nn.quantized at top creates a circular import\n",
    "\n",
    "        # Quant/Dequant\n",
    "        self.quant_attn_output = torch.ao.quantization.QuantStub()\n",
    "        self.quant_attn_output_weights = torch.ao.quantization.QuantStub()\n",
    "        self.dequant_q = torch.ao.quantization.DeQuantStub()\n",
    "        self.dequant_k = torch.ao.quantization.DeQuantStub()\n",
    "        self.dequant_v = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def _get_name(self):\n",
    "        return 'QuantizableMultiheadAttention'\n",
    "\n",
    "    @classmethod\n",
    "    def from_float(cls, other):\n",
    "        assert type(other) == cls._FLOAT_MODULE\n",
    "        assert hasattr(other, 'qconfig'), \"The float module must have 'qconfig'\"\n",
    "        # Setting the dropout to 0.0!\n",
    "        observed = cls(other.embed_dim, other.num_heads, other.dropout,\n",
    "                       (other.in_proj_bias is not None),\n",
    "                       (other.bias_k is not None),\n",
    "                       other.add_zero_attn, other.kdim, other.vdim,\n",
    "                       other.batch_first)\n",
    "        observed.bias_k = other.bias_k\n",
    "        observed.bias_v = other.bias_v\n",
    "        observed.qconfig = other.qconfig\n",
    "\n",
    "        # Set the linear weights\n",
    "        # for the type: ignores, see https://github.com/pytorch/pytorch/issues/58969\n",
    "        observed.out_proj.weight = other.out_proj.weight  # type: ignore[has-type]\n",
    "        observed.out_proj.bias = other.out_proj.bias  # type: ignore[has-type]\n",
    "        if other._qkv_same_embed_dim:\n",
    "            # Use separate params\n",
    "            bias = other.in_proj_bias\n",
    "            _start = 0\n",
    "            _end = _start + other.embed_dim\n",
    "            weight = other.in_proj_weight[_start:_end, :]\n",
    "            if bias is not None:\n",
    "                bias = torch.nn.Parameter(bias[_start:_end], bias.requires_grad)\n",
    "            observed.linear_Q.weight = torch.nn.Parameter(weight,\n",
    "                                                          weight.requires_grad)\n",
    "            observed.linear_Q.bias = bias\n",
    "\n",
    "            bias = other.in_proj_bias\n",
    "            _start = _end\n",
    "            _end = _start + other.embed_dim\n",
    "            weight = other.in_proj_weight[_start:_end, :]\n",
    "            if bias is not None:\n",
    "                bias = torch.nn.Parameter(bias[_start:_end], bias.requires_grad)\n",
    "            observed.linear_K.weight = torch.nn.Parameter(weight,\n",
    "                                                          weight.requires_grad)\n",
    "            observed.linear_K.bias = bias\n",
    "\n",
    "            bias = other.in_proj_bias\n",
    "            _start = _end\n",
    "            weight = other.in_proj_weight[_start:, :]\n",
    "            if bias is not None:\n",
    "                bias = torch.nn.Parameter(bias[_start:], bias.requires_grad)\n",
    "            observed.linear_V.weight = torch.nn.Parameter(weight,\n",
    "                                                          weight.requires_grad)\n",
    "            observed.linear_V.bias = bias\n",
    "        else:\n",
    "            observed.linear_Q.weight = nn.Parameter(other.q_proj_weight)\n",
    "            observed.linear_K.weight = nn.Parameter(other.k_proj_weight)\n",
    "            observed.linear_V.weight = nn.Parameter(other.v_proj_weight)\n",
    "            if other.in_proj_bias is None:\n",
    "                observed.linear_Q.bias = None  # type: ignore[assignment]\n",
    "                observed.linear_K.bias = None  # type: ignore[assignment]\n",
    "                observed.linear_V.bias = None  # type: ignore[assignment]\n",
    "            else:\n",
    "                observed.linear_Q.bias = nn.Parameter(other.in_proj_bias[0:other.embed_dim])\n",
    "                observed.linear_K.bias = nn.Parameter(other.in_proj_bias[other.embed_dim:(other.embed_dim * 2)])\n",
    "                observed.linear_V.bias = nn.Parameter(other.in_proj_bias[(other.embed_dim * 2):])\n",
    "        observed.eval()\n",
    "        # Explicit prepare\n",
    "        observed = torch.ao.quantization.prepare(observed, inplace=True)\n",
    "        return observed\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def dequantize(self):\n",
    "        r\"\"\"Utility to convert the quantized MHA back to float.\n",
    "\n",
    "        The motivation for this is that it is not trivial to conver the weights\n",
    "        from the format that is used in the quantized version back to the\n",
    "        float.\n",
    "        \"\"\"\n",
    "        fp = self._FLOAT_MODULE(self.embed_dim, self.num_heads, self.dropout,\n",
    "                                (self.linear_Q._weight_bias()[1] is not None),\n",
    "                                (self.bias_k is not None),\n",
    "                                self.add_zero_attn, self.kdim, self.vdim, self.batch_first)\n",
    "        assert fp._qkv_same_embed_dim == self._qkv_same_embed_dim\n",
    "        if self.bias_k is not None:\n",
    "            fp.bias_k = nn.Parameter(self.bias_k.dequantize())\n",
    "        if self.bias_v is not None:\n",
    "            fp.bias_v = nn.Parameter(self.bias_v.dequantize())\n",
    "\n",
    "        # Set the linear weights\n",
    "        # Note: Because the linear layers are quantized, mypy does not nkow how\n",
    "        # to deal with them -- might need to ignore the typing checks.\n",
    "        # for the type: ignore[has-type], see https://github.com/pytorch/pytorch/issues/58969\n",
    "        w, b = self.out_proj._weight_bias()  # type: ignore[operator, has-type]\n",
    "        fp.out_proj.weight = nn.Parameter(w.dequantize())\n",
    "        if b is not None:\n",
    "            fp.out_proj.bias = nn.Parameter(b)\n",
    "\n",
    "        wQ, bQ = self.linear_Q._weight_bias()  # type: ignore[operator]\n",
    "        wQ = wQ.dequantize()\n",
    "        wK, bK = self.linear_K._weight_bias()  # type: ignore[operator]\n",
    "        wK = wK.dequantize()\n",
    "        wV, bV = self.linear_V._weight_bias()  # type: ignore[operator]\n",
    "        wV = wV.dequantize()\n",
    "        if fp._qkv_same_embed_dim:\n",
    "            # Use separate params\n",
    "            _start = 0\n",
    "            _end = _start + fp.embed_dim\n",
    "            fp.in_proj_weight[_start:_end, :] = wQ\n",
    "            if fp.in_proj_bias is not None:\n",
    "                assert all(bQ == 0)\n",
    "                fp.in_proj_bias[_start:_end] = bQ\n",
    "\n",
    "            _start = _end\n",
    "            _end = _start + fp.embed_dim\n",
    "            fp.in_proj_weight[_start:_end, :] = wK\n",
    "            if fp.in_proj_bias is not None:\n",
    "                assert all(bK == 0)\n",
    "                fp.in_proj_bias[_start:_end] = bK\n",
    "\n",
    "            _start = _end\n",
    "            fp.in_proj_weight[_start:, :] = wV\n",
    "            if fp.in_proj_bias is not None:\n",
    "                assert all(bV == 0)\n",
    "                fp.in_proj_bias[_start:] = bV\n",
    "        else:\n",
    "            fp.q_proj_weight = nn.Parameter(wQ)\n",
    "            fp.k_proj_weight = nn.Parameter(wK)\n",
    "            fp.v_proj_weight = nn.Parameter(wV)\n",
    "            if fp.in_proj_bias is None:\n",
    "                self.linear_Q.bias = None\n",
    "                self.linear_K.bias = None\n",
    "                self.linear_V.bias = None\n",
    "            else:\n",
    "                fp.in_proj_bias[0:fp.embed_dim] = bQ\n",
    "                fp.in_proj_bias[fp.embed_dim:(fp.embed_dim * 2)] = bK\n",
    "                fp.in_proj_bias[(fp.embed_dim * 2):] = bV\n",
    "\n",
    "        return fp\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_observed(cls, other):\n",
    "        # The whole flow is float -> observed -> quantized\n",
    "        # This class does float -> observed only\n",
    "        # See nn.quantized.MultiheadAttention\n",
    "        raise NotImplementedError(\"It looks like you are trying to prepare an \"\n",
    "                                  \"MHA module. Please, see \"\n",
    "                                  \"the examples on quantizable MHAs.\")\n",
    "\n",
    "    def forward(self,\n",
    "                query: Tensor,\n",
    "                key: Tensor,\n",
    "                value: Tensor,\n",
    "                key_padding_mask: Optional[Tensor] = None,\n",
    "                need_weights: bool = True,\n",
    "                attn_mask: Optional[Tensor] = None,\n",
    "                average_attn_weights: bool = True,\n",
    "                is_causal: bool = False) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        r\"\"\"\n",
    "    Note::\n",
    "        Please, refer to :func:`~torch.nn.MultiheadAttention.forward` for more\n",
    "        information\n",
    "\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. When given a binary mask and a value is True,\n",
    "            the corresponding value on the attention layer will be ignored.\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
    "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
    "\n",
    "    Shape:\n",
    "        - Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n",
    "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
    "          If a BoolTensor is provided, the positions with the\n",
    "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked\n",
    "          positions. If a BoolTensor is provided, positions with ``True``\n",
    "          is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "          is provided, it will be added to the attention weight.\n",
    "        - is_causal: If specified, applies a causal mask as attention mask. Mutually exclusive with providing attn_mask.\n",
    "          Default: ``False``.\n",
    "        - average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across\n",
    "          heads. Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an\n",
    "          effect when ``need_weights=True.``. Default: True (i.e. average weights across heads)\n",
    "\n",
    "        - Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n",
    "        - attn_output_weights: If ``average_attn_weights=True``, returns attention weights averaged\n",
    "          across heads of shape :math:`(N, L, S)`, where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. If ``average_attn_weights=False``, returns attention weights per\n",
    "          head of shape :math:`(N, num_heads, L, S)`.\n",
    "        \"\"\"\n",
    "        return self._forward_impl(query, key, value, key_padding_mask,\n",
    "                                  need_weights, attn_mask, average_attn_weights,\n",
    "                                  is_causal)\n",
    "\n",
    "    def _forward_impl(self,\n",
    "                      query: Tensor,\n",
    "                      key: Tensor,\n",
    "                      value: Tensor,\n",
    "                      key_padding_mask: Optional[Tensor] = None,\n",
    "                      need_weights: bool = True,\n",
    "                      attn_mask: Optional[Tensor] = None,\n",
    "                      average_attn_weights: bool = True,\n",
    "                      is_causal: bool = False) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        # This version will not deal with the static key/value pairs.\n",
    "        # Keeping it here for future changes.\n",
    "        #\n",
    "        # TODO: This method has some duplicate lines with the\n",
    "        # `torch.nn.functional.multi_head_attention`. Will need to refactor.\n",
    "        static_k = None\n",
    "        static_v = None\n",
    "\n",
    "        if attn_mask is not None and is_causal:\n",
    "            raise AssertionError(\"Only allow causal mask or attn_mask\")\n",
    "\n",
    "        if is_causal:\n",
    "            raise AssertionError(\"causal mask not supported by AO MHA module\")\n",
    "\n",
    "        if self.batch_first:\n",
    "            query, key, value = (x.transpose(0, 1) for x in (query, key, value))\n",
    "\n",
    "        tgt_len, bsz, embed_dim_to_check = query.size()\n",
    "        assert self.embed_dim == embed_dim_to_check\n",
    "        # allow MHA to have different sizes for the feature dimension\n",
    "        assert key.size(0) == value.size(0) and key.size(1) == value.size(1)\n",
    "\n",
    "        head_dim = self.embed_dim // self.num_heads\n",
    "        assert head_dim * self.num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        scaling = float(head_dim) ** -0.5\n",
    "\n",
    "        q = self.linear_Q(query)\n",
    "        k = self.linear_K(key)\n",
    "        v = self.linear_V(value)\n",
    "\n",
    "        #JP fix here: disabled this\n",
    "        # q = self.q_scaling_product.mul_scalar(q, scaling)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.uint8:\n",
    "                warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "                attn_mask = attn_mask.to(torch.bool)\n",
    "            assert attn_mask.is_floating_point() or attn_mask.dtype == torch.bool, \\\n",
    "                f'Only float and bool types are supported for attn_mask, not {attn_mask.dtype}'\n",
    "\n",
    "            if attn_mask.dim() == 2:\n",
    "                attn_mask = attn_mask.unsqueeze(0)\n",
    "                if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:\n",
    "                    raise RuntimeError('The size of the 2D attn_mask is not correct.')\n",
    "            elif attn_mask.dim() == 3:\n",
    "                if list(attn_mask.size()) != [bsz * self.num_heads, query.size(0), key.size(0)]:\n",
    "                    raise RuntimeError('The size of the 3D attn_mask is not correct.')\n",
    "            else:\n",
    "                raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "            # attn_mask's dim is 3 now.\n",
    "\n",
    "        # convert ByteTensor key_padding_mask to bool\n",
    "        if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n",
    "            warnings.warn(\"Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "            key_padding_mask = key_padding_mask.to(torch.bool)\n",
    "        if self.bias_k is not None and self.bias_v is not None:\n",
    "            if static_k is None and static_v is None:\n",
    "\n",
    "                # Explicitly assert that bias_k and bias_v are not None\n",
    "                # in a way that TorchScript can understand.\n",
    "                bias_k = self.bias_k\n",
    "                assert bias_k is not None\n",
    "                bias_v = self.bias_v\n",
    "                assert bias_v is not None\n",
    "\n",
    "                k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
    "                v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
    "                if attn_mask is not None:\n",
    "                    attn_mask = nnF.pad(attn_mask, (0, 1))\n",
    "                if key_padding_mask is not None:\n",
    "                    key_padding_mask = nnF.pad(key_padding_mask, (0, 1))\n",
    "            else:\n",
    "                assert static_k is None, \"bias cannot be added to static key.\"\n",
    "                assert static_v is None, \"bias cannot be added to static value.\"\n",
    "        else:\n",
    "            assert self.bias_k is None\n",
    "            assert self.bias_v is None\n",
    "\n",
    "        q = q.contiguous().view(tgt_len, bsz * self.num_heads, head_dim).transpose(0, 1)\n",
    "        if k is not None:\n",
    "            k = k.contiguous().view(-1, bsz * self.num_heads, head_dim).transpose(0, 1)\n",
    "        if v is not None:\n",
    "            v = v.contiguous().view(-1, bsz * self.num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "        if static_k is not None:\n",
    "            assert static_k.size(0) == bsz * self.num_heads\n",
    "            assert static_k.size(2) == head_dim\n",
    "            k = static_k\n",
    "\n",
    "        if static_v is not None:\n",
    "            assert static_v.size(0) == bsz * self.num_heads\n",
    "            assert static_v.size(2) == head_dim\n",
    "            v = static_v\n",
    "\n",
    "        src_len = k.size(1)\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            assert key_padding_mask.size(0) == bsz\n",
    "            assert key_padding_mask.size(1) == src_len\n",
    "\n",
    "        if self.add_zero_attn:\n",
    "            src_len += 1\n",
    "            k_zeros = torch.zeros((k.size(0), 1) + k.size()[2:])\n",
    "            if k.is_quantized:\n",
    "                k_zeros = torch.quantize_per_tensor(k_zeros, k.q_scale(), k.q_zero_point(), k.dtype)\n",
    "            k = torch.cat([k, k_zeros], dim=1)\n",
    "            v_zeros = torch.zeros((v.size(0), 1) + k.size()[2:])\n",
    "            if v.is_quantized:\n",
    "                v_zeros = torch.quantize_per_tensor(v_zeros, v.q_scale(), v.q_zero_point(), v.dtype)\n",
    "            v = torch.cat([v, v_zeros], dim=1)\n",
    "\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = nnF.pad(attn_mask, (0, 1))\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = nnF.pad(key_padding_mask, (0, 1))\n",
    "\n",
    "        # Leaving the quantized zone here\n",
    "        q = self.dequant_q(q)\n",
    "        k = self.dequant_k(k)\n",
    "        v = self.dequant_v(v)\n",
    "        attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "        assert list(attn_output_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                attn_output_weights.masked_fill_(attn_mask, float('-inf'))\n",
    "            else:\n",
    "                attn_output_weights += attn_mask\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            attn_output_weights = attn_output_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_output_weights = attn_output_weights.masked_fill(\n",
    "                key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "                float('-inf'),\n",
    "            )\n",
    "            attn_output_weights = attn_output_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        attn_output_weights = nnF.softmax(\n",
    "            attn_output_weights, dim=-1)\n",
    "        attn_output_weights = nnF.dropout(attn_output_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.bmm(attn_output_weights, v)\n",
    "        assert list(attn_output.size()) == [bsz * self.num_heads, tgt_len, head_dim]\n",
    "        if self.batch_first:\n",
    "            attn_output = attn_output.view(bsz, tgt_len, self.embed_dim)\n",
    "        else:\n",
    "            attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, self.embed_dim)\n",
    "\n",
    "        # Reentering the quantized zone\n",
    "        attn_output = self.quant_attn_output(attn_output)\n",
    "        # for the type: ignore[has-type], see https://github.com/pytorch/pytorch/issues/58969\n",
    "        attn_output = self.out_proj(attn_output)  # type: ignore[has-type]\n",
    "\n",
    "        #JP fix: removed need_weights part from here, return attn_output instead of tuple\n",
    "        return attn_output\n",
    "\n",
    "class QuantizedMultiheadAttention(QuantizeableMultiheadAttention):\n",
    "    _FLOAT_MODULE = torch.ao.nn.quantizable.MultiheadAttention\n",
    "\n",
    "    def _get_name(self):\n",
    "        return \"QuantizedMultiheadAttention\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_float(cls, other):\n",
    "        # The whole flow is float -> observed -> quantized\n",
    "        # This class does observed -> quantized only\n",
    "        raise NotImplementedError(\"It looks like you are trying to convert a \"\n",
    "                                  \"non-observed MHA module. Please, see \"\n",
    "                                  \"the examples on quantizable MHAs.\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_observed(cls, other):\n",
    "        converted = torch.ao.quantization.convert(other, mapping=None,\n",
    "                                                  inplace=False,\n",
    "                                                  remove_qconfig=True,\n",
    "                                                  convert_custom_config_dict=None)\n",
    "        converted.__class__ = cls\n",
    "        # Remove the parameters for the bias_k and bias_v to quantize them\n",
    "        # TODO: This is a potential source of accuracy drop.\n",
    "        #       quantized cat takes the scale and zp of the first\n",
    "        #       element, which might lose the precision in the bias_k\n",
    "        #       and the bias_v (which are cat'ed with k/v being first).\n",
    "        if converted.bias_k is not None:\n",
    "            bias_k = converted._parameters.pop('bias_k')\n",
    "            sc, zp = torch._choose_qparams_per_tensor(bias_k,\n",
    "                                                      reduce_range=False)\n",
    "            bias_k = torch.quantize_per_tensor(bias_k, sc, zp, torch.quint8)\n",
    "            setattr(converted, 'bias_k', bias_k)  # noqa: B010\n",
    "\n",
    "        if converted.bias_v is not None:\n",
    "            bias_v = converted._parameters.pop('bias_v')\n",
    "            sc, zp = torch._choose_qparams_per_tensor(bias_k,  # type: ignore[possibly-undefined]\n",
    "                                                      reduce_range=False)\n",
    "            bias_v = torch.quantize_per_tensor(bias_v, sc, zp, torch.quint8)\n",
    "            setattr(converted, 'bias_v', bias_v)  # noqa: B010\n",
    "\n",
    "        del converted.in_proj_weight\n",
    "        del converted.in_proj_bias\n",
    "\n",
    "        return converted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24d8fa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss, as described in https://arxiv.org/abs/1708.02002.\n",
    "    It is essentially an enhancement to cross entropy loss and is\n",
    "    useful for classification tasks when there is a large class imbalance.\n",
    "    x is expected to contain raw, unnormalized scores for each class.\n",
    "    y is expected to contain class labels.\n",
    "    Shape:\n",
    "        - x: (batch_size, C) or (batch_size, C, d1, d2, ..., dK), K > 0.\n",
    "        - y: (batch_size,) or (batch_size, d1, d2, ..., dK), K > 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, alpha = None, gamma = 0.0, reduction = \"mean\", ignore_index = -100\n",
    "    ):\n",
    "        \"\"\"Constructor.\n",
    "        Args:\n",
    "            alpha (Tensor, optional): Weights for each class. Defaults to None.\n",
    "            gamma (float, optional): A constant, as described in the paper.\n",
    "                Defaults to 0.\n",
    "            reduction (str, optional): 'mean', 'sum' or 'none'.\n",
    "                Defaults to 'mean'.\n",
    "            ignore_index (int, optional): class label to ignore.\n",
    "                Defaults to -100.\n",
    "        \"\"\"\n",
    "        if reduction not in (\"mean\", \"sum\", \"none\"):\n",
    "            raise ValueError('Reduction must be one of: \"mean\", \"sum\", \"none\".')\n",
    "\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "        self.nll_loss = nn.NLLLoss(weight=alpha, reduction=\"none\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        arg_keys = [\"alpha\", \"gamma\", \"reduction\"]\n",
    "        arg_vals = [self.__dict__[k] for k in arg_keys]\n",
    "        arg_strs = [f\"{k}={v!r}\" for k, v in zip(arg_keys, arg_vals)]\n",
    "        arg_str = \", \".join(arg_strs)\n",
    "        return f\"{type(self).__name__}({arg_str})\"\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        if x.ndim > 2:\n",
    "            # (N, C, d1, d2, ..., dK) --> (N * d1 * ... * dK, C)\n",
    "            c = x.shape[1]\n",
    "            x = x.permute(0, *range(2, x.ndim), 1).reshape(-1, c)\n",
    "            # (N, d1, d2, ..., dK) --> (N * d1 * ... * dK,)\n",
    "            y = y.view(-1)\n",
    "\n",
    "        # compute weighted cross entropy term: -alpha * log(pt)\n",
    "        # (alpha is already part of self.nll_loss)\n",
    "        log_p = F.log_softmax(x, dim=-1)\n",
    "        ce = self.nll_loss(log_p, y)\n",
    "\n",
    "        # get true class column from each row\n",
    "        # this is slow due to indexing\n",
    "        # all_rows = torch.arange(len(x))\n",
    "        # log_pt = log_p[all_rows, y]\n",
    "        log_pt = torch.gather(log_p, 1, y.unsqueeze(axis=-1)).squeeze(axis=-1)\n",
    "\n",
    "        # compute focal term: (1 - pt)^gamma\n",
    "        pt = log_pt.exp()\n",
    "        focal_term = (1 - pt) ** self.gamma\n",
    "\n",
    "        # the full loss: -alpha * ((1 - pt)^gamma) * log(pt)\n",
    "        loss = focal_term * ce\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            loss = loss.sum()\n",
    "\n",
    "        return loss\n",
    "        \n",
    "def mlpf_loss(y, ypred, mask):\n",
    "    loss = {}\n",
    "    loss_obj_id = FocalLoss(gamma=2.0, reduction=\"none\")\n",
    "\n",
    "    msk_true_particle = torch.unsqueeze((y[\"cls_id\"] != 0).to(dtype=torch.float32), axis=-1)\n",
    "    nelem = torch.sum(mask)\n",
    "    npart = torch.sum(y[\"cls_id\"] != 0)\n",
    "    \n",
    "    ypred[\"momentum\"] = ypred[\"momentum\"] * msk_true_particle\n",
    "    y[\"momentum\"] = y[\"momentum\"] * msk_true_particle\n",
    "\n",
    "    ypred[\"cls_id_onehot\"] = ypred[\"cls_id_onehot\"].permute((0, 2, 1))\n",
    "\n",
    "    loss_classification = loss_obj_id(ypred[\"cls_id_onehot\"], y[\"cls_id\"]).reshape(y[\"cls_id\"].shape)\n",
    "    loss_regression = torch.nn.functional.huber_loss(ypred[\"momentum\"], y[\"momentum\"], reduction=\"none\")\n",
    "    \n",
    "    # average over all elements that were not padded\n",
    "    loss[\"Classification\"] = loss_classification.sum() / npart\n",
    "    \n",
    "    mom_normalizer = y[\"momentum\"][y[\"cls_id\"] != 0].std(axis=0)\n",
    "    reg_losses = loss_regression[y[\"cls_id\"] != 0]\n",
    "    # average over all true particles\n",
    "    loss[\"Regression\"] = (reg_losses / mom_normalizer).sum() / npart\n",
    "\n",
    "    px = ypred[\"momentum\"][..., 0:1] * ypred[\"momentum\"][..., 3:4] * msk_true_particle\n",
    "    py = ypred[\"momentum\"][..., 0:1] * ypred[\"momentum\"][..., 2:3] * msk_true_particle\n",
    "    pred_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)\n",
    "\n",
    "    px = y[\"momentum\"][..., 0:1] * y[\"momentum\"][..., 3:4] * msk_true_particle\n",
    "    py = y[\"momentum\"][..., 0:1] * y[\"momentum\"][..., 2:3] * msk_true_particle\n",
    "    true_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)\n",
    "    loss[\"MET\"] = torch.nn.functional.huber_loss(pred_met, true_met).mean()\n",
    "\n",
    "    loss[\"Total\"] = loss[\"Classification\"] + loss[\"Regression\"]\n",
    "    # loss[\"Total\"] += 0.1*loss[\"MET\"]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "294ff768",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizeFeaturesStub(torch.ao.quantization.QuantStub):\n",
    "    def __init__(self, num_feats):\n",
    "        super().__init__()\n",
    "        self.num_feats = num_feats\n",
    "        self.quants = torch.nn.ModuleList()\n",
    "        for ifeat in range(self.num_feats):\n",
    "            self.quants.append(torch.ao.quantization.QuantStub())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.quants[ifeat](x[..., ifeat:ifeat+1]) for ifeat in range(self.num_feats)], axis=-1)\n",
    "        \n",
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim=128,\n",
    "        num_heads=2,\n",
    "        width=128,\n",
    "        dropout_mha=0.1,\n",
    "        dropout_ff=0.1,\n",
    "        attention_type=\"efficient\",\n",
    "    ):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "\n",
    "        self.attention_type = attention_type\n",
    "        self.act = nn.ReLU\n",
    "        self.mha = torch.nn.MultiheadAttention(embedding_dim, num_heads, dropout=dropout_mha, batch_first=True)\n",
    "        self.norm0 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.norm1 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.seq = torch.nn.Sequential(\n",
    "            nn.Linear(embedding_dim, width), self.act(), nn.Linear(width, embedding_dim), self.act()\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(dropout_ff)\n",
    "\n",
    "        self.add0 = torch.ao.nn.quantized.FloatFunctional()\n",
    "        self.add1 = torch.ao.nn.quantized.FloatFunctional()\n",
    "        self.mul = torch.ao.nn.quantized.FloatFunctional()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        mha_out = self.mha(x, x, x, need_weights=False)[0]\n",
    "        x = self.add0.add(x, mha_out)\n",
    "        x = self.norm0(x)\n",
    "        x = self.add1.add(x, self.seq(x))\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(x)\n",
    "        # x = self.mul.mul(x, mask.unsqueeze(-1))\n",
    "        return x\n",
    "\n",
    "class RegressionOutput(nn.Module):\n",
    "    def __init__(self, embed_dim, width, act, dropout):\n",
    "        super(RegressionOutput, self).__init__()\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "        self.nn = ffn(embed_dim, 1, width, act, dropout)\n",
    "\n",
    "    def forward(self, elems, x, orig_value):\n",
    "        nn_out = self.nn(x)\n",
    "        nn_out = self.dequant(nn_out)\n",
    "        return orig_value + nn_out\n",
    "\n",
    "def ffn(input_dim, output_dim, width, act, dropout):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, width),\n",
    "        act(),\n",
    "        torch.nn.LayerNorm(width),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(width, output_dim),\n",
    "    )\n",
    "\n",
    "def transform_batch(Xbatch):\n",
    "    Xbatch = Xbatch.clone()\n",
    "    Xbatch[..., 1] = torch.log(Xbatch[..., 1])\n",
    "    Xbatch[..., 5] = torch.log(Xbatch[..., 5])\n",
    "    Xbatch[torch.isnan(Xbatch)] = 0.0\n",
    "    Xbatch[torch.isinf(Xbatch)] = 0.0\n",
    "    return Xbatch\n",
    "    \n",
    "def unpack_target(y):\n",
    "    ret = {}\n",
    "    ret[\"cls_id\"] = y[..., 0].long()\n",
    "\n",
    "    for i, feat in enumerate(Y_FEATURES):\n",
    "        if i >= 2:  # skip the cls and charge as they are defined above\n",
    "            ret[feat] = y[..., i].to(dtype=torch.float32)\n",
    "    ret[\"phi\"] = torch.atan2(ret[\"sin_phi\"], ret[\"cos_phi\"])\n",
    "    \n",
    "    # note ~ momentum = [\"pt\", \"eta\", \"sin_phi\", \"cos_phi\", \"energy\"]\n",
    "    ret[\"momentum\"] = y[..., 2:7].to(dtype=torch.float32)\n",
    "    ret[\"p4\"] = torch.cat(\n",
    "        [ret[\"pt\"].unsqueeze(1), ret[\"eta\"].unsqueeze(1), ret[\"phi\"].unsqueeze(1), ret[\"energy\"].unsqueeze(1)], axis=1\n",
    "    )\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def unpack_predictions(preds):\n",
    "    ret = {}\n",
    "    ret[\"cls_id_onehot\"], ret[\"momentum\"] = preds\n",
    "\n",
    "    ret[\"pt\"] = ret[\"momentum\"][..., 0]\n",
    "    ret[\"eta\"] = ret[\"momentum\"][..., 1]\n",
    "    ret[\"sin_phi\"] = ret[\"momentum\"][..., 2]\n",
    "    ret[\"cos_phi\"] = ret[\"momentum\"][..., 3]\n",
    "    ret[\"energy\"] = ret[\"momentum\"][..., 4]\n",
    "\n",
    "    ret[\"cls_id\"] = torch.argmax(ret[\"cls_id_onehot\"], axis=-1)\n",
    "    ret[\"phi\"] = torch.atan2(ret[\"sin_phi\"], ret[\"cos_phi\"])\n",
    "    ret[\"p4\"] = torch.cat(\n",
    "        [\n",
    "            ret[\"pt\"].unsqueeze(axis=-1),\n",
    "            ret[\"eta\"].unsqueeze(axis=-1),\n",
    "            ret[\"phi\"].unsqueeze(axis=-1),\n",
    "            ret[\"energy\"].unsqueeze(axis=-1),\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "    return ret\n",
    "\n",
    "class MLPF(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=16,\n",
    "        num_classes=6,\n",
    "        num_convs=2,\n",
    "        dropout_ff=0.0,\n",
    "        dropout_conv_reg_mha=0.0,\n",
    "        dropout_conv_reg_ff=0.0,\n",
    "        dropout_conv_id_mha=0.0,\n",
    "        dropout_conv_id_ff=0.0,\n",
    "        num_heads=16,\n",
    "        head_dim=16,\n",
    "        elemtypes=[0,1,2],\n",
    "    ):\n",
    "        super(MLPF, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.act = nn.ReLU\n",
    "        self.elemtypes = elemtypes\n",
    "        self.num_elemtypes = len(self.elemtypes)\n",
    "\n",
    "        embedding_dim = num_heads * head_dim\n",
    "        width = num_heads * head_dim\n",
    "        \n",
    "        self.nn0_id = ffn(self.input_dim, embedding_dim, width, self.act, dropout_ff)\n",
    "        self.nn0_reg = ffn(self.input_dim, embedding_dim, width, self.act, dropout_ff)\n",
    "        \n",
    "        self.conv_id = nn.ModuleList()\n",
    "        self.conv_reg = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_convs):\n",
    "            self.conv_id.append(\n",
    "                SelfAttentionLayer(\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    width=width,\n",
    "                    dropout_mha=dropout_conv_id_mha,\n",
    "                    dropout_ff=dropout_conv_id_ff,\n",
    "                )\n",
    "            )\n",
    "            self.conv_reg.append(\n",
    "                SelfAttentionLayer(\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    width=width,\n",
    "                    dropout_mha=dropout_conv_reg_mha,\n",
    "                    dropout_ff=dropout_conv_reg_ff,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        decoding_dim = self.input_dim + embedding_dim\n",
    "\n",
    "        # DNN that acts on the node level to predict the PID\n",
    "        self.nn_id = ffn(decoding_dim, num_classes, width, self.act, dropout_ff)\n",
    "\n",
    "        # elementwise DNN for node momentum regression\n",
    "        embed_dim = decoding_dim + num_classes\n",
    "        self.nn_pt = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_eta = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_sin_phi = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_cos_phi = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_energy = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        \n",
    "        self.quant = QuantizeFeaturesStub(self.input_dim + len(self.elemtypes))\n",
    "        self.dequant_id = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, X_features, mask):\n",
    "        Xfeat_transformed = transform_batch(X_features)\n",
    "        Xfeat_normed = self.quant(Xfeat_transformed)\n",
    "\n",
    "        embeddings_id, embeddings_reg = [], []\n",
    "        embedding_id = self.nn0_id(Xfeat_normed)\n",
    "        embedding_reg = self.nn0_reg(Xfeat_normed)\n",
    "        for num, conv in enumerate(self.conv_id):\n",
    "            conv_input = embedding_id if num == 0 else embeddings_id[-1]\n",
    "            out_padded = conv(conv_input, mask)\n",
    "            embeddings_id.append(out_padded)\n",
    "        for num, conv in enumerate(self.conv_reg):\n",
    "            conv_input = embedding_reg if num == 0 else embeddings_reg[-1]\n",
    "            out_padded = conv(conv_input, mask)\n",
    "            embeddings_reg.append(out_padded)\n",
    "\n",
    "        final_embedding_id = torch.cat([Xfeat_normed] + [embeddings_id[-1]], axis=-1)\n",
    "        preds_id = self.nn_id(final_embedding_id)\n",
    "\n",
    "        final_embedding_reg = torch.cat([Xfeat_normed] + [embeddings_reg[-1]] + [preds_id], axis=-1)\n",
    "        preds_pt = self.nn_pt(X_features, final_embedding_reg, X_features[..., 1:2])\n",
    "        preds_eta = self.nn_eta(X_features, final_embedding_reg, X_features[..., 2:3])\n",
    "        preds_sin_phi = self.nn_sin_phi(X_features, final_embedding_reg, X_features[..., 3:4])\n",
    "        preds_cos_phi = self.nn_cos_phi(X_features, final_embedding_reg, X_features[..., 4:5])\n",
    "        preds_energy = self.nn_energy(X_features, final_embedding_reg, X_features[..., 5:6])\n",
    "        preds_momentum = torch.cat([preds_pt, preds_eta, preds_sin_phi, preds_cos_phi, preds_energy], axis=-1)\n",
    "        \n",
    "        preds_id = self.dequant_id(preds_id)\n",
    "        return preds_id, preds_momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcc32d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_events_train = 10000\n",
    "max_events_eval = 10000\n",
    "events_per_batch = 1000  # data points per batch\n",
    "nepochs = 10\n",
    "batch_size = 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c8c5298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model\n",
    "# Load the saved model\n",
    "loaded_model = MLPF(input_dim=INPUT_DIM, num_classes=NUM_CLASSES).to(device=device)\n",
    "loaded_model.load_state_dict(torch.load('trained_model_batch_size_150.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d02b4b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the model back on CPU\n",
    "model = loaded_model.to(device=\"cpu\")\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "ds_elems = [ds_train[i] for i in range(max_events_train, max_events_train + max_events_eval)]\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 3000\n",
    "for i in range(0, len(ds_elems), batch_size):\n",
    "    batch_elems = ds_elems[i:i + batch_size]\n",
    "    # input features\n",
    "    X_features = [torch.tensor(elem[\"X\"]).to(torch.float32) for elem in batch_elems]\n",
    "    X_features_padded = pad_sequence(X_features, batch_first=True)\n",
    "    #  target labels\n",
    "    y_targets = [torch.tensor(elem[\"ygen\"]).to(torch.float32) for elem in batch_elems]\n",
    "    y_targets_padded = pad_sequence(y_targets, batch_first=True)\n",
    "    #  mask for the batch\n",
    "    mask = X_features_padded[:, :, 0] != 0\n",
    "    #  model prediction, loss computation\n",
    "    preds = model(X_features_padded, mask)\n",
    "    preds = preds[0].detach(), preds[1].detach()\n",
    "    # Update mask for the batch\n",
    "    mask = X_features_padded[:, :, 0] != 0\n",
    "    # Unpack predictions and targets for the batch\n",
    "    preds_unpacked = unpack_predictions(preds)\n",
    "    targets_unpacked = unpack_target(y_targets_padded)\n",
    "    # append to a list \n",
    "    all_preds.append(preds_unpacked)\n",
    "    all_targets.append(targets_unpacked)\n",
    "    # Compute loss for the batch\n",
    "    loss = mlpf_loss(targets_unpacked, preds_unpacked, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "285b9f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "msk_true_particles = targets_unpacked[\"cls_id\"]!=0\n",
    "msk_pred_particles = preds_unpacked[\"cls_id\"]!=0\n",
    "\n",
    "\n",
    "pt_target = targets_unpacked[\"pt\"][msk_true_particles].numpy()\n",
    "pt_pred = preds_unpacked[\"pt\"][msk_true_particles].numpy()\n",
    "\n",
    "eta_target = targets_unpacked[\"eta\"][msk_true_particles].numpy()\n",
    "eta_pred = preds_unpacked[\"eta\"][msk_true_particles].numpy()\n",
    "\n",
    "sphi_target = targets_unpacked[\"sin_phi\"][msk_true_particles].numpy()\n",
    "sphi_pred = preds_unpacked[\"sin_phi\"][msk_true_particles].numpy()\n",
    "\n",
    "cphi_target = targets_unpacked[\"cos_phi\"][msk_true_particles].numpy()\n",
    "cphi_pred = preds_unpacked[\"cos_phi\"][msk_true_particles].numpy()\n",
    "\n",
    "energy_target = targets_unpacked[\"energy\"][msk_true_particles].numpy()\n",
    "energy_pred = preds_unpacked[\"energy\"][msk_true_particles].numpy()\n",
    "\n",
    "px = preds_unpacked[\"pt\"] * preds_unpacked[\"cos_phi\"] * msk_true_particles\n",
    "py = preds_unpacked[\"pt\"] * preds_unpacked[\"sin_phi\"] * msk_true_particles\n",
    "pred_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)\n",
    "\n",
    "px = targets_unpacked[\"pt\"] * targets_unpacked[\"cos_phi\"] * msk_true_particles\n",
    "py = targets_unpacked[\"pt\"] * targets_unpacked[\"sin_phi\"] * msk_true_particles\n",
    "true_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e8c987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "px = targets_unpacked[\"pt\"] * targets_unpacked[\"cos_phi\"] * msk_true_particles\n",
    "py = targets_unpacked[\"pt\"] * targets_unpacked[\"sin_phi\"] * msk_true_particles\n",
    "pz = targets_unpacked[\"pt\"] * np.sinh(targets_unpacked[\"eta\"]) * msk_true_particles\n",
    "# phi = np.arctan2(targets_unpacked[\"sin_phi\"], targets_unpacked[\"cos_phi\"]) * msk_true_particles\n",
    "\n",
    "px_np = px.detach().cpu().numpy()\n",
    "py_np = py.detach().cpu().numpy()\n",
    "pz_np = pz.detach().cpu().numpy()\n",
    "# phi_np = phi.detach().cpu().numpy()\n",
    "\n",
    "# print(\"px_np\", px_np)\n",
    "# print(\"py_np\", py_np)\n",
    "# print(\"pz_np\", pz_np)\n",
    "# print(\"phi_np\", phi_np)\n",
    "\n",
    "true_mom = np.sqrt(np.sum(px_np, axis=1)**2 + np.sum(py_np, axis=1)**2 + np.sum(pz_np, axis=1)**2)\n",
    "\n",
    "E_np = np.sqrt(px_np**2 + py_np**2 + pz_np**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e9480a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate four-momentum components for predicted particles\n",
    "reco_px = preds_unpacked[\"pt\"] * preds_unpacked[\"cos_phi\"] * msk_pred_particles\n",
    "reco_py = preds_unpacked[\"pt\"] * preds_unpacked[\"sin_phi\"] * msk_pred_particles\n",
    "reco_pz = preds_unpacked[\"pt\"] * np.sinh(preds_unpacked[\"eta\"]) * msk_pred_particles\n",
    "reco_E = np.sqrt(reco_px**2 + reco_py**2 + reco_pz**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6d5b8b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input for fastjet clustering\n",
    "gen_particles = np.array([(px, py, pz, E) for E, px, py, pz in zip(E_np.flatten(), px_np.flatten(), py_np.flatten(), pz_np.flatten()) if E > 0])\n",
    "reco_particles = np.array([(px, py, pz, E) for E, px, py, pz in zip(reco_E.flatten(), reco_px.flatten(), reco_py.flatten(), reco_pz.flatten()) if E > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "467fdc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming gen_particles and reco_particles are numpy arrays\n",
    "gen_particles_list = gen_particles.tolist()\n",
    "reco_particles_list = reco_particles.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "224e3f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define jet clustering parameters\n",
    "R = 0.4  # Jet radius parameter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8faaf189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastjet as fj\n",
    "from fastjet import PseudoJet\n",
    "import awkward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "90dcdeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_particles_pj = [PseudoJet(px, py, pz, E) for px, py, pz, E in gen_particles_list]\n",
    "reco_particles_pj = [PseudoJet(px, py, pz, E) for px, py, pz, E in reco_particles_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c2c82086",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_def = fastjet.JetDefinition(fastjet.antikt_algorithm, R)\n",
    "\n",
    "# Perform clustering\n",
    "gen_sequence = fastjet.ClusterSequence(gen_particles_pj, jet_def)\n",
    "reco_sequence = fastjet.ClusterSequence(reco_particles_pj, jet_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "90276ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastjet._swig.ClusterSequence; proxy of <Swig Object of type 'fastjet::ClusterSequence *' at 0x7f111064c960> >"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "579f180a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Jets:\n",
      "[-0.022600, 0.020038, 0.627816, 0.628543]\n",
      "[0.032802, -0.024951, 0.818359, 0.819397]\n",
      "[-0.023004, 0.056934, 0.499999, 0.503755]\n",
      "[-0.073158, 0.014644, 1.722393, 1.724008]\n",
      "[0.132676, 0.029666, -2.864488, 2.867712]\n",
      "[-0.178000, 0.151061, -6.196912, 6.201437]\n",
      "[0.211493, -0.151355, -4.760786, 4.767884]\n",
      "[0.169005, 0.304854, 7.732807, 7.740811]\n",
      "[-0.129675, 0.448706, -6.779679, 6.795919]\n",
      "[0.237871, -0.599089, 7.377327, 7.406162]\n",
      "[-0.723902, 0.009146, 10.779224, 10.803932]\n",
      "[0.531948, -0.540256, 2.951358, 3.047249]\n",
      "[0.847647, -0.024372, 9.226895, 9.265966]\n",
      "[-0.358763, -0.834288, -2.102699, 2.290466]\n",
      "[0.806861, 0.564100, 18.180601, 18.207237]\n",
      "[-1.114310, -0.782677, 20.572634, 20.619189]\n",
      "[-1.129489, 0.604421, -22.192500, 22.229630]\n",
      "[-1.485200, 0.056774, -18.037331, 18.098928]\n",
      "[0.906770, 1.347816, 20.027581, 20.094104]\n",
      "[-1.211425, -2.252107, -28.320104, 28.439488]\n",
      "[0.960873, 1.609318, 13.184620, 13.318550]\n",
      "[1.336388, -1.747003, -25.679054, 25.773534]\n",
      "[-2.081342, -1.129532, -19.434319, 19.578279]\n",
      "[-2.885471, 1.914635, 45.576931, 45.714521]\n",
      "[-1.949553, -1.684124, 14.048231, 14.287043]\n",
      "[-0.579087, 2.934510, 37.816785, 37.937502]\n",
      "[0.997877, -2.980189, -53.526500, 53.621257]\n",
      "[-1.198976, -3.093379, -0.315378, 3.332740]\n",
      "[1.025816, 3.348100, -43.581443, 43.722959]\n",
      "[1.505351, 3.293168, -3.533942, 5.060024]\n",
      "[-2.887746, 2.420019, 29.268326, 29.515800]\n",
      "[-3.143613, 2.698916, -32.607160, 32.878666]\n",
      "[3.518499, 2.507160, -55.847054, 56.017686]\n",
      "[3.440186, -2.163516, 7.066484, 8.152904]\n",
      "[-3.407110, -6.871328, 70.071828, 70.509939]\n",
      "[-3.248306, 3.197909, -53.202237, 53.398758]\n",
      "[4.998708, -2.034248, -45.282083, 45.615144]\n",
      "[-3.584863, 3.187041, 16.785385, 17.458318]\n",
      "[7.666140, 2.530715, -60.026222, 60.584252]\n",
      "[-4.436388, 7.077556, -44.699633, 45.495845]\n",
      "[8.303016, 5.136975, 73.811700, 74.481275]\n",
      "[8.631562, -3.021648, 59.690123, 60.395824]\n",
      "[-0.458519, 11.272112, -86.560004, 87.312522]\n",
      "[4.399544, 9.946583, -57.745046, 58.779044]\n",
      "[2.233720, -11.938622, 88.988237, 89.833915]\n",
      "[-22.761636, 1.601040, -152.643090, 154.440613]\n",
      "[1.698204, 22.979240, 151.212296, 153.061130]\n",
      "[11.997428, -5.691285, -42.308953, 44.349455]\n",
      "[10.926760, 11.958835, -117.668819, 118.818914]\n",
      "[5.700692, -14.933088, -123.134456, 124.196413]\n",
      "[-29.079608, -11.342196, 234.212938, 236.401244]\n",
      "[24.409537, -23.648017, 220.037604, 222.781202]\n",
      "[-10.644639, -24.679854, -186.035734, 188.054821]\n",
      "[41.465084, 4.660492, 241.725266, 245.486106]\n",
      "[-38.484923, 12.643965, 232.239066, 235.921123]\n",
      "[-14.834342, -37.500585, -167.862134, 172.868422]\n",
      "[-30.170434, -72.671970, 420.177148, 428.016809]\n",
      "[34.493699, -99.339274, 481.715375, 493.931323]\n",
      "[54.772245, -46.710049, -385.065599, 392.183544]\n",
      "[61.974230, -0.826587, -321.559362, 327.889612]\n",
      "[-20.552455, 35.173341, -7.651111, 41.453276]\n",
      "[-63.177033, -43.792109, -428.590376, 435.748330]\n",
      "[92.540271, -34.734198, 393.639299, 406.718738]\n",
      "[-40.977023, 64.090012, 401.253406, 408.807901]\n",
      "[-48.562790, -35.312524, 96.158005, 113.635041]\n",
      "[-86.069715, 67.527428, -467.176374, 480.525605]\n",
      "[-101.552761, 15.841804, 25.547615, 107.283845]\n",
      "[18.594035, -130.586857, -625.316024, 639.913656]\n",
      "[66.085219, -34.683123, 27.200832, 79.550154]\n",
      "[95.437391, 74.077996, -515.515633, 530.200263]\n",
      "[-10.855123, 136.061007, -661.961931, 676.668151]\n",
      "[87.228974, -11.138235, -4.868743, 88.271121]\n",
      "[-91.524956, 15.148129, 80.715151, 123.010184]\n",
      "[-102.459108, -31.994275, 468.085705, 480.669234]\n",
      "[-135.350719, -6.742028, -521.157069, 539.383505]\n",
      "[112.631554, 122.047637, 777.200766, 795.802223]\n",
      "[134.587563, -170.267512, -666.756473, 703.214707]\n",
      "[42.186886, -102.596976, 91.734337, 144.077668]\n",
      "[7.664644, 175.848571, 617.399384, 642.930225]\n",
      "[-138.513920, 42.931225, 483.459575, 506.015440]\n",
      "[178.125982, 60.326815, 568.711046, 600.214216]\n",
      "[2.700714, 151.877297, -329.614170, 363.887264]\n",
      "[228.414958, 3.466647, -705.842173, 743.966732]\n",
      "[96.737513, 105.271294, 33.598688, 147.504776]\n",
      "[98.368679, 221.603774, -738.214443, 779.600615]\n",
      "[-138.663117, 42.472580, -62.600300, 158.085454]\n",
      "[-160.423150, -174.583802, 747.041702, 785.838927]\n",
      "[-83.812782, 128.819861, 102.771856, 185.018323]\n",
      "[-435.123438, 126.966772, -1076.630717, 1173.314284]\n",
      "[-2.482382, 172.394890, -10.195409, 173.702159]\n",
      "[-159.438146, 304.279990, -905.109675, 972.257777]\n",
      "[-240.932288, -246.197713, -1005.114547, 1065.703159]\n",
      "[-188.516011, -74.244899, 17.009698, 203.410979]\n",
      "[-513.469904, -161.498117, 1333.482808, 1446.487473]\n",
      "[-234.464457, 383.234350, 1170.730478, 1258.597291]\n",
      "[-24.111159, -384.201632, 981.995255, 1058.647480]\n",
      "[-95.448564, -217.855184, 80.883649, 251.839885]\n",
      "[-1.914907, -508.276447, -1138.606802, 1252.391650]\n",
      "[129.970305, -223.018848, 1.796828, 258.612282]\n",
      "[365.641999, -438.821473, 1487.351964, 1600.608103]\n",
      "[328.457862, 185.402836, -831.263581, 917.408436]\n",
      "[242.269156, 312.530786, 980.161972, 1061.288875]\n",
      "[-781.094495, 354.816293, 1419.911436, 1670.250247]\n",
      "[-2.683979, -352.855718, -154.191105, 385.715730]\n",
      "[-301.478774, -202.382233, -106.663371, 379.372774]\n",
      "[138.115012, 991.392385, 1890.700006, 2153.848368]\n",
      "[-1026.456358, -264.695217, -2054.100162, 2330.024574]\n",
      "[690.361908, -98.706899, 1504.172128, 1668.640981]\n",
      "[-521.390619, -934.238008, 1666.109959, 1996.870683]\n",
      "[728.488929, 1102.385683, -2163.928309, 2553.217770]\n",
      "[-259.669759, 539.864080, -722.968612, 946.980113]\n",
      "[529.442294, -260.103459, -1200.190546, 1344.419409]\n",
      "[1073.698945, 625.461726, 1960.572340, 2339.773453]\n",
      "[-1236.281835, 993.066467, -2435.887185, 2936.599519]\n",
      "[602.610948, -975.709876, -1695.001443, 2064.049354]\n",
      "[-1927.855696, -343.330212, 2554.422852, 3262.059159]\n",
      "[337.616542, -1584.851907, 2397.192336, 2922.866902]\n",
      "[1750.818876, 17.453262, -2418.464787, 3012.656102]\n",
      "[-827.672977, 1336.064101, 2202.848593, 2734.720583]\n",
      "[1644.917579, -1512.603033, 2557.516634, 3461.330130]\n",
      "[-1048.188546, -1526.863877, -2899.213075, 3470.780755]\n",
      "[-2667.812161, -1791.242585, -2627.380670, 4228.420096]\n",
      "[2410.296595, -214.784011, 2577.900388, 3595.016671]\n",
      "[1220.322504, 2282.797393, 2643.345078, 3747.426336]\n",
      "[122.090834, -2687.122657, -2799.281575, 3939.279371]\n",
      "[2623.308460, 1757.244004, -2818.238998, 4299.239375]\n",
      "[-10.362195, 2215.629991, -2374.446584, 3295.210679]\n",
      "[-2903.889593, -2895.495431, 3233.033008, 5324.993952]\n",
      "[5273.674370, 3563.137020, 4050.450890, 7722.318162]\n",
      "[-3466.218154, 362.860962, -3307.263523, 4888.535972]\n",
      "[4232.590855, -5273.240614, 2963.104690, 7577.411717]\n",
      "[-4044.649056, 2563.216289, 3593.227809, 6118.864718]\n",
      "[-2940.599646, -6116.055543, -3751.932177, 7968.474102]\n",
      "[-4679.878946, 4032.238890, -3544.339965, 7314.859668]\n",
      "[3274.661295, -2045.334234, -3361.434265, 5199.892158]\n",
      "[6507.176524, 139.493612, -3462.210057, 7589.562136]\n",
      "[-1555.996932, 7656.810596, -3569.814262, 8838.453767]\n",
      "[-599.329569, -6375.918726, 4577.239875, 8060.879396]\n",
      "[8829.549139, -614.502681, 3310.225037, 9739.320282]\n",
      "[3309.889709, -6879.438941, -3544.505246, 8660.404905]\n",
      "[-200.304770, 5267.072331, 4367.589013, 7008.082445]\n",
      "[-7357.686456, -1252.889208, 3622.689500, 8537.647658]\n",
      "[-8948.478689, 4974.780357, 338.304467, 10641.066587]\n",
      "[3073.199909, 5369.570550, -3053.842119, 7102.424439]\n",
      "[-3769.725918, 8833.314764, 2047.151015, 10182.874640]\n",
      "[9245.451678, -5980.112823, -1253.064632, 11492.516594]\n",
      "[11694.396063, 5934.245100, -663.627962, 13647.940538]\n",
      "[3584.232719, 9987.721427, 1627.103600, 11127.580005]\n",
      "[-10324.931135, -1216.921300, -2843.853568, 11177.271639]\n",
      "[296.595872, -11461.673176, 810.737481, 11932.904760]\n",
      "[-6904.175606, -7586.496553, 802.091442, 10679.974810]\n",
      "Reconstructed Jets:\n",
      "[0.168660, -0.115051, 2.390475, 2.399178]\n",
      "[-0.310594, 0.022113, 5.812896, 5.821230]\n",
      "[-0.152380, -0.278830, -5.351557, 5.360982]\n",
      "[-0.335733, 0.206017, 8.910760, 8.919462]\n",
      "[-0.417996, -0.012903, -4.297825, 4.318123]\n",
      "[-0.307160, -0.303294, 9.020905, 9.031228]\n",
      "[0.500897, 0.167197, 4.989241, 5.017108]\n",
      "[0.661243, -0.462330, -6.801124, 6.849416]\n",
      "[-0.708903, 0.714940, -20.319626, 20.344555]\n",
      "[1.033863, -0.153783, 9.772880, 9.829433]\n",
      "[-0.587045, 1.189728, 25.471091, 25.506523]\n",
      "[-1.972169, -1.378979, -27.042309, 27.153672]\n",
      "[0.129506, 1.475639, -27.557320, 27.597105]\n",
      "[0.265697, 1.460970, -2.646764, 3.034894]\n",
      "[0.812735, -1.244541, 11.560247, 11.655929]\n",
      "[0.228068, -1.545807, -13.849010, 13.936911]\n",
      "[1.565146, 0.133887, 2.041031, 2.575541]\n",
      "[-0.471985, 1.599688, 14.696042, 14.790834]\n",
      "[-0.860796, 1.434929, 20.293869, 20.362738]\n",
      "[-1.478166, 0.994194, -14.403117, 14.513348]\n",
      "[0.781978, 1.869576, -10.863088, 11.051509]\n",
      "[0.208153, 2.393285, 28.929358, 29.031585]\n",
      "[2.421207, 2.249671, -74.376042, 74.449510]\n",
      "[4.188522, 3.792295, 53.930947, 54.240197]\n",
      "[0.290653, -3.970109, -12.788725, 13.394840]\n",
      "[-1.213303, -3.812565, 34.210603, 34.447345]\n",
      "[-2.819519, 3.174021, 33.587608, 33.857727]\n",
      "[0.433629, -4.360137, 36.870632, 37.131027]\n",
      "[-1.309083, -4.282829, -36.112590, 36.390791]\n",
      "[-2.973747, 4.604421, -47.774705, 48.093715]\n",
      "[-3.932163, -7.720505, 0.893981, 8.852163]\n",
      "[5.781865, 0.482259, -44.889213, 45.267477]\n",
      "[4.453262, 5.256811, 11.856827, 13.723437]\n",
      "[-4.389311, 7.462332, -49.620193, 50.387802]\n",
      "[0.859358, 9.498643, -81.193027, 81.774327]\n",
      "[-12.777625, -0.127295, -88.812832, 89.764399]\n",
      "[-9.340951, 2.892990, 17.535212, 20.079968]\n",
      "[20.315127, 16.835424, -191.252806, 193.183847]\n",
      "[12.722848, -2.261781, -4.488120, 13.753580]\n",
      "[7.196071, -12.570857, -104.931223, 105.965148]\n",
      "[5.343984, 16.604982, 106.758729, 108.234004]\n",
      "[-3.135463, 18.192174, -1.526182, 18.758184]\n",
      "[-15.098388, -2.670767, 117.786156, 118.806280]\n",
      "[-15.398750, 4.500195, 8.354452, 18.090555]\n",
      "[15.685794, -8.777485, 124.904465, 126.234413]\n",
      "[-18.166157, -1.212823, -13.266467, 22.527996]\n",
      "[-11.046897, 16.273709, 11.694001, 22.985829]\n",
      "[34.652148, -19.444238, -225.077413, 228.758963]\n",
      "[-21.250362, -15.670132, 169.311316, 171.448149]\n",
      "[34.238541, 5.683384, 213.975100, 216.914939]\n",
      "[-38.487543, 18.895132, 243.648145, 247.618150]\n",
      "[-40.669257, 20.964953, -230.274660, 235.015054]\n",
      "[-24.366933, -8.792697, 35.235913, 43.746840]\n",
      "[1.187431, -26.181900, 137.695536, 140.246311]\n",
      "[-33.013176, -31.707364, -309.218152, 312.760221]\n",
      "[-4.776902, -51.542350, -282.093219, 287.042814]\n",
      "[-36.400273, 4.044684, 3.996428, 36.847128]\n",
      "[-29.449364, -65.416440, 371.595078, 378.890712]\n",
      "[10.031411, 38.144614, -119.237450, 125.657379]\n",
      "[-14.853365, 41.251349, 249.947467, 253.909714]\n",
      "[49.293868, -90.117404, 513.003979, 523.856696]\n",
      "[-46.964423, -58.274263, -299.177588, 308.776922]\n",
      "[80.107422, -31.907614, 370.229311, 380.862119]\n",
      "[-27.484645, 43.604658, 63.458963, 81.767783]\n",
      "[-53.734617, -28.125389, -83.524384, 103.262730]\n",
      "[-80.634538, -25.270252, -395.508995, 405.021936]\n",
      "[17.208421, 59.219770, 35.991398, 71.409419]\n",
      "[80.428048, 13.560954, -381.771133, 390.804972]\n",
      "[-27.292304, -62.747832, 167.927164, 181.671228]\n",
      "[50.778097, -137.170649, -694.310776, 710.499316]\n",
      "[81.025284, -6.108882, -232.674542, 247.100093]\n",
      "[71.092294, 12.816267, 1.543963, 72.260880]\n",
      "[68.528710, 23.747232, 230.172914, 241.674990]\n",
      "[-50.337755, 74.253851, 308.530750, 321.808441]\n",
      "[-73.048347, -25.248834, 5.211800, 77.470037]\n",
      "[22.418609, -74.598429, -1.898455, 78.158442]\n",
      "[-107.104555, -18.191081, 462.134860, 475.292391]\n",
      "[-3.207078, 154.521956, -743.813766, 760.908302]\n",
      "[94.819455, 89.381440, 630.078376, 644.088708]\n",
      "[118.155937, 124.427332, -677.010232, 699.712579]\n",
      "[-155.175857, 80.567939, 534.739720, 564.385561]\n",
      "[174.851480, -99.850904, -668.406284, 700.045960]\n",
      "[-116.735272, 141.973925, -629.724963, 657.645597]\n",
      "[-185.255994, -162.774202, 808.536488, 847.454028]\n",
      "[-358.664291, 51.551987, -1036.670671, 1101.535893]\n",
      "[85.403701, -115.746646, -79.580924, 164.407331]\n",
      "[6.426560, 178.502846, 600.223114, 627.147359]\n",
      "[349.206984, -23.382625, 884.325498, 955.298629]\n",
      "[354.393371, 105.667068, -881.147255, 960.793279]\n",
      "[174.670426, 211.314348, 820.803947, 868.188727]\n",
      "[-96.967902, -300.592437, -848.713946, 909.686629]\n",
      "[-90.631326, 435.600971, -995.703604, 1097.447457]\n",
      "[-216.820682, 91.412957, -56.140392, 242.514013]\n",
      "[15.508252, -347.264708, 911.157943, 978.618214]\n",
      "[386.949573, -419.336488, 1442.899025, 1559.660335]\n",
      "[-182.843310, 558.678351, 1183.213463, 1328.350127]\n",
      "[159.951664, -372.295994, -972.982652, 1058.707583]\n",
      "[-561.745921, -56.822166, 1268.928282, 1395.844197]\n",
      "[-653.596698, -344.029343, -1769.598339, 1927.537774]\n",
      "[431.650621, 671.023175, -1654.145225, 1845.229750]\n",
      "[-663.974561, 575.220129, 1575.216832, 1816.280790]\n",
      "[553.111427, -346.182293, -1202.131752, 1378.483330]\n",
      "[-599.215457, -552.198262, 1431.990298, 1661.657888]\n",
      "[369.092394, 1188.933756, 2169.693091, 2522.062432]\n",
      "[1164.444112, 553.085610, 2154.000908, 2533.000120]\n",
      "[-969.381027, 626.997492, -2014.883438, 2340.307170]\n",
      "[-1634.158980, 80.218623, 1897.946847, 2543.273813]\n",
      "[-15.361997, -1251.695310, -1711.809193, 2138.438122]\n",
      "[1270.138090, -463.750749, 1875.622505, 2335.362614]\n",
      "[-173.475466, -1265.567863, 1861.082198, 2281.913286]\n",
      "[-1070.705674, 1997.489509, -2653.124635, 3538.958916]\n",
      "[-1300.210618, -1834.433643, -3337.083377, 4067.384698]\n",
      "[903.235516, -1852.018833, 2830.493465, 3544.802154]\n",
      "[-1644.332929, -26.308487, -2269.573932, 2833.015711]\n",
      "[2252.106653, -166.340186, -3035.722575, 3827.103603]\n",
      "[1761.042929, 1424.607661, -2568.140096, 3463.922649]\n",
      "[2678.409842, 3387.655045, 3762.013463, 5841.984874]\n",
      "[-3079.878780, 2335.483023, 3492.047072, 5310.056202]\n",
      "[-2386.382894, -1661.251273, 2508.849847, 3905.877053]\n",
      "[424.915109, 1774.423127, -2023.889629, 2764.392544]\n",
      "[1020.768745, -1544.409795, -2198.626915, 2911.460510]\n",
      "[3002.969620, -2047.484740, 2158.490928, 4308.925735]\n",
      "[-300.518213, 4920.591639, 4474.946832, 6820.684333]\n",
      "[1228.580647, -5642.777089, -3413.051594, 6863.456168]\n",
      "[-3909.266615, 1423.105865, -3073.180615, 5274.310908]\n",
      "[-3405.090554, -1791.855110, -2900.848895, 4922.070498]\n",
      "[4890.523973, 726.597379, 3650.243403, 6270.924433]\n",
      "[5271.321014, -2909.469082, -3881.286135, 7333.707401]\n",
      "[-2315.455311, -5535.486254, 4309.725208, 7564.719323]\n",
      "[-338.492044, 6837.456515, -3237.643286, 7765.796044]\n",
      "[2421.391832, -6059.956827, 3532.008583, 7635.935745]\n",
      "[-3194.233416, -6106.326552, -3920.992908, 8138.228598]\n",
      "[-5213.799193, 6501.482426, -3485.014038, 9313.046278]\n",
      "[-7061.854462, -971.070403, 3454.155143, 8145.292961]\n",
      "[9690.346830, -2171.417572, 1369.129231, 10392.220570]\n",
      "[-2917.174273, 8298.809619, 2299.990088, 9384.892317]\n",
      "[3861.274931, 5355.554525, -2973.607461, 7443.911347]\n",
      "[8754.558642, 5415.391294, 1983.467295, 10845.833765]\n",
      "[-9997.925760, -887.766269, -2669.059392, 10744.713762]\n",
      "[3434.060645, 9954.490286, 1652.617481, 11044.523987]\n",
      "[7075.457118, -8552.805721, -1362.291220, 11603.609700]\n",
      "[-7187.552630, -6526.247621, 476.194092, 10064.035970]\n",
      "[-1165.308989, -12071.142668, 959.164800, 12632.182879]\n",
      "[10632.426853, 2527.904833, -4765.783758, 12400.355923]\n",
      "[-10648.664965, 6360.580828, 2047.895470, 13039.012622]\n"
     ]
    }
   ],
   "source": [
    "# Print the jets for verification\n",
    "print(\"Generated Jets:\")\n",
    "for gen_jets in gen_sequence.inclusive_jets(0.0):\n",
    "    print(gen_jets)\n",
    "\n",
    "print(\"Reconstructed Jets:\")\n",
    "for reco_jets in reco_sequence.inclusive_jets(0.0):\n",
    "    print(reco_jets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2be292b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of gen_jets: <class 'fastjet._swig.PseudoJet'>\n",
      "Type of reco_jets: <class 'fastjet._swig.PseudoJet'>\n"
     ]
    }
   ],
   "source": [
    "# Debugging: Check the type of gen_jets and reco_jets\n",
    "print(\"Type of gen_jets:\", type(gen_jets))\n",
    "print(\"Type of reco_jets:\", type(reco_jets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "85fef1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect generated jets into a list\n",
    "gen_jet_list = []\n",
    "for gen_jets in gen_sequence.inclusive_jets(0.0):\n",
    "    gen_jet_list.append(gen_jets)\n",
    "\n",
    "# Collect reconstructed jets into a list\n",
    "reco_jet_list = []\n",
    "for reco_jets in reco_sequence.inclusive_jets(0.0):\n",
    "    reco_jet_list.append(reco_jets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "534bcc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of generated jets: 151\n",
      "Total number of reconstructed jets: 145\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of jets\n",
    "print(\"Total number of generated jets:\", len(gen_jet_list))\n",
    "print(\"Total number of reconstructed jets:\", len(reco_jet_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "14d22cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your jet matching functions here\n",
    "\n",
    "def deltar(eta1, phi1, eta2, phi2):\n",
    "    delta_eta = eta1 - eta2\n",
    "    delta_phi = np.arctan2(np.sin(phi1 - phi2), np.cos(phi1 - phi2))\n",
    "    return np.sqrt(delta_eta**2 + delta_phi**2)\n",
    "\n",
    "\n",
    "def match_jets(jets1, jets2, deltaR_cut):\n",
    "    iev = len(jets1)\n",
    "    jet_inds_1_ev = []\n",
    "    jet_inds_2_ev = []\n",
    "    for ev in range(iev):\n",
    "        j1 = jets1[ev]\n",
    "        j2 = jets2[ev]\n",
    "\n",
    "        jet_inds_1 = []\n",
    "        jet_inds_2 = []\n",
    "        for ij1 in range(len(j1)):\n",
    "            drs = np.zeros(len(j2), dtype=np.float64)\n",
    "            for ij2 in range(len(j2)):\n",
    "                eta1 = j1.eta[ij1]\n",
    "                eta2 = j2.eta[ij2]\n",
    "                phi1 = j1.phi[ij1]\n",
    "                phi2 = j2.phi[ij2]\n",
    "\n",
    "                # Calculate deltaR\n",
    "                dr = deltar(eta1, phi1, eta2, phi2)\n",
    "                drs[ij2] = dr\n",
    "            if len(drs) > 0:\n",
    "                min_idx_dr = np.argmin(drs)\n",
    "                if drs[min_idx_dr] < deltaR_cut:\n",
    "                    jet_inds_1.append(ij1)\n",
    "                    jet_inds_2.append(min_idx_dr)\n",
    "        jet_inds_1_ev.append(jet_inds_1)\n",
    "        jet_inds_2_ev.append(jet_inds_2)\n",
    "    return jet_inds_1_ev, jet_inds_2_ev\n",
    "\n",
    "def build_dummy_array(num_events):\n",
    "    return awkward.Array([[] for _ in range(num_events)])\n",
    "\n",
    "\n",
    "def match_two_jet_collections(jets_coll, name1, name2, jet_match_dr):\n",
    "    num_events = len(jets_coll[name1])\n",
    "    vec1 = vector.awk(\n",
    "        awkward.zip(\n",
    "            {\n",
    "                \"pt\": jets_coll[name1].pt,\n",
    "                \"eta\": jets_coll[name1].eta,\n",
    "                \"phi\": jets_coll[name1].phi,\n",
    "                \"energy\": jets_coll[name1].energy,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    vec2 = vector.awk(\n",
    "        awkward.zip(\n",
    "            {\n",
    "                \"pt\": jets_coll[name2].pt,\n",
    "                \"eta\": jets_coll[name2].eta,\n",
    "                \"phi\": jets_coll[name2].phi,\n",
    "                \"energy\": jets_coll[name2].energy,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    ret = match_jets(vec1, vec2, jet_match_dr)\n",
    "    j1_idx = awkward.from_iter(ret[0])\n",
    "    j2_idx = awkward.from_iter(ret[1])\n",
    "\n",
    "    num_jets = len(awkward.flatten(j1_idx))\n",
    "\n",
    "    # In case there are no jets matched, create dummy array to ensure correct types\n",
    "    if num_jets > 0:\n",
    "        c1_to_c2 = awkward.Array({name1: j1_idx, name2: j2_idx})\n",
    "    else:\n",
    "        dummy = build_dummy_array(num_events)\n",
    "        c1_to_c2 = awkward.Array({name1: dummy, name2: dummy})\n",
    "\n",
    "    return c1_to_c2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c6eeec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_match_dr = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "281be828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect generated jets into a list\n",
    "gen_jet_list = []\n",
    "for gen_jets in gen_sequence.inclusive_jets(0.0):\n",
    "    gen_jet_list.append({\n",
    "        \"pt\": gen_jets.pt,\n",
    "        \"eta\": gen_jets.eta,\n",
    "        \"phi\": gen_jets.phi,\n",
    "        \"energy\": gen_jets.e\n",
    "    })\n",
    "\n",
    "# Collect reconstructed jets into a list\n",
    "reco_jet_list = []\n",
    "for reco_jets in reco_sequence.inclusive_jets(0.0):\n",
    "    reco_jet_list.append({\n",
    "        \"pt\": reco_jets.pt,\n",
    "        \"eta\": reco_jets.eta,\n",
    "        \"phi\": reco_jets.phi,\n",
    "        \"energy\": reco_jets.e\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5d50ff6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Jets:\n",
      "pt: 0.03020368565901634, eta: 3.7280091704607465, phi: 2.4162073023779485, energy: 0.6285425424575806\n",
      "pt: 0.041213055186533565, eta: 3.6823271924490286, phi: 5.632884390744783, energy: 0.8193965554237366\n",
      "pt: 0.06140570013828287, eta: 2.7939994599454585, phi: 1.9547844026799632, energy: 0.5037552118301392\n",
      "pt: 0.07460956599779874, eta: 3.8328169847968794, phi: 2.9440343454298223, energy: 1.7240079641342163\n",
      "pt: 0.1359516602608681, eta: -3.7415554085838427, phi: 0.2199765818587965, energy: 2.8677124977111816\n",
      "pt: 0.23345986844838232, eta: -3.972297975901402, phi: 2.4378801349365395, energy: 6.201436519622803\n",
      "pt: 0.26007208360622347, eta: -3.6011016351425695, phi: 5.662033049248008, energy: 4.767884254455566\n",
      "pt: 0.34856613606234244, eta: 3.793054002827001, phi: 1.064596353851168, energy: 7.740810990333557\n",
      "pt: 0.46706827462363676, eta: -3.369541212754246, phi: 1.852128704866259, energy: 6.795919299125671\n",
      "pt: 0.644585343224867, eta: 3.1326097717337107, phi: 5.090353568373526, energy: 7.406161904335022\n",
      "pt: 0.7239599766799867, eta: 3.3949127680203888, phi: 3.128959112798766, energy: 10.80393236875534\n",
      "pt: 0.7581859391071882, eta: 2.0683439759115885, phi: 5.490038589663468, energy: 3.0472491979599\n",
      "pt: 0.8479968523106942, eta: 3.082253111475849, phi: 6.254441114325361, energy: 9.265966355800629\n",
      "pt: 0.908156185616182, eta: -1.5763815993337154, phi: 4.30627128591155, energy: 2.290465831756592\n",
      "pt: 0.9844970779903413, eta: 3.609858959514952, phi: 0.6101412620989063, energy: 18.207237243652344\n",
      "pt: 1.3617161436432368, eta: 3.4094566798247583, phi: 3.753918658967614, energy: 20.619189023971558\n",
      "pt: 1.2810422112200157, eta: -3.5460595846574314, phi: 2.650239350921129, energy: 22.22963047027588\n",
      "pt: 1.4862849614918237, eta: -3.1910042034442427, phi: 3.1033847043312397, energy: 18.098927855491638\n",
      "pt: 1.6244506866491701, eta: 3.2067285139959365, phi: 0.9785801291966034, energy: 20.094103693962097\n",
      "pt: 2.5572518164955014, eta: -3.0998181730612075, phi: 4.218877483234335, energy: 28.43948793411255\n",
      "pt: 1.874348279464016, eta: 2.6489517722734544, phi: 1.032534915282694, energy: 13.318550311028957\n",
      "pt: 2.1995341611367154, eta: -3.152406397970336, phi: 5.365396157020576, energy: 25.773533821105957\n",
      "pt: 2.368085001560902, eta: -2.8017974875354215, phi: 3.6388095267542973, energy: 19.578278571367264\n",
      "pt: 3.4629139661716533, eta: 3.271878551374314, phi: 2.5557554157507325, energy: 45.714521288871765\n",
      "pt: 2.5762436958190498, eta: 2.3976147599693785, phi: 3.8540732780210907, energy: 14.287043452262878\n",
      "pt: 2.9911019019005933, eta: 3.2318187234130926, phi: 1.7656300093067747, energy: 37.93750214576721\n",
      "pt: 3.1428146466048674, eta: -3.529066005673823, phi: 5.035492139632099, energy: 53.621257305145264\n",
      "pt: 3.317610026283287, eta: -0.09491911022912067, phi: 4.342622736046442, energy: 3.3327399492263794\n",
      "pt: 3.5017244430591488, eta: -3.216133162710474, phi: 1.273489935422533, energy: 43.72295928001404\n",
      "pt: 3.620916856129804, eta: -0.86428641926975, phi: 1.142042776363067, energy: 5.060023546218872\n",
      "pt: 3.7677009011742304, eta: 2.7473054250665645, phi: 2.444088739206873, energy: 29.515800148248672\n",
      "pt: 4.14324144909995, eta: -2.7602128052001103, phi: 2.432161890766391, energy: 32.87866607308388\n",
      "pt: 4.320380467308597, eta: -3.2539133147958896, phi: 0.619109302362057, energy: 56.01768624037504\n",
      "pt: 4.063948600733234, eta: 1.3203386845184382, phi: 5.7217899260254015, energy: 8.152903713285923\n",
      "pt: 7.669650731912021, eta: 2.908378612370005, phi: 4.252071292955066, energy: 70.5099390745163\n",
      "pt: 4.558301727747785, eta: -3.15212767335532, phi: 2.3640122799988585, energy: 53.39875781536102\n",
      "pt: 5.396780612307994, eta: -2.823788307827244, phi: 5.896697894352357, energy: 45.61514401435852\n",
      "pt: 4.796714340829913, eta: 1.9655420438946078, phi: 2.414872969525708, energy: 17.45831759274006\n",
      "pt: 8.073055367504422, eta: -2.7038883279496226, phi: 0.3188521515590127, energy: 60.58425206691027\n",
      "pt: 8.353043384171418, eta: -2.3791045077690893, phi: 2.1307067700841804, energy: 45.495845437049866\n",
      "pt: 9.763635805360956, eta: 2.7203455047628737, phi: 0.5540473721116403, energy: 74.48127511516213\n",
      "pt: 9.145174662471097, eta: 2.5749047607060267, phi: 5.946448430419504, energy: 60.39582351595163\n",
      "pt: 11.281434064858635, eta: -2.7350464079692367, phi: 1.611451186932923, energy: 87.31252222135663\n",
      "pt: 10.876143878146426, eta: -2.3713659866322443, phi: 1.1543498782460624, energy: 58.77904412150383\n",
      "pt: 12.145789544822543, eta: 2.6892938156952266, phi: 4.897350762881345, energy: 89.83391526341438\n",
      "pt: 22.817874577558133, eta: -2.599245645244043, phi: 3.0713689120811796, energy: 154.44061335362494\n",
      "pt: 23.041904497000704, eta: 2.580272570304257, phi: 1.4970287513187517, energy: 153.06113049387932\n",
      "pt: 13.278892682246498, eta: -1.8757335370053796, phi: 5.84024670376764, energy: 44.349455088377\n",
      "pt: 16.19900684578416, eta: -2.680775967801694, phi: 0.8304647639590724, energy: 118.81891445070505\n",
      "pt: 15.98421113592986, eta: -2.7390089946250944, phi: 5.077063467270499, energy: 124.19641342759132\n",
      "pt: 31.213282404320996, eta: 2.7129450101463055, phi: 3.5134830165549507, energy: 236.40124443173409\n",
      "pt: 33.98608857267263, eta: 2.5669058461583782, phi: 5.513631814462791, energy: 222.7812023870647\n",
      "pt: 26.877565420815586, eta: -2.6329719129698246, phi: 4.305186843764543, energy: 188.05482110381126\n",
      "pt: 41.72617114059412, eta: 2.4571878303077326, phi: 0.11192584036303387, energy: 245.4861056227237\n",
      "pt: 40.50875428611645, eta: 2.446917220801267, phi: 2.824159271301794, energy: 235.92112332955003\n",
      "pt: 40.32804987064846, eta: -2.1333696995421474, phi: 4.335701954453538, energy: 172.86842219531536\n",
      "pt: 78.68589656096208, eta: 2.3770139084078417, phi: 4.318883047875701, energy: 428.0168091561645\n",
      "pt: 105.15753267543474, eta: 2.2267471531197502, phi: 5.046595073465449, energy: 493.9313230365515\n",
      "pt: 71.98491165064598, eta: -2.3787288485787763, phi: 5.577065004822945, energy: 392.18354407697916\n",
      "pt: 61.979742256145826, eta: -2.3486828561370157, phi: 6.269848502255347, energy: 327.88961150869727\n",
      "pt: 40.7377877954502, eta: -0.186726630787628, phi: 2.0996059632345614, energy: 41.45327627658844\n",
      "pt: 76.87058152744135, eta: -2.4194724536685484, phi: 3.747716632026855, energy: 435.7483301013708\n",
      "pt: 98.84415162631927, eta: 2.090440848153245, phi: 5.92411533611901, energy: 406.71873797662556\n",
      "pt: 76.07000788985873, eta: 2.364952768158661, phi: 2.139660155522495, energy: 408.80790074169636\n",
      "pt: 60.04430852850651, eta: 1.2497519951699032, phi: 3.7703098930122803, energy: 113.63504142127931\n",
      "pt: 109.39812281012355, eta: -2.1582954608525915, phi: 2.476333390545626, energy: 480.52560517191887\n",
      "pt: 102.78096103279478, eta: 0.2460728002752081, phi: 2.986844068541714, energy: 107.28384473919868\n",
      "pt: 131.90400085710087, eta: -2.2602726899306456, phi: 4.8538264948281284, energy: 639.9136555716395\n",
      "pt: 74.63360624736781, eta: 0.35683702832166647, phi: 5.799876117108143, energy: 79.55015363544226\n",
      "pt: 120.81326498814177, eta: -2.1575247368164954, phi: 0.6600560252383059, energy: 530.20026306808\n",
      "pt: 136.4933380773694, eta: -2.2825429572270255, phi: 1.6504089926555672, energy: 676.6681511551142\n",
      "pt: 87.9372174040825, eta: -0.05533788064143445, phi: 6.1561829476058065, energy: 88.27112060785294\n",
      "pt: 92.77005660417974, eta: 0.7864434765821569, phi: 2.97757136579591, energy: 123.0101840943098\n",
      "pt: 107.33826197026566, eta: 2.178707648598991, phi: 3.4442623579075584, energy: 480.66923436522484\n",
      "pt: 135.51853078924057, eta: -2.056581459307834, phi: 3.1913630588613353, energy: 539.3835045844316\n",
      "pt: 166.07676702067036, eta: 2.247620502662403, phi: 0.825499865088077, energy: 795.8022234225646\n",
      "pt: 217.03648894277336, eta: -1.841001319888501, phi: 5.381278148324077, energy: 703.214707262814\n",
      "pt: 110.9318385290589, eta: 0.753569437395093, phi: 5.102504826537994, energy: 144.07766799628735\n",
      "pt: 176.01552973944507, eta: 1.9678176480855796, phi: 1.527237284144311, energy: 642.9302250165492\n",
      "pt: 145.01446839473422, eta: 1.9190510456076348, phi: 2.8410402693335413, energy: 506.0154401399195\n",
      "pt: 188.0643239554135, eta: 1.826016290847293, phi: 0.32655031411934254, energy: 600.2142157405615\n",
      "pt: 151.901307849578, eta: -1.5171436322531848, phi: 1.5530159909315844, energy: 363.8872636780143\n",
      "pt: 228.44126325324518, eta: -1.846473450720221, phi: 0.015175805602263967, energy: 743.9667317345738\n",
      "pt: 142.96919811044606, eta: 0.23289538167256094, phi: 0.8276176588033106, energy: 147.50477576255798\n",
      "pt: 242.4554184017136, eta: -1.832501279537589, phi: 1.1530313885873946, energy: 779.6006154716015\n",
      "pt: 145.02199861815453, eta: -0.4192686902142985, phi: 2.844365667710213, energy: 158.08545370399952\n",
      "pt: 237.09721875074294, eta: 1.8650793778640917, phi: 3.969235395995393, energy: 785.8389273863286\n",
      "pt: 153.685194162387, eta: 0.6268500438339308, phi: 2.1476072996758457, energy: 185.0183229893446\n",
      "pt: 453.26919962587874, eta: -1.5998792455618223, phi: 2.8576804273944956, energy: 1173.3142842203379\n",
      "pt: 172.4127614964974, eta: -0.0590993172140617, phi: 1.5851947256066783, energy: 173.70215873420238\n",
      "pt: 343.52122903030954, eta: -1.6961632448788027, phi: 2.0534472942114133, energy: 972.2577774189413\n",
      "pt: 344.4730485871279, eta: -1.7921374069568576, phi: 3.9377994700486836, energy: 1065.703159284778\n",
      "pt: 202.6094555816797, eta: 0.08385482449280914, phi: 3.5167763010139086, energy: 203.41097936034203\n",
      "pt: 538.2685051967584, eta: 1.6387884526910843, phi: 3.4463195160037827, energy: 1446.4874731451273\n",
      "pt: 449.2684596961638, eta: 1.6858445148126175, phi: 2.1198503414673846, energy: 1258.5972914248705\n",
      "pt: 384.9574545401857, eta: 1.665977707550134, phi: 4.649714649394035, energy: 1058.6474800147116\n",
      "pt: 237.8472402371721, eta: 0.3338303986509825, phi: 4.299451139104619, energy: 251.83988539129496\n",
      "pt: 508.2800540754491, eta: -1.546136722436749, phi: 4.7086215463890815, energy: 1252.3916500769556\n",
      "pt: 258.12726876090625, eta: 0.006960961476076055, phi: 5.240048404885021, energy: 258.6122817248106\n",
      "pt: 571.19029799824, eta: 1.68516079332797, phi: 5.407070013740083, energy: 1600.6081031672657\n",
      "pt: 377.17208125201995, eta: -1.531288512882113, phi: 0.513880603755174, energy: 917.4084363281727\n",
      "pt: 395.4362606778551, eta: 1.6392857721086236, phi: 0.9113707311058107, energy: 1061.2888747937977\n",
      "pt: 857.9062952049413, eta: 1.277823845589759, phi: 2.7152056798099964, energy: 1670.2502471283078\n",
      "pt: 352.865925450341, eta: -0.42413663564646975, phi: 4.704782679352437, energy: 385.7157298102975\n",
      "pt: 363.1088255497703, eta: -0.2896819166475964, phi: 3.7327950239908696, energy: 379.372773591429\n",
      "pt: 1000.9668411852548, eta: 1.3928044480905635, phi: 1.4323730896245024, energy: 2153.848368080333\n",
      "pt: 1060.0359490102865, eta: -1.4154514370230769, phi: 3.393967213899768, energy: 2330.0245737694204\n",
      "pt: 697.3826896846673, eta: 1.5116717861397526, phi: 6.1411693402104754, energy: 1668.6409811638296\n",
      "pt: 1069.8826258873096, eta: 1.2261237466562223, phi: 4.203354451069046, energy: 1996.870683092624\n",
      "pt: 1321.3441312821335, eta: -1.2687819564249359, phi: 0.9868457652181241, energy: 2553.217770021409\n",
      "pt: 599.0672820963226, eta: -1.0203342602468721, phi: 2.0191214128774324, energy: 946.9801127500832\n",
      "pt: 589.8838465704349, eta: -1.459012471559711, phi: 5.826539448392306, energy: 1344.4194092452526\n",
      "pt: 1242.5907591054763, eta: 1.237163784030772, phi: 0.527474777267379, energy: 2339.77345290035\n",
      "pt: 1585.7407673666294, eta: -1.2146331010118068, phi: 2.4648617946168674, energy: 2936.5995191298425\n",
      "pt: 1146.7997714424976, eta: -1.1825128639946092, phi: 5.265658536267442, energy: 2064.049353610724\n",
      "pt: 1958.1887602248796, eta: 1.0811810936942874, phi: 3.3178341106682647, energy: 3262.059158688411\n",
      "pt: 1620.4136805176936, eta: 1.1832653307495449, phi: 4.922278728370695, energy: 2922.8669020030648\n",
      "pt: 1750.905866702392, eta: -1.1270440204462546, phi: 0.009968297856903622, energy: 3012.6561023667455\n",
      "pt: 1571.6583087275178, eta: 1.1389162236335277, phi: 2.125420672940545, energy: 2734.7205830290914\n",
      "pt: 2234.6636837974065, eta: 0.9799357766618524, phi: 5.53966721236602, energy: 3461.330130495131\n",
      "pt: 1852.0292994575539, eta: -1.2305150754939078, phi: 4.110782557081986, energy: 3470.7807553038\n",
      "pt: 3213.3738851405087, eta: -0.7463830280293259, phi: 3.732884015340102, energy: 4228.420095553622\n",
      "pt: 2419.8474846754743, eta: 0.9268126820336295, phi: 6.194309030652654, energy: 3595.0166705548763\n",
      "pt: 2588.503612003807, eta: 0.8962756742261404, phi: 1.0798741364701954, energy: 3747.4263355359435\n",
      "pt: 2689.894858172474, eta: -0.9098383205508477, phi: 4.757793280406085, energy: 3939.2793707847595\n",
      "pt: 3157.475853625869, eta: -0.8033271436596167, phi: 0.5902086946664071, energy: 4299.239374706522\n",
      "pt: 2215.6542220594647, eta: -0.9311539113406918, phi: 1.5754731542706504, energy: 3295.21067949757\n",
      "pt: 4100.788772308981, eta: 0.7235790028479877, phi: 3.925543395206473, energy: 5324.993952491321\n",
      "pt: 6364.557076609164, eta: 0.5997922773406816, phi: 0.5941933328738263, energy: 7722.318162115291\n",
      "pt: 3485.159446887768, eta: -0.8448157158815294, phi: 3.037287595568959, energy: 4888.5359722189605\n",
      "pt: 6761.796500638245, eta: 0.42527691727582495, phi: 5.388746488595332, energy: 7577.4117170181125\n",
      "pt: 4788.451078153027, eta: 0.6934628269505573, phi: 2.5767400534618026, energy: 6118.86471830681\n",
      "pt: 6786.255350425702, eta: -0.5279956207980702, phi: 4.26421899432742, energy: 7968.474101960659\n",
      "pt: 6177.395681936448, eta: -0.546194101885408, phi: 2.430395853152096, energy: 7314.859667682089\n",
      "pt: 3860.932364538499, eta: -0.7868745896026452, phi: 5.72487790419937, energy: 5199.892158079892\n",
      "pt: 6508.671507140644, eta: -0.5095941386116499, phi: 0.021433604800481468, energy: 7589.562135562301\n",
      "pt: 7813.313954339559, eta: -0.4423234360484657, phi: 1.771283591347223, energy: 8838.453766923398\n",
      "pt: 6404.0249473628455, eta: 0.6647036731402063, phi: 4.618665438931171, energy: 8060.879396040924\n",
      "pt: 8850.906819955264, eta: 0.36578662760832126, phi: 6.21370119143681, energy: 9739.32028208673\n",
      "pt: 7634.268139641362, eta: -0.44904492232211884, phi: 5.1608252102623675, energy: 8660.404904881492\n",
      "pt: 5270.879712342151, eta: 0.7548658277229283, phi: 1.6088076312930697, energy: 7008.082444727421\n",
      "pt: 7463.597079020056, eta: 0.46809827081143146, phi: 3.3102578865897065, energy: 8537.647658241913\n",
      "pt: 10238.345103055317, eta: 0.03303687579097063, phi: 2.6342035723642225, energy: 10641.066587332636\n",
      "pt: 6186.828393142635, eta: -0.475483600599392, phi: 1.050966225029106, energy: 7102.424439295195\n",
      "pt: 9604.076385349339, eta: 0.21157241840866223, phi: 1.974158729185662, energy: 10182.87464039959\n",
      "pt: 11010.90941291513, eta: -0.11355788200544989, phi: 5.709051142140476, energy: 11492.516594120301\n",
      "pt: 13113.892030731304, eta: -0.050583384613786087, phi: 0.4695846505745282, energy: 13647.940537765622\n",
      "pt: 10611.376135452241, eta: 0.15274117813671012, phi: 1.2262468594961327, energy: 11127.580005363561\n",
      "pt: 10396.39843369989, eta: -0.2702408680694997, phi: 3.2589138072656456, energy: 11177.271638830192\n",
      "pt: 11465.510067157098, eta: 0.07065218829879058, phi: 4.738260394665854, energy: 11932.904760139063\n",
      "pt: 10257.80535744802, eta: 0.07811381656271008, phi: 3.9740429712591236, energy: 10679.974809641019\n",
      "Reconstructed Jets:\n",
      "pt: 0.20416347955164016, eta: 3.155292145743037, phi: 5.684541161407011, energy: 2.3991775512695312\n",
      "pt: 0.31138030683690704, eta: 3.6206829731013688, phi: 3.070517502112541, energy: 5.821230411529541\n",
      "pt: 0.31775151198517665, eta: -3.517900585552465, phi: 4.212239876747194, energy: 5.360982418060303\n",
      "pt: 0.3939027398602262, eta: 3.8125461301545527, phi: 2.5912081202073747, energy: 8.919462203979492\n",
      "pt: 0.4181950467101698, eta: -3.0254222162656528, phi: 3.172450416975602, energy: 4.318122863769531\n",
      "pt: 0.4316649231044551, eta: 3.7333694831191058, phi: 3.920656614720821, energy: 9.031228065490723\n",
      "pt: 0.5280648152190388, eta: 2.941756117790998, phi: 0.32216695836254344, energy: 5.01710844039917\n",
      "pt: 0.8068398777477905, eta: -2.828365174520989, phi: 5.673008007047438, energy: 6.849415570497513\n",
      "pt: 1.0068182756147563, eta: -3.6985524670526893, phi: 2.3519550034591297, energy: 20.344554901123047\n",
      "pt: 1.0452379353632164, eta: 2.93136135020348, phi: 6.13552213050226, energy: 9.829433254897594\n",
      "pt: 1.3266776044422015, eta: 3.648691077835622, phi: 2.0291726333978284, energy: 25.506523072719574\n",
      "pt: 2.4064561255665224, eta: -3.114368522651009, phi: 3.7517945271678332, energy: 27.153671741485596\n",
      "pt: 1.4813111268738828, eta: -3.6172093611863003, phi: 1.4832577492403094, energy: 27.597105026245117\n",
      "pt: 1.4849333136371077, eta: -1.341867704286553, phi: 1.3908990180856655, energy: 3.0348942279815674\n",
      "pt: 1.4864119504985596, eta: 2.748462042753795, phi: 5.290898061709581, energy: 11.655929207801819\n",
      "pt: 1.5625414851598376, eta: -2.8782146711953502, phi: 4.858872207068092, energy: 13.936911344528198\n",
      "pt: 1.5708618341590594, eta: 1.0780280191966483, phi: 0.08533512234315707, energy: 2.5755412578582764\n",
      "pt: 1.667864692370486, eta: 2.8723858039984145, phi: 1.8577036497772286, energy: 14.790833950042725\n",
      "pt: 1.6733170520991762, eta: 3.1903534484496854, phi: 2.1111332104173024, energy: 20.36273765563965\n",
      "pt: 1.7814025563086282, eta: -2.7869933755389584, phi: 2.549503226585691, energy: 14.51334822177887\n",
      "pt: 2.0265255059304006, eta: -2.380783974161823, phi: 1.174644005602626, energy: 11.051509439945221\n",
      "pt: 2.402320024920876, eta: 3.1832886592283174, phi: 1.4840408866827448, energy: 29.031585216522217\n",
      "pt: 3.3050360917496096, eta: -3.8073269587208016, phi: 0.7486901395356719, energy: 74.4495096206665\n",
      "pt: 5.650240717195371, eta: 2.9518863630613708, phi: 0.7357914203722226, energy: 54.24019744992256\n",
      "pt: 3.98073454937066, eta: -1.8836313469985353, phi: 4.78546902241378, energy: 13.394840266555548\n",
      "pt: 4.000969445117963, eta: 2.8425481133141166, phi: 4.4042851524329425, energy: 34.44734525680542\n",
      "pt: 4.245479953590659, eta: 2.7654200254972516, phi: 2.297115904075956, energy: 33.85772714018822\n",
      "pt: 4.381646900612858, eta: 2.826649968688816, phi: 4.811516052026037, energy: 37.13102698326111\n",
      "pt: 4.4784287407476215, eta: -2.7843393047352896, phi: 4.415748982291785, energy: 36.39079096913338\n",
      "pt: 5.4812278709110664, eta: -2.8615890148749954, phi: 2.1442457639104084, energy: 48.09371542930603\n",
      "pt: 8.664185024898346, eta: 0.10299901195271638, phi: 4.241317789218813, energy: 8.85216262564063\n",
      "pt: 5.801942500785189, eta: -2.743302418632556, phi: 0.08321627329405162, energy: 45.267477095127106\n",
      "pt: 6.889528398844281, eta: 1.311415525749485, phi: 0.8679641279303649, energy: 13.723436802625656\n",
      "pt: 8.657508413808758, eta: -2.4466430765194613, phi: 2.102490947118286, energy: 50.38780218362808\n",
      "pt: 9.537437183779085, eta: -2.838183604133035, phi: 1.480570274899383, energy: 81.77432650327682\n",
      "pt: 12.77825898652115, eta: -2.637068649323531, phi: 3.1515546368332292, energy: 89.76439929008484\n",
      "pt: 9.778689339381112, eta: 1.3471368969755202, phi: 2.841251182176881, energy: 20.07996779680252\n",
      "pt: 26.384386605823675, eta: -2.678695239439059, phi: 0.6920057340375766, energy: 193.1838474944234\n",
      "pt: 12.922325884408087, eta: -0.3406864315319929, phi: 6.1072501465540965, energy: 13.753579504787922\n",
      "pt: 14.484816058329216, eta: -2.678081508617503, phi: 5.232297804003471, energy: 105.9651475250721\n",
      "pt: 17.443726443675217, eta: 2.5113470773480233, phi: 1.2594341131658726, energy: 108.23400355130434\n",
      "pt: 18.460398744462715, eta: -0.08257939042496357, phi: 1.7414718397904623, energy: 18.7581844329834\n",
      "pt: 15.332785808721283, eta: 2.7362342232222914, phi: 3.3166724097261198, energy: 118.80627971887589\n",
      "pt: 16.042857209612773, eta: 0.499701165010203, phi: 2.857266383830366, energy: 18.090555384755135\n",
      "pt: 17.97465908904736, eta: 2.636870984202576, phi: 5.773015397026294, energy: 126.23441332578659\n",
      "pt: 18.206597302276517, eta: -0.6759897867229442, phi: 3.208256514679968, energy: 22.527995642274618\n",
      "pt: 19.66894846794331, eta: 0.5641384279222336, phi: 2.1671646908693623, energy: 22.985828831791878\n",
      "pt: 39.73474257281682, eta: -2.4350675629797585, phi: 5.771839843676772, energy: 228.75896343914792\n",
      "pt: 26.403237041075254, eta: 2.5574246843700577, phi: 3.776984366820822, energy: 171.44814869761467\n",
      "pt: 34.707038450522155, eta: 2.518577687369053, phi: 0.16449387574726818, energy: 216.91493900492787\n",
      "pt: 42.87559935806635, eta: 2.438222730755243, phi: 2.6852180731847395, energy: 247.61815011501312\n",
      "pt: 45.754974413736974, eta: -2.318846592076987, phi: 2.6656230336023405, energy: 235.0150535441935\n",
      "pt: 25.9048055493098, eta: 1.1146334910027051, phi: 3.4878964662583356, energy: 43.746839942614315\n",
      "pt: 26.208813030334625, eta: 2.361033028636308, phi: 4.757711065799654, energy: 140.24631145223975\n",
      "pt: 45.77364722213789, eta: -2.6089194279308114, phi: 3.906817394338937, energy: 312.76022097840905\n",
      "pt: 51.7632358556254, eta: -2.397018031125992, phi: 4.619973808987525, energy: 287.0428142827004\n",
      "pt: 36.62430023077419, eta: 0.10890418566701605, phi: 3.030929767920846, energy: 36.847128331661224\n",
      "pt: 71.739637756605, eta: 2.347098824999045, phi: 4.289382942490526, energy: 378.890711940825\n",
      "pt: 39.44161266961733, eta: -1.8257381471973722, phi: 1.3136355423115211, energy: 125.65737947821617\n",
      "pt: 43.843999245024136, eta: 2.441365227491635, phi: 1.9164137065438802, energy: 253.90971376001835\n",
      "pt: 102.718215722538, eta: 2.3113167961975813, phi: 5.212923004924185, energy: 523.8566961437464\n",
      "pt: 74.84348163970509, eta: -2.094076103281389, phi: 4.034048395191347, energy: 308.7769224345684\n",
      "pt: 86.22815583420673, eta: 2.16356631921512, phi: 5.90413638482283, energy: 380.8621189855039\n",
      "pt: 51.543883825353234, eta: 1.0357718307110566, phi: 2.1332081277479724, energy: 81.76778338849545\n",
      "pt: 60.65019829097153, eta: -1.1246282460311552, phi: 3.6237946345323024, energy: 103.26273021101952\n",
      "pt: 84.50156396395944, eta: -2.2477720000960075, phi: 3.445290328224433, energy: 405.02193641103804\n",
      "pt: 61.669367632215234, eta: 0.5547274164449001, phi: 1.2879986855830068, energy: 71.40941932797432\n",
      "pt: 81.56329055933527, eta: -2.247809675006951, phi: 0.1670386563645222, energy: 390.8049721494317\n",
      "pt: 68.42631242021895, eta: 1.6300599473484398, phi: 4.302119066293873, energy: 181.67122775316238\n",
      "pt: 146.26757032690708, eta: -2.2615440272984464, phi: 5.06692891975454, energy: 710.4993156045675\n",
      "pt: 81.2552463535602, eta: -1.7743747267539096, phi: 6.207932913073396, energy: 247.10009311884642\n",
      "pt: 72.2382924921292, eta: 0.021371569178833066, phi: 0.1783607116808828, energy: 72.26088017225266\n",
      "pt: 72.52665081877468, eta: 1.8719690642856355, phi: 0.33357988567319624, energy: 241.67498990893364\n",
      "pt: 89.70799263990853, eta: 1.9489037582844566, phi: 2.166545451769167, energy: 321.8084411416203\n",
      "pt: 77.28883914680105, eta: 0.06738176364086708, phi: 3.4743829456238284, energy: 77.4700366705656\n",
      "pt: 77.89428467075368, eta: -0.024369789270149794, phi: 5.004326321694204, energy: 78.15844186022878\n",
      "pt: 108.63839673096341, eta: 2.15451678882658, phi: 3.309831327057685, energy: 475.29239086806774\n",
      "pt: 154.55523313363574, eta: -2.275009570326887, phi: 1.5915481829543756, energy: 760.9083019644022\n",
      "pt: 130.3064497530392, eta: 2.2796274671886274, phi: 0.7558845433038527, energy: 644.0887079052627\n",
      "pt: 171.58958753905583, eta: -2.081413926902437, phi: 0.8112449525331366, energy: 699.7125793186715\n",
      "pt: 174.84490112777482, eta: 1.8367436161518296, phi: 2.6627000744427827, energy: 564.3855607409496\n",
      "pt: 201.3535278391687, eta: -1.9149328859824908, phi: 5.764316142643878, energy: 700.0459596281871\n",
      "pt: 183.80347977895374, eta: -1.9452116096604737, phi: 2.2589461507077817, energy: 657.6455971915275\n",
      "pt: 246.60742956186766, eta: 1.9030604392104955, phi: 3.862483196464514, energy: 847.454027841799\n",
      "pt: 362.35021833288727, eta: -1.7735374653317693, phi: 2.998837110523147, energy: 1101.5358930192888\n",
      "pt: 143.84393680251065, eta: -0.5283217266869797, phi: 5.348068846193515, energy: 164.407331392169\n",
      "pt: 178.61849478695564, eta: 1.926634847003592, phi: 1.5348093063048323, energy: 627.1473594978452\n",
      "pt: 349.9889492833052, eta: 1.6571107382289152, phi: 6.216325895499827, energy: 955.2986288773827\n",
      "pt: 369.81102009638585, eta: -1.6027620254329675, phi: 0.2897708221345048, energy: 960.7932788012549\n",
      "pt: 274.15964598210934, eta: 1.8165126780497354, phi: 0.8800506642078918, energy: 868.1887274831533\n",
      "pt: 315.84582813863904, eta: -1.7145671624929375, phi: 4.400339025309451, energy: 909.6866288883612\n",
      "pt: 444.92948127478735, eta: -1.545228873861781, phi: 1.7759301513457473, energy: 1097.4474570825696\n",
      "pt: 235.30307421793256, eta: -0.2363801133409693, phi: 2.74260004323334, energy: 242.51401269435883\n",
      "pt: 347.6108215099785, eta: 1.6913270591190215, phi: 4.757017628389579, energy: 978.6182138137519\n",
      "pt: 570.5900999251088, eta: 1.6578684657084537, phi: 5.45764070649951, energy: 1559.6603354895487\n",
      "pt: 587.8377120675275, eta: 1.4493555801997897, phi: 1.8870875020873288, energy: 1328.3501266605163\n",
      "pt: 405.20222337679724, eta: -1.6099098583765226, phi: 5.118179577916953, energy: 1058.7075827460503\n",
      "pt: 564.6124675924705, eta: 1.5491146075014717, phi: 3.2424025487966746, energy: 1395.8441972534638\n",
      "pt: 738.6100685840466, eta: -1.607840271945227, phi: 3.6261078204047896, energy: 1927.5377738880925\n",
      "pt: 797.8686352101696, eta: -1.4759024756040293, phi: 0.9991651453220212, energy: 1845.2297498881817\n",
      "pt: 878.4875718453691, eta: 1.347085313029813, phi: 2.4276950655738685, energy: 1816.2807900047046\n",
      "pt: 652.5139315086453, eta: -1.370804740399562, phi: 5.7239522471774436, energy: 1378.483330248855\n",
      "pt: 814.8509581839406, eta: 1.3295458881991946, phi: 3.8861791719250087, energy: 1661.6578884767368\n",
      "pt: 1244.9066920917735, eta: 1.322347884278812, phi: 1.2697894382615647, energy: 2522.0624315107707\n",
      "pt: 1289.1213217168104, eta: 1.285974886211011, phi: 0.44343057875476344, energy: 2533.000119634904\n",
      "pt: 1154.4805893277298, eta: -1.323549971166255, phi: 2.5674689431143767, energy: 2340.307169736596\n",
      "pt: 1636.1267060030807, eta: 0.9901276388990017, phi: 3.09254339756343, energy: 2543.27381287748\n",
      "pt: 1251.789574686449, eta: -1.1189393522220874, phi: 4.700116643886465, energy: 2138.4381223535165\n",
      "pt: 1352.152182962969, eta: 1.1304837115712956, phi: 5.933106001612677, energy: 2335.3626136891544\n",
      "pt: 1277.401954530693, eta: 1.1706310162248825, phi: 4.576164699648767, energy: 2281.9132862240076\n",
      "pt: 2266.3572047842404, eta: -0.9970511538183097, phi: 2.062847427679098, energy: 3538.9589156769216\n",
      "pt: 2248.487145479653, eta: -1.1859367465373531, phi: 4.095794403338208, energy: 4067.384697624948\n",
      "pt: 2060.5358904243344, eta: 1.1225813402747093, phi: 5.16615084641493, energy: 3544.8021536506712\n",
      "pt: 1644.5433769681392, eta: -1.1263391738688175, phi: 3.1575907776381342, energy: 2833.0157105252147\n",
      "pt: 2258.241225686813, eta: -1.1051667927349922, phi: 6.209459360486736, energy: 3827.1036033155397\n",
      "pt: 2265.122333526709, eta: -0.9728770439654133, phi: 0.6801785969078272, energy: 3463.9226494431496\n",
      "pt: 4318.574531391955, eta: 0.787248855507322, phi: 0.9017898603149938, energy: 5841.984874212416\n",
      "pt: 3865.246983946624, eta: 0.8114271215444528, phi: 2.492798417060049, energy: 5310.056202497333\n",
      "pt: 2907.675894530874, eta: 0.7809873796892589, phi: 3.7497218134136, energy: 3905.877052657306\n",
      "pt: 1824.5904422996405, eta: -0.9565411598371449, phi: 1.3357557374255182, energy: 2764.3925444744527\n",
      "pt: 1851.2618521766576, eta: -1.0080343548639157, phi: 5.2964194278868595, energy: 2911.4605101567577\n",
      "pt: 3634.559161299478, eta: 0.5635697100133481, phi: 5.684765174050491, energy: 4308.9257347323\n",
      "pt: 4929.759960923472, eta: 0.8146099526543568, phi: 1.6317941551607809, energy: 6820.6843334421865\n",
      "pt: 5774.975643301499, eta: -0.5610982674283246, phi: 4.926769464257091, energy: 6863.456168191042\n",
      "pt: 4160.239869000128, eta: -0.6840848635908355, phi: 2.7924705199103044, energy: 5274.3109079813585\n",
      "pt: 3847.776815294573, eta: -0.6962663264289016, phi: 3.626002091112301, energy: 4922.070498378482\n",
      "pt: 4944.205545910037, eta: 0.6837505466593699, phi: 0.14749356934516689, energy: 6270.924432663713\n",
      "pt: 6020.949723236277, eta: -0.6067165328489832, phi: 5.778851523163146, energy: 7333.707400657237\n",
      "pt: 6000.245109065281, eta: 0.6675601065467108, phi: 4.316212885266932, energy: 7564.719322584569\n",
      "pt: 6845.8300051511505, eta: -0.45687551076967714, phi: 1.620261493273122, energy: 7765.7960439659655\n",
      "pt: 6525.81145496709, eta: 0.5177877114649209, phi: 5.092526730702032, energy: 7635.935745276045\n",
      "pt: 6891.324333773707, eta: -0.5420400198891958, phi: 4.230430809600353, energy: 8138.228597630514\n",
      "pt: 8333.845196467653, eta: -0.40685796414783426, phi: 2.2467191121039267, energy: 9313.046277865767\n",
      "pt: 7128.307385118435, eta: 0.4673672962070746, phi: 3.2782449072326414, energy: 8145.292960772291\n",
      "pt: 9930.653349584247, eta: 0.1374359267855017, phi: 6.062746249107331, energy: 10392.220570208505\n",
      "pt: 8796.59859451159, eta: 0.2585725734377963, phi: 1.908822126973509, energy: 9384.892317419872\n",
      "pt: 6602.378993320294, eta: -0.43640004252227727, phi: 0.9461248120773624, energy: 7443.911347246729\n",
      "pt: 10294.112874447577, eta: 0.19150703597171082, phi: 0.5539690632164158, energy: 10845.833764849813\n",
      "pt: 10037.262995635623, eta: -0.2628769281541984, phi: 3.2301554265721255, energy: 10744.713762275875\n",
      "pt: 10530.178031147998, eta: 0.1563038585557787, phi: 1.2386041871895048, energy: 11044.523987096385\n",
      "pt: 11100.116176446296, eta: -0.12242163496004137, phi: 5.403536649104562, energy: 11603.609700151952\n",
      "pt: 9708.389197910694, eta: 0.04903010629954704, phi: 3.878806278643656, energy: 10064.035970325116\n",
      "pt: 12127.259803404128, eta: 0.07900940631770653, phi: 4.61615044222563, energy: 12632.18287890777\n",
      "pt: 10928.806138985261, eta: -0.4233186351545351, phi: 0.23342047571416155, energy: 12400.355923021853\n",
      "pt: 12403.670988851329, eta: 0.16436293276936312, phi: 2.6031515900168394, energy: 13039.012621906586\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated Jets:\")\n",
    "for gen_jet in gen_sequence.inclusive_jets(0.0):\n",
    "    print(f\"pt: {gen_jet.pt()}, eta: {gen_jet.eta()}, phi: {gen_jet.phi()}, energy: {gen_jet.e()}\")\n",
    "\n",
    "print(\"Reconstructed Jets:\")\n",
    "for reco_jet in reco_sequence.inclusive_jets(0.0):\n",
    "    print(f\"pt: {reco_jet.pt()}, eta: {reco_jet.eta()}, phi: {reco_jet.phi()}, energy: {reco_jet.e()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "006130d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Jets:\n",
      "pt: 0.03020368565901634, eta: 3.7280091704607465, phi: 2.4162073023779485, energy: 0.6285425424575806\n",
      "pt: 0.041213055186533565, eta: 3.6823271924490286, phi: 5.632884390744783, energy: 0.8193965554237366\n",
      "pt: 0.06140570013828287, eta: 2.7939994599454585, phi: 1.9547844026799632, energy: 0.5037552118301392\n",
      "pt: 0.07460956599779874, eta: 3.8328169847968794, phi: 2.9440343454298223, energy: 1.7240079641342163\n",
      "pt: 0.1359516602608681, eta: -3.7415554085838427, phi: 0.2199765818587965, energy: 2.8677124977111816\n",
      "pt: 0.23345986844838232, eta: -3.972297975901402, phi: 2.4378801349365395, energy: 6.201436519622803\n",
      "pt: 0.26007208360622347, eta: -3.6011016351425695, phi: 5.662033049248008, energy: 4.767884254455566\n",
      "pt: 0.34856613606234244, eta: 3.793054002827001, phi: 1.064596353851168, energy: 7.740810990333557\n",
      "pt: 0.46706827462363676, eta: -3.369541212754246, phi: 1.852128704866259, energy: 6.795919299125671\n",
      "pt: 0.644585343224867, eta: 3.1326097717337107, phi: 5.090353568373526, energy: 7.406161904335022\n",
      "pt: 0.7239599766799867, eta: 3.3949127680203888, phi: 3.128959112798766, energy: 10.80393236875534\n",
      "pt: 0.7581859391071882, eta: 2.0683439759115885, phi: 5.490038589663468, energy: 3.0472491979599\n",
      "pt: 0.8479968523106942, eta: 3.082253111475849, phi: 6.254441114325361, energy: 9.265966355800629\n",
      "pt: 0.908156185616182, eta: -1.5763815993337154, phi: 4.30627128591155, energy: 2.290465831756592\n",
      "pt: 0.9844970779903413, eta: 3.609858959514952, phi: 0.6101412620989063, energy: 18.207237243652344\n",
      "pt: 1.3617161436432368, eta: 3.4094566798247583, phi: 3.753918658967614, energy: 20.619189023971558\n",
      "pt: 1.2810422112200157, eta: -3.5460595846574314, phi: 2.650239350921129, energy: 22.22963047027588\n",
      "pt: 1.4862849614918237, eta: -3.1910042034442427, phi: 3.1033847043312397, energy: 18.098927855491638\n",
      "pt: 1.6244506866491701, eta: 3.2067285139959365, phi: 0.9785801291966034, energy: 20.094103693962097\n",
      "pt: 2.5572518164955014, eta: -3.0998181730612075, phi: 4.218877483234335, energy: 28.43948793411255\n",
      "pt: 1.874348279464016, eta: 2.6489517722734544, phi: 1.032534915282694, energy: 13.318550311028957\n",
      "pt: 2.1995341611367154, eta: -3.152406397970336, phi: 5.365396157020576, energy: 25.773533821105957\n",
      "pt: 2.368085001560902, eta: -2.8017974875354215, phi: 3.6388095267542973, energy: 19.578278571367264\n",
      "pt: 3.4629139661716533, eta: 3.271878551374314, phi: 2.5557554157507325, energy: 45.714521288871765\n",
      "pt: 2.5762436958190498, eta: 2.3976147599693785, phi: 3.8540732780210907, energy: 14.287043452262878\n",
      "pt: 2.9911019019005933, eta: 3.2318187234130926, phi: 1.7656300093067747, energy: 37.93750214576721\n",
      "pt: 3.1428146466048674, eta: -3.529066005673823, phi: 5.035492139632099, energy: 53.621257305145264\n",
      "pt: 3.317610026283287, eta: -0.09491911022912067, phi: 4.342622736046442, energy: 3.3327399492263794\n",
      "pt: 3.5017244430591488, eta: -3.216133162710474, phi: 1.273489935422533, energy: 43.72295928001404\n",
      "pt: 3.620916856129804, eta: -0.86428641926975, phi: 1.142042776363067, energy: 5.060023546218872\n",
      "pt: 3.7677009011742304, eta: 2.7473054250665645, phi: 2.444088739206873, energy: 29.515800148248672\n",
      "pt: 4.14324144909995, eta: -2.7602128052001103, phi: 2.432161890766391, energy: 32.87866607308388\n",
      "pt: 4.320380467308597, eta: -3.2539133147958896, phi: 0.619109302362057, energy: 56.01768624037504\n",
      "pt: 4.063948600733234, eta: 1.3203386845184382, phi: 5.7217899260254015, energy: 8.152903713285923\n",
      "pt: 7.669650731912021, eta: 2.908378612370005, phi: 4.252071292955066, energy: 70.5099390745163\n",
      "pt: 4.558301727747785, eta: -3.15212767335532, phi: 2.3640122799988585, energy: 53.39875781536102\n",
      "pt: 5.396780612307994, eta: -2.823788307827244, phi: 5.896697894352357, energy: 45.61514401435852\n",
      "pt: 4.796714340829913, eta: 1.9655420438946078, phi: 2.414872969525708, energy: 17.45831759274006\n",
      "pt: 8.073055367504422, eta: -2.7038883279496226, phi: 0.3188521515590127, energy: 60.58425206691027\n",
      "pt: 8.353043384171418, eta: -2.3791045077690893, phi: 2.1307067700841804, energy: 45.495845437049866\n",
      "pt: 9.763635805360956, eta: 2.7203455047628737, phi: 0.5540473721116403, energy: 74.48127511516213\n",
      "pt: 9.145174662471097, eta: 2.5749047607060267, phi: 5.946448430419504, energy: 60.39582351595163\n",
      "pt: 11.281434064858635, eta: -2.7350464079692367, phi: 1.611451186932923, energy: 87.31252222135663\n",
      "pt: 10.876143878146426, eta: -2.3713659866322443, phi: 1.1543498782460624, energy: 58.77904412150383\n",
      "pt: 12.145789544822543, eta: 2.6892938156952266, phi: 4.897350762881345, energy: 89.83391526341438\n",
      "pt: 22.817874577558133, eta: -2.599245645244043, phi: 3.0713689120811796, energy: 154.44061335362494\n",
      "pt: 23.041904497000704, eta: 2.580272570304257, phi: 1.4970287513187517, energy: 153.06113049387932\n",
      "pt: 13.278892682246498, eta: -1.8757335370053796, phi: 5.84024670376764, energy: 44.349455088377\n",
      "pt: 16.19900684578416, eta: -2.680775967801694, phi: 0.8304647639590724, energy: 118.81891445070505\n",
      "pt: 15.98421113592986, eta: -2.7390089946250944, phi: 5.077063467270499, energy: 124.19641342759132\n",
      "pt: 31.213282404320996, eta: 2.7129450101463055, phi: 3.5134830165549507, energy: 236.40124443173409\n",
      "pt: 33.98608857267263, eta: 2.5669058461583782, phi: 5.513631814462791, energy: 222.7812023870647\n",
      "pt: 26.877565420815586, eta: -2.6329719129698246, phi: 4.305186843764543, energy: 188.05482110381126\n",
      "pt: 41.72617114059412, eta: 2.4571878303077326, phi: 0.11192584036303387, energy: 245.4861056227237\n",
      "pt: 40.50875428611645, eta: 2.446917220801267, phi: 2.824159271301794, energy: 235.92112332955003\n",
      "pt: 40.32804987064846, eta: -2.1333696995421474, phi: 4.335701954453538, energy: 172.86842219531536\n",
      "pt: 78.68589656096208, eta: 2.3770139084078417, phi: 4.318883047875701, energy: 428.0168091561645\n",
      "pt: 105.15753267543474, eta: 2.2267471531197502, phi: 5.046595073465449, energy: 493.9313230365515\n",
      "pt: 71.98491165064598, eta: -2.3787288485787763, phi: 5.577065004822945, energy: 392.18354407697916\n",
      "pt: 61.979742256145826, eta: -2.3486828561370157, phi: 6.269848502255347, energy: 327.88961150869727\n",
      "pt: 40.7377877954502, eta: -0.186726630787628, phi: 2.0996059632345614, energy: 41.45327627658844\n",
      "pt: 76.87058152744135, eta: -2.4194724536685484, phi: 3.747716632026855, energy: 435.7483301013708\n",
      "pt: 98.84415162631927, eta: 2.090440848153245, phi: 5.92411533611901, energy: 406.71873797662556\n",
      "pt: 76.07000788985873, eta: 2.364952768158661, phi: 2.139660155522495, energy: 408.80790074169636\n",
      "pt: 60.04430852850651, eta: 1.2497519951699032, phi: 3.7703098930122803, energy: 113.63504142127931\n",
      "pt: 109.39812281012355, eta: -2.1582954608525915, phi: 2.476333390545626, energy: 480.52560517191887\n",
      "pt: 102.78096103279478, eta: 0.2460728002752081, phi: 2.986844068541714, energy: 107.28384473919868\n",
      "pt: 131.90400085710087, eta: -2.2602726899306456, phi: 4.8538264948281284, energy: 639.9136555716395\n",
      "pt: 74.63360624736781, eta: 0.35683702832166647, phi: 5.799876117108143, energy: 79.55015363544226\n",
      "pt: 120.81326498814177, eta: -2.1575247368164954, phi: 0.6600560252383059, energy: 530.20026306808\n",
      "pt: 136.4933380773694, eta: -2.2825429572270255, phi: 1.6504089926555672, energy: 676.6681511551142\n",
      "pt: 87.9372174040825, eta: -0.05533788064143445, phi: 6.1561829476058065, energy: 88.27112060785294\n",
      "pt: 92.77005660417974, eta: 0.7864434765821569, phi: 2.97757136579591, energy: 123.0101840943098\n",
      "pt: 107.33826197026566, eta: 2.178707648598991, phi: 3.4442623579075584, energy: 480.66923436522484\n",
      "pt: 135.51853078924057, eta: -2.056581459307834, phi: 3.1913630588613353, energy: 539.3835045844316\n",
      "pt: 166.07676702067036, eta: 2.247620502662403, phi: 0.825499865088077, energy: 795.8022234225646\n",
      "pt: 217.03648894277336, eta: -1.841001319888501, phi: 5.381278148324077, energy: 703.214707262814\n",
      "pt: 110.9318385290589, eta: 0.753569437395093, phi: 5.102504826537994, energy: 144.07766799628735\n",
      "pt: 176.01552973944507, eta: 1.9678176480855796, phi: 1.527237284144311, energy: 642.9302250165492\n",
      "pt: 145.01446839473422, eta: 1.9190510456076348, phi: 2.8410402693335413, energy: 506.0154401399195\n",
      "pt: 188.0643239554135, eta: 1.826016290847293, phi: 0.32655031411934254, energy: 600.2142157405615\n",
      "pt: 151.901307849578, eta: -1.5171436322531848, phi: 1.5530159909315844, energy: 363.8872636780143\n",
      "pt: 228.44126325324518, eta: -1.846473450720221, phi: 0.015175805602263967, energy: 743.9667317345738\n",
      "pt: 142.96919811044606, eta: 0.23289538167256094, phi: 0.8276176588033106, energy: 147.50477576255798\n",
      "pt: 242.4554184017136, eta: -1.832501279537589, phi: 1.1530313885873946, energy: 779.6006154716015\n",
      "pt: 145.02199861815453, eta: -0.4192686902142985, phi: 2.844365667710213, energy: 158.08545370399952\n",
      "pt: 237.09721875074294, eta: 1.8650793778640917, phi: 3.969235395995393, energy: 785.8389273863286\n",
      "pt: 153.685194162387, eta: 0.6268500438339308, phi: 2.1476072996758457, energy: 185.0183229893446\n",
      "pt: 453.26919962587874, eta: -1.5998792455618223, phi: 2.8576804273944956, energy: 1173.3142842203379\n",
      "pt: 172.4127614964974, eta: -0.0590993172140617, phi: 1.5851947256066783, energy: 173.70215873420238\n",
      "pt: 343.52122903030954, eta: -1.6961632448788027, phi: 2.0534472942114133, energy: 972.2577774189413\n",
      "pt: 344.4730485871279, eta: -1.7921374069568576, phi: 3.9377994700486836, energy: 1065.703159284778\n",
      "pt: 202.6094555816797, eta: 0.08385482449280914, phi: 3.5167763010139086, energy: 203.41097936034203\n",
      "pt: 538.2685051967584, eta: 1.6387884526910843, phi: 3.4463195160037827, energy: 1446.4874731451273\n",
      "pt: 449.2684596961638, eta: 1.6858445148126175, phi: 2.1198503414673846, energy: 1258.5972914248705\n",
      "pt: 384.9574545401857, eta: 1.665977707550134, phi: 4.649714649394035, energy: 1058.6474800147116\n",
      "pt: 237.8472402371721, eta: 0.3338303986509825, phi: 4.299451139104619, energy: 251.83988539129496\n",
      "pt: 508.2800540754491, eta: -1.546136722436749, phi: 4.7086215463890815, energy: 1252.3916500769556\n",
      "pt: 258.12726876090625, eta: 0.006960961476076055, phi: 5.240048404885021, energy: 258.6122817248106\n",
      "pt: 571.19029799824, eta: 1.68516079332797, phi: 5.407070013740083, energy: 1600.6081031672657\n",
      "pt: 377.17208125201995, eta: -1.531288512882113, phi: 0.513880603755174, energy: 917.4084363281727\n",
      "pt: 395.4362606778551, eta: 1.6392857721086236, phi: 0.9113707311058107, energy: 1061.2888747937977\n",
      "pt: 857.9062952049413, eta: 1.277823845589759, phi: 2.7152056798099964, energy: 1670.2502471283078\n",
      "pt: 352.865925450341, eta: -0.42413663564646975, phi: 4.704782679352437, energy: 385.7157298102975\n",
      "pt: 363.1088255497703, eta: -0.2896819166475964, phi: 3.7327950239908696, energy: 379.372773591429\n",
      "pt: 1000.9668411852548, eta: 1.3928044480905635, phi: 1.4323730896245024, energy: 2153.848368080333\n",
      "pt: 1060.0359490102865, eta: -1.4154514370230769, phi: 3.393967213899768, energy: 2330.0245737694204\n",
      "pt: 697.3826896846673, eta: 1.5116717861397526, phi: 6.1411693402104754, energy: 1668.6409811638296\n",
      "pt: 1069.8826258873096, eta: 1.2261237466562223, phi: 4.203354451069046, energy: 1996.870683092624\n",
      "pt: 1321.3441312821335, eta: -1.2687819564249359, phi: 0.9868457652181241, energy: 2553.217770021409\n",
      "pt: 599.0672820963226, eta: -1.0203342602468721, phi: 2.0191214128774324, energy: 946.9801127500832\n",
      "pt: 589.8838465704349, eta: -1.459012471559711, phi: 5.826539448392306, energy: 1344.4194092452526\n",
      "pt: 1242.5907591054763, eta: 1.237163784030772, phi: 0.527474777267379, energy: 2339.77345290035\n",
      "pt: 1585.7407673666294, eta: -1.2146331010118068, phi: 2.4648617946168674, energy: 2936.5995191298425\n",
      "pt: 1146.7997714424976, eta: -1.1825128639946092, phi: 5.265658536267442, energy: 2064.049353610724\n",
      "pt: 1958.1887602248796, eta: 1.0811810936942874, phi: 3.3178341106682647, energy: 3262.059158688411\n",
      "pt: 1620.4136805176936, eta: 1.1832653307495449, phi: 4.922278728370695, energy: 2922.8669020030648\n",
      "pt: 1750.905866702392, eta: -1.1270440204462546, phi: 0.009968297856903622, energy: 3012.6561023667455\n",
      "pt: 1571.6583087275178, eta: 1.1389162236335277, phi: 2.125420672940545, energy: 2734.7205830290914\n",
      "pt: 2234.6636837974065, eta: 0.9799357766618524, phi: 5.53966721236602, energy: 3461.330130495131\n",
      "pt: 1852.0292994575539, eta: -1.2305150754939078, phi: 4.110782557081986, energy: 3470.7807553038\n",
      "pt: 3213.3738851405087, eta: -0.7463830280293259, phi: 3.732884015340102, energy: 4228.420095553622\n",
      "pt: 2419.8474846754743, eta: 0.9268126820336295, phi: 6.194309030652654, energy: 3595.0166705548763\n",
      "pt: 2588.503612003807, eta: 0.8962756742261404, phi: 1.0798741364701954, energy: 3747.4263355359435\n",
      "pt: 2689.894858172474, eta: -0.9098383205508477, phi: 4.757793280406085, energy: 3939.2793707847595\n",
      "pt: 3157.475853625869, eta: -0.8033271436596167, phi: 0.5902086946664071, energy: 4299.239374706522\n",
      "pt: 2215.6542220594647, eta: -0.9311539113406918, phi: 1.5754731542706504, energy: 3295.21067949757\n",
      "pt: 4100.788772308981, eta: 0.7235790028479877, phi: 3.925543395206473, energy: 5324.993952491321\n",
      "pt: 6364.557076609164, eta: 0.5997922773406816, phi: 0.5941933328738263, energy: 7722.318162115291\n",
      "pt: 3485.159446887768, eta: -0.8448157158815294, phi: 3.037287595568959, energy: 4888.5359722189605\n",
      "pt: 6761.796500638245, eta: 0.42527691727582495, phi: 5.388746488595332, energy: 7577.4117170181125\n",
      "pt: 4788.451078153027, eta: 0.6934628269505573, phi: 2.5767400534618026, energy: 6118.86471830681\n",
      "pt: 6786.255350425702, eta: -0.5279956207980702, phi: 4.26421899432742, energy: 7968.474101960659\n",
      "pt: 6177.395681936448, eta: -0.546194101885408, phi: 2.430395853152096, energy: 7314.859667682089\n",
      "pt: 3860.932364538499, eta: -0.7868745896026452, phi: 5.72487790419937, energy: 5199.892158079892\n",
      "pt: 6508.671507140644, eta: -0.5095941386116499, phi: 0.021433604800481468, energy: 7589.562135562301\n",
      "pt: 7813.313954339559, eta: -0.4423234360484657, phi: 1.771283591347223, energy: 8838.453766923398\n",
      "pt: 6404.0249473628455, eta: 0.6647036731402063, phi: 4.618665438931171, energy: 8060.879396040924\n",
      "pt: 8850.906819955264, eta: 0.36578662760832126, phi: 6.21370119143681, energy: 9739.32028208673\n",
      "pt: 7634.268139641362, eta: -0.44904492232211884, phi: 5.1608252102623675, energy: 8660.404904881492\n",
      "pt: 5270.879712342151, eta: 0.7548658277229283, phi: 1.6088076312930697, energy: 7008.082444727421\n",
      "pt: 7463.597079020056, eta: 0.46809827081143146, phi: 3.3102578865897065, energy: 8537.647658241913\n",
      "pt: 10238.345103055317, eta: 0.03303687579097063, phi: 2.6342035723642225, energy: 10641.066587332636\n",
      "pt: 6186.828393142635, eta: -0.475483600599392, phi: 1.050966225029106, energy: 7102.424439295195\n",
      "pt: 9604.076385349339, eta: 0.21157241840866223, phi: 1.974158729185662, energy: 10182.87464039959\n",
      "pt: 11010.90941291513, eta: -0.11355788200544989, phi: 5.709051142140476, energy: 11492.516594120301\n",
      "pt: 13113.892030731304, eta: -0.050583384613786087, phi: 0.4695846505745282, energy: 13647.940537765622\n",
      "pt: 10611.376135452241, eta: 0.15274117813671012, phi: 1.2262468594961327, energy: 11127.580005363561\n",
      "pt: 10396.39843369989, eta: -0.2702408680694997, phi: 3.2589138072656456, energy: 11177.271638830192\n",
      "pt: 11465.510067157098, eta: 0.07065218829879058, phi: 4.738260394665854, energy: 11932.904760139063\n",
      "pt: 10257.80535744802, eta: 0.07811381656271008, phi: 3.9740429712591236, energy: 10679.974809641019\n",
      "Reconstructed Jets:\n",
      "pt: 0.20416347955164016, eta: 3.155292145743037, phi: 5.684541161407011, energy: 2.3991775512695312\n",
      "pt: 0.31138030683690704, eta: 3.6206829731013688, phi: 3.070517502112541, energy: 5.821230411529541\n",
      "pt: 0.31775151198517665, eta: -3.517900585552465, phi: 4.212239876747194, energy: 5.360982418060303\n",
      "pt: 0.3939027398602262, eta: 3.8125461301545527, phi: 2.5912081202073747, energy: 8.919462203979492\n",
      "pt: 0.4181950467101698, eta: -3.0254222162656528, phi: 3.172450416975602, energy: 4.318122863769531\n",
      "pt: 0.4316649231044551, eta: 3.7333694831191058, phi: 3.920656614720821, energy: 9.031228065490723\n",
      "pt: 0.5280648152190388, eta: 2.941756117790998, phi: 0.32216695836254344, energy: 5.01710844039917\n",
      "pt: 0.8068398777477905, eta: -2.828365174520989, phi: 5.673008007047438, energy: 6.849415570497513\n",
      "pt: 1.0068182756147563, eta: -3.6985524670526893, phi: 2.3519550034591297, energy: 20.344554901123047\n",
      "pt: 1.0452379353632164, eta: 2.93136135020348, phi: 6.13552213050226, energy: 9.829433254897594\n",
      "pt: 1.3266776044422015, eta: 3.648691077835622, phi: 2.0291726333978284, energy: 25.506523072719574\n",
      "pt: 2.4064561255665224, eta: -3.114368522651009, phi: 3.7517945271678332, energy: 27.153671741485596\n",
      "pt: 1.4813111268738828, eta: -3.6172093611863003, phi: 1.4832577492403094, energy: 27.597105026245117\n",
      "pt: 1.4849333136371077, eta: -1.341867704286553, phi: 1.3908990180856655, energy: 3.0348942279815674\n",
      "pt: 1.4864119504985596, eta: 2.748462042753795, phi: 5.290898061709581, energy: 11.655929207801819\n",
      "pt: 1.5625414851598376, eta: -2.8782146711953502, phi: 4.858872207068092, energy: 13.936911344528198\n",
      "pt: 1.5708618341590594, eta: 1.0780280191966483, phi: 0.08533512234315707, energy: 2.5755412578582764\n",
      "pt: 1.667864692370486, eta: 2.8723858039984145, phi: 1.8577036497772286, energy: 14.790833950042725\n",
      "pt: 1.6733170520991762, eta: 3.1903534484496854, phi: 2.1111332104173024, energy: 20.36273765563965\n",
      "pt: 1.7814025563086282, eta: -2.7869933755389584, phi: 2.549503226585691, energy: 14.51334822177887\n",
      "pt: 2.0265255059304006, eta: -2.380783974161823, phi: 1.174644005602626, energy: 11.051509439945221\n",
      "pt: 2.402320024920876, eta: 3.1832886592283174, phi: 1.4840408866827448, energy: 29.031585216522217\n",
      "pt: 3.3050360917496096, eta: -3.8073269587208016, phi: 0.7486901395356719, energy: 74.4495096206665\n",
      "pt: 5.650240717195371, eta: 2.9518863630613708, phi: 0.7357914203722226, energy: 54.24019744992256\n",
      "pt: 3.98073454937066, eta: -1.8836313469985353, phi: 4.78546902241378, energy: 13.394840266555548\n",
      "pt: 4.000969445117963, eta: 2.8425481133141166, phi: 4.4042851524329425, energy: 34.44734525680542\n",
      "pt: 4.245479953590659, eta: 2.7654200254972516, phi: 2.297115904075956, energy: 33.85772714018822\n",
      "pt: 4.381646900612858, eta: 2.826649968688816, phi: 4.811516052026037, energy: 37.13102698326111\n",
      "pt: 4.4784287407476215, eta: -2.7843393047352896, phi: 4.415748982291785, energy: 36.39079096913338\n",
      "pt: 5.4812278709110664, eta: -2.8615890148749954, phi: 2.1442457639104084, energy: 48.09371542930603\n",
      "pt: 8.664185024898346, eta: 0.10299901195271638, phi: 4.241317789218813, energy: 8.85216262564063\n",
      "pt: 5.801942500785189, eta: -2.743302418632556, phi: 0.08321627329405162, energy: 45.267477095127106\n",
      "pt: 6.889528398844281, eta: 1.311415525749485, phi: 0.8679641279303649, energy: 13.723436802625656\n",
      "pt: 8.657508413808758, eta: -2.4466430765194613, phi: 2.102490947118286, energy: 50.38780218362808\n",
      "pt: 9.537437183779085, eta: -2.838183604133035, phi: 1.480570274899383, energy: 81.77432650327682\n",
      "pt: 12.77825898652115, eta: -2.637068649323531, phi: 3.1515546368332292, energy: 89.76439929008484\n",
      "pt: 9.778689339381112, eta: 1.3471368969755202, phi: 2.841251182176881, energy: 20.07996779680252\n",
      "pt: 26.384386605823675, eta: -2.678695239439059, phi: 0.6920057340375766, energy: 193.1838474944234\n",
      "pt: 12.922325884408087, eta: -0.3406864315319929, phi: 6.1072501465540965, energy: 13.753579504787922\n",
      "pt: 14.484816058329216, eta: -2.678081508617503, phi: 5.232297804003471, energy: 105.9651475250721\n",
      "pt: 17.443726443675217, eta: 2.5113470773480233, phi: 1.2594341131658726, energy: 108.23400355130434\n",
      "pt: 18.460398744462715, eta: -0.08257939042496357, phi: 1.7414718397904623, energy: 18.7581844329834\n",
      "pt: 15.332785808721283, eta: 2.7362342232222914, phi: 3.3166724097261198, energy: 118.80627971887589\n",
      "pt: 16.042857209612773, eta: 0.499701165010203, phi: 2.857266383830366, energy: 18.090555384755135\n",
      "pt: 17.97465908904736, eta: 2.636870984202576, phi: 5.773015397026294, energy: 126.23441332578659\n",
      "pt: 18.206597302276517, eta: -0.6759897867229442, phi: 3.208256514679968, energy: 22.527995642274618\n",
      "pt: 19.66894846794331, eta: 0.5641384279222336, phi: 2.1671646908693623, energy: 22.985828831791878\n",
      "pt: 39.73474257281682, eta: -2.4350675629797585, phi: 5.771839843676772, energy: 228.75896343914792\n",
      "pt: 26.403237041075254, eta: 2.5574246843700577, phi: 3.776984366820822, energy: 171.44814869761467\n",
      "pt: 34.707038450522155, eta: 2.518577687369053, phi: 0.16449387574726818, energy: 216.91493900492787\n",
      "pt: 42.87559935806635, eta: 2.438222730755243, phi: 2.6852180731847395, energy: 247.61815011501312\n",
      "pt: 45.754974413736974, eta: -2.318846592076987, phi: 2.6656230336023405, energy: 235.0150535441935\n",
      "pt: 25.9048055493098, eta: 1.1146334910027051, phi: 3.4878964662583356, energy: 43.746839942614315\n",
      "pt: 26.208813030334625, eta: 2.361033028636308, phi: 4.757711065799654, energy: 140.24631145223975\n",
      "pt: 45.77364722213789, eta: -2.6089194279308114, phi: 3.906817394338937, energy: 312.76022097840905\n",
      "pt: 51.7632358556254, eta: -2.397018031125992, phi: 4.619973808987525, energy: 287.0428142827004\n",
      "pt: 36.62430023077419, eta: 0.10890418566701605, phi: 3.030929767920846, energy: 36.847128331661224\n",
      "pt: 71.739637756605, eta: 2.347098824999045, phi: 4.289382942490526, energy: 378.890711940825\n",
      "pt: 39.44161266961733, eta: -1.8257381471973722, phi: 1.3136355423115211, energy: 125.65737947821617\n",
      "pt: 43.843999245024136, eta: 2.441365227491635, phi: 1.9164137065438802, energy: 253.90971376001835\n",
      "pt: 102.718215722538, eta: 2.3113167961975813, phi: 5.212923004924185, energy: 523.8566961437464\n",
      "pt: 74.84348163970509, eta: -2.094076103281389, phi: 4.034048395191347, energy: 308.7769224345684\n",
      "pt: 86.22815583420673, eta: 2.16356631921512, phi: 5.90413638482283, energy: 380.8621189855039\n",
      "pt: 51.543883825353234, eta: 1.0357718307110566, phi: 2.1332081277479724, energy: 81.76778338849545\n",
      "pt: 60.65019829097153, eta: -1.1246282460311552, phi: 3.6237946345323024, energy: 103.26273021101952\n",
      "pt: 84.50156396395944, eta: -2.2477720000960075, phi: 3.445290328224433, energy: 405.02193641103804\n",
      "pt: 61.669367632215234, eta: 0.5547274164449001, phi: 1.2879986855830068, energy: 71.40941932797432\n",
      "pt: 81.56329055933527, eta: -2.247809675006951, phi: 0.1670386563645222, energy: 390.8049721494317\n",
      "pt: 68.42631242021895, eta: 1.6300599473484398, phi: 4.302119066293873, energy: 181.67122775316238\n",
      "pt: 146.26757032690708, eta: -2.2615440272984464, phi: 5.06692891975454, energy: 710.4993156045675\n",
      "pt: 81.2552463535602, eta: -1.7743747267539096, phi: 6.207932913073396, energy: 247.10009311884642\n",
      "pt: 72.2382924921292, eta: 0.021371569178833066, phi: 0.1783607116808828, energy: 72.26088017225266\n",
      "pt: 72.52665081877468, eta: 1.8719690642856355, phi: 0.33357988567319624, energy: 241.67498990893364\n",
      "pt: 89.70799263990853, eta: 1.9489037582844566, phi: 2.166545451769167, energy: 321.8084411416203\n",
      "pt: 77.28883914680105, eta: 0.06738176364086708, phi: 3.4743829456238284, energy: 77.4700366705656\n",
      "pt: 77.89428467075368, eta: -0.024369789270149794, phi: 5.004326321694204, energy: 78.15844186022878\n",
      "pt: 108.63839673096341, eta: 2.15451678882658, phi: 3.309831327057685, energy: 475.29239086806774\n",
      "pt: 154.55523313363574, eta: -2.275009570326887, phi: 1.5915481829543756, energy: 760.9083019644022\n",
      "pt: 130.3064497530392, eta: 2.2796274671886274, phi: 0.7558845433038527, energy: 644.0887079052627\n",
      "pt: 171.58958753905583, eta: -2.081413926902437, phi: 0.8112449525331366, energy: 699.7125793186715\n",
      "pt: 174.84490112777482, eta: 1.8367436161518296, phi: 2.6627000744427827, energy: 564.3855607409496\n",
      "pt: 201.3535278391687, eta: -1.9149328859824908, phi: 5.764316142643878, energy: 700.0459596281871\n",
      "pt: 183.80347977895374, eta: -1.9452116096604737, phi: 2.2589461507077817, energy: 657.6455971915275\n",
      "pt: 246.60742956186766, eta: 1.9030604392104955, phi: 3.862483196464514, energy: 847.454027841799\n",
      "pt: 362.35021833288727, eta: -1.7735374653317693, phi: 2.998837110523147, energy: 1101.5358930192888\n",
      "pt: 143.84393680251065, eta: -0.5283217266869797, phi: 5.348068846193515, energy: 164.407331392169\n",
      "pt: 178.61849478695564, eta: 1.926634847003592, phi: 1.5348093063048323, energy: 627.1473594978452\n",
      "pt: 349.9889492833052, eta: 1.6571107382289152, phi: 6.216325895499827, energy: 955.2986288773827\n",
      "pt: 369.81102009638585, eta: -1.6027620254329675, phi: 0.2897708221345048, energy: 960.7932788012549\n",
      "pt: 274.15964598210934, eta: 1.8165126780497354, phi: 0.8800506642078918, energy: 868.1887274831533\n",
      "pt: 315.84582813863904, eta: -1.7145671624929375, phi: 4.400339025309451, energy: 909.6866288883612\n",
      "pt: 444.92948127478735, eta: -1.545228873861781, phi: 1.7759301513457473, energy: 1097.4474570825696\n",
      "pt: 235.30307421793256, eta: -0.2363801133409693, phi: 2.74260004323334, energy: 242.51401269435883\n",
      "pt: 347.6108215099785, eta: 1.6913270591190215, phi: 4.757017628389579, energy: 978.6182138137519\n",
      "pt: 570.5900999251088, eta: 1.6578684657084537, phi: 5.45764070649951, energy: 1559.6603354895487\n",
      "pt: 587.8377120675275, eta: 1.4493555801997897, phi: 1.8870875020873288, energy: 1328.3501266605163\n",
      "pt: 405.20222337679724, eta: -1.6099098583765226, phi: 5.118179577916953, energy: 1058.7075827460503\n",
      "pt: 564.6124675924705, eta: 1.5491146075014717, phi: 3.2424025487966746, energy: 1395.8441972534638\n",
      "pt: 738.6100685840466, eta: -1.607840271945227, phi: 3.6261078204047896, energy: 1927.5377738880925\n",
      "pt: 797.8686352101696, eta: -1.4759024756040293, phi: 0.9991651453220212, energy: 1845.2297498881817\n",
      "pt: 878.4875718453691, eta: 1.347085313029813, phi: 2.4276950655738685, energy: 1816.2807900047046\n",
      "pt: 652.5139315086453, eta: -1.370804740399562, phi: 5.7239522471774436, energy: 1378.483330248855\n",
      "pt: 814.8509581839406, eta: 1.3295458881991946, phi: 3.8861791719250087, energy: 1661.6578884767368\n",
      "pt: 1244.9066920917735, eta: 1.322347884278812, phi: 1.2697894382615647, energy: 2522.0624315107707\n",
      "pt: 1289.1213217168104, eta: 1.285974886211011, phi: 0.44343057875476344, energy: 2533.000119634904\n",
      "pt: 1154.4805893277298, eta: -1.323549971166255, phi: 2.5674689431143767, energy: 2340.307169736596\n",
      "pt: 1636.1267060030807, eta: 0.9901276388990017, phi: 3.09254339756343, energy: 2543.27381287748\n",
      "pt: 1251.789574686449, eta: -1.1189393522220874, phi: 4.700116643886465, energy: 2138.4381223535165\n",
      "pt: 1352.152182962969, eta: 1.1304837115712956, phi: 5.933106001612677, energy: 2335.3626136891544\n",
      "pt: 1277.401954530693, eta: 1.1706310162248825, phi: 4.576164699648767, energy: 2281.9132862240076\n",
      "pt: 2266.3572047842404, eta: -0.9970511538183097, phi: 2.062847427679098, energy: 3538.9589156769216\n",
      "pt: 2248.487145479653, eta: -1.1859367465373531, phi: 4.095794403338208, energy: 4067.384697624948\n",
      "pt: 2060.5358904243344, eta: 1.1225813402747093, phi: 5.16615084641493, energy: 3544.8021536506712\n",
      "pt: 1644.5433769681392, eta: -1.1263391738688175, phi: 3.1575907776381342, energy: 2833.0157105252147\n",
      "pt: 2258.241225686813, eta: -1.1051667927349922, phi: 6.209459360486736, energy: 3827.1036033155397\n",
      "pt: 2265.122333526709, eta: -0.9728770439654133, phi: 0.6801785969078272, energy: 3463.9226494431496\n",
      "pt: 4318.574531391955, eta: 0.787248855507322, phi: 0.9017898603149938, energy: 5841.984874212416\n",
      "pt: 3865.246983946624, eta: 0.8114271215444528, phi: 2.492798417060049, energy: 5310.056202497333\n",
      "pt: 2907.675894530874, eta: 0.7809873796892589, phi: 3.7497218134136, energy: 3905.877052657306\n",
      "pt: 1824.5904422996405, eta: -0.9565411598371449, phi: 1.3357557374255182, energy: 2764.3925444744527\n",
      "pt: 1851.2618521766576, eta: -1.0080343548639157, phi: 5.2964194278868595, energy: 2911.4605101567577\n",
      "pt: 3634.559161299478, eta: 0.5635697100133481, phi: 5.684765174050491, energy: 4308.9257347323\n",
      "pt: 4929.759960923472, eta: 0.8146099526543568, phi: 1.6317941551607809, energy: 6820.6843334421865\n",
      "pt: 5774.975643301499, eta: -0.5610982674283246, phi: 4.926769464257091, energy: 6863.456168191042\n",
      "pt: 4160.239869000128, eta: -0.6840848635908355, phi: 2.7924705199103044, energy: 5274.3109079813585\n",
      "pt: 3847.776815294573, eta: -0.6962663264289016, phi: 3.626002091112301, energy: 4922.070498378482\n",
      "pt: 4944.205545910037, eta: 0.6837505466593699, phi: 0.14749356934516689, energy: 6270.924432663713\n",
      "pt: 6020.949723236277, eta: -0.6067165328489832, phi: 5.778851523163146, energy: 7333.707400657237\n",
      "pt: 6000.245109065281, eta: 0.6675601065467108, phi: 4.316212885266932, energy: 7564.719322584569\n",
      "pt: 6845.8300051511505, eta: -0.45687551076967714, phi: 1.620261493273122, energy: 7765.7960439659655\n",
      "pt: 6525.81145496709, eta: 0.5177877114649209, phi: 5.092526730702032, energy: 7635.935745276045\n",
      "pt: 6891.324333773707, eta: -0.5420400198891958, phi: 4.230430809600353, energy: 8138.228597630514\n",
      "pt: 8333.845196467653, eta: -0.40685796414783426, phi: 2.2467191121039267, energy: 9313.046277865767\n",
      "pt: 7128.307385118435, eta: 0.4673672962070746, phi: 3.2782449072326414, energy: 8145.292960772291\n",
      "pt: 9930.653349584247, eta: 0.1374359267855017, phi: 6.062746249107331, energy: 10392.220570208505\n",
      "pt: 8796.59859451159, eta: 0.2585725734377963, phi: 1.908822126973509, energy: 9384.892317419872\n",
      "pt: 6602.378993320294, eta: -0.43640004252227727, phi: 0.9461248120773624, energy: 7443.911347246729\n",
      "pt: 10294.112874447577, eta: 0.19150703597171082, phi: 0.5539690632164158, energy: 10845.833764849813\n",
      "pt: 10037.262995635623, eta: -0.2628769281541984, phi: 3.2301554265721255, energy: 10744.713762275875\n",
      "pt: 10530.178031147998, eta: 0.1563038585557787, phi: 1.2386041871895048, energy: 11044.523987096385\n",
      "pt: 11100.116176446296, eta: -0.12242163496004137, phi: 5.403536649104562, energy: 11603.609700151952\n",
      "pt: 9708.389197910694, eta: 0.04903010629954704, phi: 3.878806278643656, energy: 10064.035970325116\n",
      "pt: 12127.259803404128, eta: 0.07900940631770653, phi: 4.61615044222563, energy: 12632.18287890777\n",
      "pt: 10928.806138985261, eta: -0.4233186351545351, phi: 0.23342047571416155, energy: 12400.355923021853\n",
      "pt: 12403.670988851329, eta: 0.16436293276936312, phi: 2.6031515900168394, energy: 13039.012621906586\n"
     ]
    }
   ],
   "source": [
    "gen_jets = [\n",
    "    {\"pt\": gen_jet.pt(), \"eta\": gen_jet.eta(), \"phi\": gen_jet.phi(), \"energy\": gen_jet.e()} for gen_jet in gen_sequence.inclusive_jets(0.0)\n",
    "]\n",
    "\n",
    "reco_jets = [\n",
    "    {\"pt\": reco_jet.pt(), \"eta\": reco_jet.eta(), \"phi\": reco_jet.phi(), \"energy\": reco_jet.e()} for reco_jet in reco_sequence.inclusive_jets(0.0)\n",
    "]\n",
    "\n",
    "print(\"Generated Jets:\")\n",
    "for jet in gen_jets:\n",
    "    print(f\"pt: {jet['pt']}, eta: {jet['eta']}, phi: {jet['phi']}, energy: {jet['energy']}\")\n",
    "\n",
    "print(\"Reconstructed Jets:\")\n",
    "for jet in reco_jets:\n",
    "    print(f\"pt: {jet['pt']}, eta: {jet['eta']}, phi: {jet['phi']}, energy: {jet['energy']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bf3fe8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_jets_coll = {\"gen_jets\": gen_jets}\n",
    "reco_jets_coll = {\"reco_jets\": reco_jets}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cab93ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching result:\n",
      "[{gen_jets: [], reco_jets: []}, {gen_jets: [], ... {gen_jets: [], reco_jets: []}]\n"
     ]
    }
   ],
   "source": [
    "def match_two_jet_collections(jets_coll, name1, name2, jet_match_dr):\n",
    "    num_events = len(jets_coll[name1])\n",
    "    \n",
    "    # Check if reco_jets list is empty or shorter than expected\n",
    "    if len(jets_coll[name2]) == 0 or len(jets_coll[name2]) < num_events:\n",
    "        dummy = build_dummy_array(num_events)\n",
    "        return awkward.Array({name1: dummy, name2: dummy})\n",
    "\n",
    "    # Convert gen_jets and reco_jets to arrays of dictionaries\n",
    "    vec1 = []\n",
    "    vec2 = []\n",
    "    for event in range(num_events):\n",
    "        vec1.append(\n",
    "            {\n",
    "                \"pt\": jets_coll[name1][event][\"pt\"],\n",
    "                \"eta\": jets_coll[name1][event][\"eta\"],\n",
    "                \"phi\": jets_coll[name1][event][\"phi\"],\n",
    "                \"energy\": jets_coll[name1][event][\"energy\"],\n",
    "            }\n",
    "        )\n",
    "        vec2.append(\n",
    "            {\n",
    "                \"pt\": jets_coll[name2][event][\"pt\"],\n",
    "                \"eta\": jets_coll[name2][event][\"eta\"],\n",
    "                \"phi\": jets_coll[name2][event][\"phi\"],\n",
    "                \"energy\": jets_coll[name2][event][\"energy\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    ret = match_jets(vec1, vec2, jet_match_dr)\n",
    "    j1_idx = awkward.from_iter(ret[0])\n",
    "    j2_idx = awkward.from_iter(ret[1])\n",
    "\n",
    "    num_jets = len(awkward.flatten(j1_idx))\n",
    "\n",
    "    # In case there are no jets matched, create dummy array to ensure correct types\n",
    "    if num_jets > 0:\n",
    "        c1_to_c2 = awkward.Array({name1: j1_idx, name2: j2_idx})\n",
    "    else:\n",
    "        dummy = build_dummy_array(num_events)\n",
    "        c1_to_c2 = awkward.Array({name1: dummy, name2: dummy})\n",
    "\n",
    "    return c1_to_c2\n",
    "\n",
    "# Now you can call the match_two_jet_collections function with gen_jets and reco_jets\n",
    "matching_result = match_two_jet_collections({\"gen_jets\": gen_jets, \"reco_jets\": reco_jets}, \"gen_jets\", \"reco_jets\", jet_match_dr)\n",
    "\n",
    "print(\"Matching result:\")\n",
    "print(matching_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4bae7421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching result:\n",
      "Indices of matched jets from 'gen_jets': [[], [], [], [], [], [], [], [], [], [], ... [], [], [], [], [], [], [], [], [], []]\n",
      "Indices of matched jets from 'reco_jets': [[], [], [], [], [], [], [], [], [], [], ... [], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "# Assuming you already defined the function match_two_jet_collections and obtained matching_result\n",
    "\n",
    "# Access the values inside matching_result\n",
    "print(\"Matching result:\")\n",
    "print(\"Indices of matched jets from 'gen_jets':\", matching_result[\"gen_jets\"])\n",
    "print(\"Indices of matched jets from 'reco_jets':\", matching_result[\"reco_jets\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
