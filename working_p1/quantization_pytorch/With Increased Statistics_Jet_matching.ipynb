{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e832d4",
   "metadata": {},
   "source": [
    "Task:\n",
    "* correct jet matching, 1000 events, response distributions, simple code \n",
    "Issues:\n",
    "Check the code\n",
    "Check the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f443036a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.30/02\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import ROOT\n",
    "\n",
    "import vector\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8b83502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import tensorflow_datasets as tfds\n",
    "import torch_geometric\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e15a8e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7003596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "data_dir = \"../../mlpf/tensorflow_datasets/\"\n",
    "dataset = \"clic_edm_ttbar_pf\"\n",
    "\n",
    "# Load dataset\n",
    "builder = tfds.builder(dataset, data_dir=data_dir)\n",
    "ds_train = builder.as_data_source(split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3038a58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_FEATURES_TRK = [\n",
    "    \"elemtype\",\n",
    "    \"pt\",\n",
    "    \"eta\",\n",
    "    \"sin_phi\",\n",
    "    \"cos_phi\",\n",
    "    \"p\",\n",
    "    \"chi2\",\n",
    "    \"ndf\",\n",
    "    \"dEdx\",\n",
    "    \"dEdxError\",\n",
    "    \"radiusOfInnermostHit\",\n",
    "    \"tanLambda\",\n",
    "    \"D0\",\n",
    "    \"omega\",\n",
    "    \"Z0\",\n",
    "    \"time\",\n",
    "]\n",
    "X_FEATURES_CL = [\n",
    "    \"elemtype\",\n",
    "    \"et\",\n",
    "    \"eta\",\n",
    "    \"sin_phi\",\n",
    "    \"cos_phi\",\n",
    "    \"energy\",\n",
    "    \"position.x\",\n",
    "    \"position.y\",\n",
    "    \"position.z\",\n",
    "    \"iTheta\",\n",
    "    \"energy_ecal\",\n",
    "    \"energy_hcal\",\n",
    "    \"energy_other\",\n",
    "    \"num_hits\",\n",
    "    \"sigma_x\",\n",
    "    \"sigma_y\",\n",
    "    \"sigma_z\",\n",
    "]\n",
    "Y_FEATURES = [\"cls_id\", \"charge\", \"pt\", \"eta\", \"sin_phi\", \"cos_phi\", \"energy\"]\n",
    "Y_CLASSES = [0, 211, 130, 22, 11, 13]\n",
    "\n",
    "INPUT_DIM = max(len(X_FEATURES_TRK), len(X_FEATURES_CL))\n",
    "NUM_CLASSES = len(Y_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cf5476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JP 2024-02-29: currently torch int8 onnx export does not work with MultiheadAttention because of the following:\n",
    "# - it uses q_scaling_product.mul_scalar which is not supported in ONNX opset 17: the fix is to just remove the q_scaling_product\n",
    "# - somehow, the \"need_weights\" option confuses the ONNX exporter because the multiheaded attention layer then returns a tuple: the fix is to make the MHA always return just the attended values only\n",
    "# I lifted these two modules directly from the pytorch code and made the modifications here.\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as nnF\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class QuantizeableMultiheadAttention(nn.MultiheadAttention):\n",
    "    _FLOAT_MODULE = nn.MultiheadAttention\n",
    "\n",
    "    r\"\"\"Quantizable implementation of the MultiheadAttention.\n",
    "\n",
    "    Note::\n",
    "        Please, refer to :class:`~torch.nn.MultiheadAttention` for more\n",
    "        information\n",
    "\n",
    "    Allows the model to jointly attend to information from different\n",
    "    representation subspaces.\n",
    "    See reference: Attention Is All You Need\n",
    "\n",
    "    The original MHA module is not quantizable.\n",
    "    This reimplements it by explicitly instantiating the linear layers.\n",
    "\n",
    "    .. math::\n",
    "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
    "        \\text{where} head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "\n",
    "    Args:\n",
    "        embed_dim: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n",
    "        bias: add bias as module parameter. Default: True.\n",
    "        add_bias_kv: add bias to the key and value sequences at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        kdim: total number of features in key. Default: None.\n",
    "        vdim: total number of features in value. Default: None.\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
    "\n",
    "    Note that if :attr:`kdim` and :attr:`vdim` are None, they will be set\n",
    "    to :attr:`embed_dim` such that query, key, and value have the same\n",
    "    number of features.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> import torch.ao.nn.quantizable as nnqa\n",
    "        >>> multihead_attn = nnqa.MultiheadAttention(embed_dim, num_heads)\n",
    "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "\n",
    "    Note::\n",
    "        Please, follow the quantization flow to convert the quantizable MHA.\n",
    "    \"\"\"\n",
    "    __constants__ = ['batch_first']\n",
    "\n",
    "    def __init__(self, embed_dim: int, num_heads: int,\n",
    "                 dropout: float = 0., bias: bool = True,\n",
    "                 add_bias_kv: bool = False, add_zero_attn: bool = False,\n",
    "                 kdim: Optional[int] = None, vdim: Optional[int] = None, batch_first: bool = False,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, num_heads, dropout,\n",
    "                         bias, add_bias_kv,\n",
    "                         add_zero_attn, kdim, vdim, batch_first,\n",
    "                         **factory_kwargs)\n",
    "        self.linear_Q = nn.Linear(self.embed_dim, self.embed_dim, bias=bias, **factory_kwargs)\n",
    "        self.linear_K = nn.Linear(self.kdim, self.embed_dim, bias=bias, **factory_kwargs)\n",
    "        self.linear_V = nn.Linear(self.vdim, self.embed_dim, bias=bias, **factory_kwargs)\n",
    "        # for the type: ignore, see https://github.com/pytorch/pytorch/issues/58969\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=bias, **factory_kwargs)  # type: ignore[assignment]\n",
    "\n",
    "        # Functionals\n",
    "        # self.q_scaling_product = torch.ao.nn.quantized.FloatFunctional()\n",
    "        # note: importing torch.ao.nn.quantized at top creates a circular import\n",
    "\n",
    "        # Quant/Dequant\n",
    "        self.quant_attn_output = torch.ao.quantization.QuantStub()\n",
    "        self.quant_attn_output_weights = torch.ao.quantization.QuantStub()\n",
    "        self.dequant_q = torch.ao.quantization.DeQuantStub()\n",
    "        self.dequant_k = torch.ao.quantization.DeQuantStub()\n",
    "        self.dequant_v = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def _get_name(self):\n",
    "        return 'QuantizableMultiheadAttention'\n",
    "\n",
    "    @classmethod\n",
    "    def from_float(cls, other):\n",
    "        assert type(other) == cls._FLOAT_MODULE\n",
    "        assert hasattr(other, 'qconfig'), \"The float module must have 'qconfig'\"\n",
    "        # Setting the dropout to 0.0!\n",
    "        observed = cls(other.embed_dim, other.num_heads, other.dropout,\n",
    "                       (other.in_proj_bias is not None),\n",
    "                       (other.bias_k is not None),\n",
    "                       other.add_zero_attn, other.kdim, other.vdim,\n",
    "                       other.batch_first)\n",
    "        observed.bias_k = other.bias_k\n",
    "        observed.bias_v = other.bias_v\n",
    "        observed.qconfig = other.qconfig\n",
    "\n",
    "        # Set the linear weights\n",
    "        # for the type: ignores, see https://github.com/pytorch/pytorch/issues/58969\n",
    "        observed.out_proj.weight = other.out_proj.weight  # type: ignore[has-type]\n",
    "        observed.out_proj.bias = other.out_proj.bias  # type: ignore[has-type]\n",
    "        if other._qkv_same_embed_dim:\n",
    "            # Use separate params\n",
    "            bias = other.in_proj_bias\n",
    "            _start = 0\n",
    "            _end = _start + other.embed_dim\n",
    "            weight = other.in_proj_weight[_start:_end, :]\n",
    "            if bias is not None:\n",
    "                bias = torch.nn.Parameter(bias[_start:_end], bias.requires_grad)\n",
    "            observed.linear_Q.weight = torch.nn.Parameter(weight,\n",
    "                                                          weight.requires_grad)\n",
    "            observed.linear_Q.bias = bias\n",
    "\n",
    "            bias = other.in_proj_bias\n",
    "            _start = _end\n",
    "            _end = _start + other.embed_dim\n",
    "            weight = other.in_proj_weight[_start:_end, :]\n",
    "            if bias is not None:\n",
    "                bias = torch.nn.Parameter(bias[_start:_end], bias.requires_grad)\n",
    "            observed.linear_K.weight = torch.nn.Parameter(weight,\n",
    "                                                          weight.requires_grad)\n",
    "            observed.linear_K.bias = bias\n",
    "\n",
    "            bias = other.in_proj_bias\n",
    "            _start = _end\n",
    "            weight = other.in_proj_weight[_start:, :]\n",
    "            if bias is not None:\n",
    "                bias = torch.nn.Parameter(bias[_start:], bias.requires_grad)\n",
    "            observed.linear_V.weight = torch.nn.Parameter(weight,\n",
    "                                                          weight.requires_grad)\n",
    "            observed.linear_V.bias = bias\n",
    "        else:\n",
    "            observed.linear_Q.weight = nn.Parameter(other.q_proj_weight)\n",
    "            observed.linear_K.weight = nn.Parameter(other.k_proj_weight)\n",
    "            observed.linear_V.weight = nn.Parameter(other.v_proj_weight)\n",
    "            if other.in_proj_bias is None:\n",
    "                observed.linear_Q.bias = None  # type: ignore[assignment]\n",
    "                observed.linear_K.bias = None  # type: ignore[assignment]\n",
    "                observed.linear_V.bias = None  # type: ignore[assignment]\n",
    "            else:\n",
    "                observed.linear_Q.bias = nn.Parameter(other.in_proj_bias[0:other.embed_dim])\n",
    "                observed.linear_K.bias = nn.Parameter(other.in_proj_bias[other.embed_dim:(other.embed_dim * 2)])\n",
    "                observed.linear_V.bias = nn.Parameter(other.in_proj_bias[(other.embed_dim * 2):])\n",
    "        observed.eval()\n",
    "        # Explicit prepare\n",
    "        observed = torch.ao.quantization.prepare(observed, inplace=True)\n",
    "        return observed\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def dequantize(self):\n",
    "        r\"\"\"Utility to convert the quantized MHA back to float.\n",
    "\n",
    "        The motivation for this is that it is not trivial to conver the weights\n",
    "        from the format that is used in the quantized version back to the\n",
    "        float.\n",
    "        \"\"\"\n",
    "        fp = self._FLOAT_MODULE(self.embed_dim, self.num_heads, self.dropout,\n",
    "                                (self.linear_Q._weight_bias()[1] is not None),\n",
    "                                (self.bias_k is not None),\n",
    "                                self.add_zero_attn, self.kdim, self.vdim, self.batch_first)\n",
    "        assert fp._qkv_same_embed_dim == self._qkv_same_embed_dim\n",
    "        if self.bias_k is not None:\n",
    "            fp.bias_k = nn.Parameter(self.bias_k.dequantize())\n",
    "        if self.bias_v is not None:\n",
    "            fp.bias_v = nn.Parameter(self.bias_v.dequantize())\n",
    "\n",
    "        # Set the linear weights\n",
    "        # Note: Because the linear layers are quantized, mypy does not nkow how\n",
    "        # to deal with them -- might need to ignore the typing checks.\n",
    "        # for the type: ignore[has-type], see https://github.com/pytorch/pytorch/issues/58969\n",
    "        w, b = self.out_proj._weight_bias()  # type: ignore[operator, has-type]\n",
    "        fp.out_proj.weight = nn.Parameter(w.dequantize())\n",
    "        if b is not None:\n",
    "            fp.out_proj.bias = nn.Parameter(b)\n",
    "\n",
    "        wQ, bQ = self.linear_Q._weight_bias()  # type: ignore[operator]\n",
    "        wQ = wQ.dequantize()\n",
    "        wK, bK = self.linear_K._weight_bias()  # type: ignore[operator]\n",
    "        wK = wK.dequantize()\n",
    "        wV, bV = self.linear_V._weight_bias()  # type: ignore[operator]\n",
    "        wV = wV.dequantize()\n",
    "        if fp._qkv_same_embed_dim:\n",
    "            # Use separate params\n",
    "            _start = 0\n",
    "            _end = _start + fp.embed_dim\n",
    "            fp.in_proj_weight[_start:_end, :] = wQ\n",
    "            if fp.in_proj_bias is not None:\n",
    "                assert all(bQ == 0)\n",
    "                fp.in_proj_bias[_start:_end] = bQ\n",
    "\n",
    "            _start = _end\n",
    "            _end = _start + fp.embed_dim\n",
    "            fp.in_proj_weight[_start:_end, :] = wK\n",
    "            if fp.in_proj_bias is not None:\n",
    "                assert all(bK == 0)\n",
    "                fp.in_proj_bias[_start:_end] = bK\n",
    "\n",
    "            _start = _end\n",
    "            fp.in_proj_weight[_start:, :] = wV\n",
    "            if fp.in_proj_bias is not None:\n",
    "                assert all(bV == 0)\n",
    "                fp.in_proj_bias[_start:] = bV\n",
    "        else:\n",
    "            fp.q_proj_weight = nn.Parameter(wQ)\n",
    "            fp.k_proj_weight = nn.Parameter(wK)\n",
    "            fp.v_proj_weight = nn.Parameter(wV)\n",
    "            if fp.in_proj_bias is None:\n",
    "                self.linear_Q.bias = None\n",
    "                self.linear_K.bias = None\n",
    "                self.linear_V.bias = None\n",
    "            else:\n",
    "                fp.in_proj_bias[0:fp.embed_dim] = bQ\n",
    "                fp.in_proj_bias[fp.embed_dim:(fp.embed_dim * 2)] = bK\n",
    "                fp.in_proj_bias[(fp.embed_dim * 2):] = bV\n",
    "\n",
    "        return fp\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_observed(cls, other):\n",
    "        # The whole flow is float -> observed -> quantized\n",
    "        # This class does float -> observed only\n",
    "        # See nn.quantized.MultiheadAttention\n",
    "        raise NotImplementedError(\"It looks like you are trying to prepare an \"\n",
    "                                  \"MHA module. Please, see \"\n",
    "                                  \"the examples on quantizable MHAs.\")\n",
    "\n",
    "    def forward(self,\n",
    "                query: Tensor,\n",
    "                key: Tensor,\n",
    "                value: Tensor,\n",
    "                key_padding_mask: Optional[Tensor] = None,\n",
    "                need_weights: bool = True,\n",
    "                attn_mask: Optional[Tensor] = None,\n",
    "                average_attn_weights: bool = True,\n",
    "                is_causal: bool = False) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        r\"\"\"\n",
    "    Note::\n",
    "        Please, refer to :func:`~torch.nn.MultiheadAttention.forward` for more\n",
    "        information\n",
    "\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. When given a binary mask and a value is True,\n",
    "            the corresponding value on the attention layer will be ignored.\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
    "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
    "\n",
    "    Shape:\n",
    "        - Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n",
    "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
    "          If a BoolTensor is provided, the positions with the\n",
    "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked\n",
    "          positions. If a BoolTensor is provided, positions with ``True``\n",
    "          is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "          is provided, it will be added to the attention weight.\n",
    "        - is_causal: If specified, applies a causal mask as attention mask. Mutually exclusive with providing attn_mask.\n",
    "          Default: ``False``.\n",
    "        - average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across\n",
    "          heads. Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an\n",
    "          effect when ``need_weights=True.``. Default: True (i.e. average weights across heads)\n",
    "\n",
    "        - Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n",
    "        - attn_output_weights: If ``average_attn_weights=True``, returns attention weights averaged\n",
    "          across heads of shape :math:`(N, L, S)`, where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. If ``average_attn_weights=False``, returns attention weights per\n",
    "          head of shape :math:`(N, num_heads, L, S)`.\n",
    "        \"\"\"\n",
    "        return self._forward_impl(query, key, value, key_padding_mask,\n",
    "                                  need_weights, attn_mask, average_attn_weights,\n",
    "                                  is_causal)\n",
    "\n",
    "    def _forward_impl(self,\n",
    "                      query: Tensor,\n",
    "                      key: Tensor,\n",
    "                      value: Tensor,\n",
    "                      key_padding_mask: Optional[Tensor] = None,\n",
    "                      need_weights: bool = True,\n",
    "                      attn_mask: Optional[Tensor] = None,\n",
    "                      average_attn_weights: bool = True,\n",
    "                      is_causal: bool = False) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        # This version will not deal with the static key/value pairs.\n",
    "        # Keeping it here for future changes.\n",
    "        #\n",
    "        # TODO: This method has some duplicate lines with the\n",
    "        # `torch.nn.functional.multi_head_attention`. Will need to refactor.\n",
    "        static_k = None\n",
    "        static_v = None\n",
    "\n",
    "        if attn_mask is not None and is_causal:\n",
    "            raise AssertionError(\"Only allow causal mask or attn_mask\")\n",
    "\n",
    "        if is_causal:\n",
    "            raise AssertionError(\"causal mask not supported by AO MHA module\")\n",
    "\n",
    "        if self.batch_first:\n",
    "            query, key, value = (x.transpose(0, 1) for x in (query, key, value))\n",
    "\n",
    "        tgt_len, bsz, embed_dim_to_check = query.size()\n",
    "        assert self.embed_dim == embed_dim_to_check\n",
    "        # allow MHA to have different sizes for the feature dimension\n",
    "        assert key.size(0) == value.size(0) and key.size(1) == value.size(1)\n",
    "\n",
    "        head_dim = self.embed_dim // self.num_heads\n",
    "        assert head_dim * self.num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        scaling = float(head_dim) ** -0.5\n",
    "\n",
    "        q = self.linear_Q(query)\n",
    "        k = self.linear_K(key)\n",
    "        v = self.linear_V(value)\n",
    "\n",
    "        #JP fix here: disabled this\n",
    "        # q = self.q_scaling_product.mul_scalar(q, scaling)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.uint8:\n",
    "                warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "                attn_mask = attn_mask.to(torch.bool)\n",
    "            assert attn_mask.is_floating_point() or attn_mask.dtype == torch.bool, \\\n",
    "                f'Only float and bool types are supported for attn_mask, not {attn_mask.dtype}'\n",
    "\n",
    "            if attn_mask.dim() == 2:\n",
    "                attn_mask = attn_mask.unsqueeze(0)\n",
    "                if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:\n",
    "                    raise RuntimeError('The size of the 2D attn_mask is not correct.')\n",
    "            elif attn_mask.dim() == 3:\n",
    "                if list(attn_mask.size()) != [bsz * self.num_heads, query.size(0), key.size(0)]:\n",
    "                    raise RuntimeError('The size of the 3D attn_mask is not correct.')\n",
    "            else:\n",
    "                raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "            # attn_mask's dim is 3 now.\n",
    "\n",
    "        # convert ByteTensor key_padding_mask to bool\n",
    "        if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n",
    "            warnings.warn(\"Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "            key_padding_mask = key_padding_mask.to(torch.bool)\n",
    "        if self.bias_k is not None and self.bias_v is not None:\n",
    "            if static_k is None and static_v is None:\n",
    "\n",
    "                # Explicitly assert that bias_k and bias_v are not None\n",
    "                # in a way that TorchScript can understand.\n",
    "                bias_k = self.bias_k\n",
    "                assert bias_k is not None\n",
    "                bias_v = self.bias_v\n",
    "                assert bias_v is not None\n",
    "\n",
    "                k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
    "                v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
    "                if attn_mask is not None:\n",
    "                    attn_mask = nnF.pad(attn_mask, (0, 1))\n",
    "                if key_padding_mask is not None:\n",
    "                    key_padding_mask = nnF.pad(key_padding_mask, (0, 1))\n",
    "            else:\n",
    "                assert static_k is None, \"bias cannot be added to static key.\"\n",
    "                assert static_v is None, \"bias cannot be added to static value.\"\n",
    "        else:\n",
    "            assert self.bias_k is None\n",
    "            assert self.bias_v is None\n",
    "\n",
    "        q = q.contiguous().view(tgt_len, bsz * self.num_heads, head_dim).transpose(0, 1)\n",
    "        if k is not None:\n",
    "            k = k.contiguous().view(-1, bsz * self.num_heads, head_dim).transpose(0, 1)\n",
    "        if v is not None:\n",
    "            v = v.contiguous().view(-1, bsz * self.num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "        if static_k is not None:\n",
    "            assert static_k.size(0) == bsz * self.num_heads\n",
    "            assert static_k.size(2) == head_dim\n",
    "            k = static_k\n",
    "\n",
    "        if static_v is not None:\n",
    "            assert static_v.size(0) == bsz * self.num_heads\n",
    "            assert static_v.size(2) == head_dim\n",
    "            v = static_v\n",
    "\n",
    "        src_len = k.size(1)\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            assert key_padding_mask.size(0) == bsz\n",
    "            assert key_padding_mask.size(1) == src_len\n",
    "\n",
    "        if self.add_zero_attn:\n",
    "            src_len += 1\n",
    "            k_zeros = torch.zeros((k.size(0), 1) + k.size()[2:])\n",
    "            if k.is_quantized:\n",
    "                k_zeros = torch.quantize_per_tensor(k_zeros, k.q_scale(), k.q_zero_point(), k.dtype)\n",
    "            k = torch.cat([k, k_zeros], dim=1)\n",
    "            v_zeros = torch.zeros((v.size(0), 1) + k.size()[2:])\n",
    "            if v.is_quantized:\n",
    "                v_zeros = torch.quantize_per_tensor(v_zeros, v.q_scale(), v.q_zero_point(), v.dtype)\n",
    "            v = torch.cat([v, v_zeros], dim=1)\n",
    "\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = nnF.pad(attn_mask, (0, 1))\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = nnF.pad(key_padding_mask, (0, 1))\n",
    "\n",
    "        # Leaving the quantized zone here\n",
    "        q = self.dequant_q(q)\n",
    "        k = self.dequant_k(k)\n",
    "        v = self.dequant_v(v)\n",
    "        attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "        assert list(attn_output_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                attn_output_weights.masked_fill_(attn_mask, float('-inf'))\n",
    "            else:\n",
    "                attn_output_weights += attn_mask\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            attn_output_weights = attn_output_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_output_weights = attn_output_weights.masked_fill(\n",
    "                key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "                float('-inf'),\n",
    "            )\n",
    "            attn_output_weights = attn_output_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        attn_output_weights = nnF.softmax(\n",
    "            attn_output_weights, dim=-1)\n",
    "        attn_output_weights = nnF.dropout(attn_output_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.bmm(attn_output_weights, v)\n",
    "        assert list(attn_output.size()) == [bsz * self.num_heads, tgt_len, head_dim]\n",
    "        if self.batch_first:\n",
    "            attn_output = attn_output.view(bsz, tgt_len, self.embed_dim)\n",
    "        else:\n",
    "            attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, self.embed_dim)\n",
    "\n",
    "        # Reentering the quantized zone\n",
    "        attn_output = self.quant_attn_output(attn_output)\n",
    "        # for the type: ignore[has-type], see https://github.com/pytorch/pytorch/issues/58969\n",
    "        attn_output = self.out_proj(attn_output)  # type: ignore[has-type]\n",
    "\n",
    "        #JP fix: removed need_weights part from here, return attn_output instead of tuple\n",
    "        return attn_output\n",
    "\n",
    "class QuantizedMultiheadAttention(QuantizeableMultiheadAttention):\n",
    "    _FLOAT_MODULE = torch.ao.nn.quantizable.MultiheadAttention\n",
    "\n",
    "    def _get_name(self):\n",
    "        return \"QuantizedMultiheadAttention\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_float(cls, other):\n",
    "        # The whole flow is float -> observed -> quantized\n",
    "        # This class does observed -> quantized only\n",
    "        raise NotImplementedError(\"It looks like you are trying to convert a \"\n",
    "                                  \"non-observed MHA module. Please, see \"\n",
    "                                  \"the examples on quantizable MHAs.\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_observed(cls, other):\n",
    "        converted = torch.ao.quantization.convert(other, mapping=None,\n",
    "                                                  inplace=False,\n",
    "                                                  remove_qconfig=True,\n",
    "                                                  convert_custom_config_dict=None)\n",
    "        converted.__class__ = cls\n",
    "        # Remove the parameters for the bias_k and bias_v to quantize them\n",
    "        # TODO: This is a potential source of accuracy drop.\n",
    "        #       quantized cat takes the scale and zp of the first\n",
    "        #       element, which might lose the precision in the bias_k\n",
    "        #       and the bias_v (which are cat'ed with k/v being first).\n",
    "        if converted.bias_k is not None:\n",
    "            bias_k = converted._parameters.pop('bias_k')\n",
    "            sc, zp = torch._choose_qparams_per_tensor(bias_k,\n",
    "                                                      reduce_range=False)\n",
    "            bias_k = torch.quantize_per_tensor(bias_k, sc, zp, torch.quint8)\n",
    "            setattr(converted, 'bias_k', bias_k)  # noqa: B010\n",
    "\n",
    "        if converted.bias_v is not None:\n",
    "            bias_v = converted._parameters.pop('bias_v')\n",
    "            sc, zp = torch._choose_qparams_per_tensor(bias_k,  # type: ignore[possibly-undefined]\n",
    "                                                      reduce_range=False)\n",
    "            bias_v = torch.quantize_per_tensor(bias_v, sc, zp, torch.quint8)\n",
    "            setattr(converted, 'bias_v', bias_v)  # noqa: B010\n",
    "\n",
    "        del converted.in_proj_weight\n",
    "        del converted.in_proj_bias\n",
    "\n",
    "        return converted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "101cdde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss, as described in https://arxiv.org/abs/1708.02002.\n",
    "    It is essentially an enhancement to cross entropy loss and is\n",
    "    useful for classification tasks when there is a large class imbalance.\n",
    "    x is expected to contain raw, unnormalized scores for each class.\n",
    "    y is expected to contain class labels.\n",
    "    Shape:\n",
    "        - x: (batch_size, C) or (batch_size, C, d1, d2, ..., dK), K > 0.\n",
    "        - y: (batch_size,) or (batch_size, d1, d2, ..., dK), K > 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, alpha = None, gamma = 0.0, reduction = \"mean\", ignore_index = -100\n",
    "    ):\n",
    "        \"\"\"Constructor.\n",
    "        Args:\n",
    "            alpha (Tensor, optional): Weights for each class. Defaults to None.\n",
    "            gamma (float, optional): A constant, as described in the paper.\n",
    "                Defaults to 0.\n",
    "            reduction (str, optional): 'mean', 'sum' or 'none'.\n",
    "                Defaults to 'mean'.\n",
    "            ignore_index (int, optional): class label to ignore.\n",
    "                Defaults to -100.\n",
    "        \"\"\"\n",
    "        if reduction not in (\"mean\", \"sum\", \"none\"):\n",
    "            raise ValueError('Reduction must be one of: \"mean\", \"sum\", \"none\".')\n",
    "\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "        self.nll_loss = nn.NLLLoss(weight=alpha, reduction=\"none\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        arg_keys = [\"alpha\", \"gamma\", \"reduction\"]\n",
    "        arg_vals = [self.__dict__[k] for k in arg_keys]\n",
    "        arg_strs = [f\"{k}={v!r}\" for k, v in zip(arg_keys, arg_vals)]\n",
    "        arg_str = \", \".join(arg_strs)\n",
    "        return f\"{type(self).__name__}({arg_str})\"\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        if x.ndim > 2:\n",
    "            # (N, C, d1, d2, ..., dK) --> (N * d1 * ... * dK, C)\n",
    "            c = x.shape[1]\n",
    "            x = x.permute(0, *range(2, x.ndim), 1).reshape(-1, c)\n",
    "            # (N, d1, d2, ..., dK) --> (N * d1 * ... * dK,)\n",
    "            y = y.view(-1)\n",
    "\n",
    "        # compute weighted cross entropy term: -alpha * log(pt)\n",
    "        # (alpha is already part of self.nll_loss)\n",
    "        log_p = F.log_softmax(x, dim=-1)\n",
    "        ce = self.nll_loss(log_p, y)\n",
    "\n",
    "        # get true class column from each row\n",
    "        # this is slow due to indexing\n",
    "        # all_rows = torch.arange(len(x))\n",
    "        # log_pt = log_p[all_rows, y]\n",
    "        log_pt = torch.gather(log_p, 1, y.unsqueeze(axis=-1)).squeeze(axis=-1)\n",
    "\n",
    "        # compute focal term: (1 - pt)^gamma\n",
    "        pt = log_pt.exp()\n",
    "        focal_term = (1 - pt) ** self.gamma\n",
    "\n",
    "        # the full loss: -alpha * ((1 - pt)^gamma) * log(pt)\n",
    "        loss = focal_term * ce\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            loss = loss.sum()\n",
    "\n",
    "        return loss\n",
    "        \n",
    "def mlpf_loss(y, ypred, mask):\n",
    "    loss = {}\n",
    "    loss_obj_id = FocalLoss(gamma=2.0, reduction=\"none\")\n",
    "\n",
    "    msk_true_particle = torch.unsqueeze((y[\"cls_id\"] != 0).to(dtype=torch.float32), axis=-1)\n",
    "    nelem = torch.sum(mask)\n",
    "    npart = torch.sum(y[\"cls_id\"] != 0)\n",
    "    \n",
    "    ypred[\"momentum\"] = ypred[\"momentum\"] * msk_true_particle\n",
    "    y[\"momentum\"] = y[\"momentum\"] * msk_true_particle\n",
    "\n",
    "    ypred[\"cls_id_onehot\"] = ypred[\"cls_id_onehot\"].permute((0, 2, 1))\n",
    "\n",
    "    loss_classification = loss_obj_id(ypred[\"cls_id_onehot\"], y[\"cls_id\"]).reshape(y[\"cls_id\"].shape)\n",
    "    loss_regression = torch.nn.functional.huber_loss(ypred[\"momentum\"], y[\"momentum\"], reduction=\"none\")\n",
    "    \n",
    "    # average over all elements that were not padded\n",
    "    loss[\"Classification\"] = loss_classification.sum() / npart\n",
    "    \n",
    "    mom_normalizer = y[\"momentum\"][y[\"cls_id\"] != 0].std(axis=0)\n",
    "    reg_losses = loss_regression[y[\"cls_id\"] != 0]\n",
    "    # average over all true particles\n",
    "    loss[\"Regression\"] = (reg_losses / mom_normalizer).sum() / npart\n",
    "\n",
    "    px = ypred[\"momentum\"][..., 0:1] * ypred[\"momentum\"][..., 3:4] * msk_true_particle\n",
    "    py = ypred[\"momentum\"][..., 0:1] * ypred[\"momentum\"][..., 2:3] * msk_true_particle\n",
    "    pred_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)\n",
    "\n",
    "    px = y[\"momentum\"][..., 0:1] * y[\"momentum\"][..., 3:4] * msk_true_particle\n",
    "    py = y[\"momentum\"][..., 0:1] * y[\"momentum\"][..., 2:3] * msk_true_particle\n",
    "    true_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)\n",
    "    loss[\"MET\"] = torch.nn.functional.huber_loss(pred_met, true_met).mean()\n",
    "\n",
    "    loss[\"Total\"] = loss[\"Classification\"] + loss[\"Regression\"]\n",
    "    # loss[\"Total\"] += 0.1*loss[\"MET\"]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daa50dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizeFeaturesStub(torch.ao.quantization.QuantStub):\n",
    "    def __init__(self, num_feats):\n",
    "        super().__init__()\n",
    "        self.num_feats = num_feats\n",
    "        self.quants = torch.nn.ModuleList()\n",
    "        for ifeat in range(self.num_feats):\n",
    "            self.quants.append(torch.ao.quantization.QuantStub())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.quants[ifeat](x[..., ifeat:ifeat+1]) for ifeat in range(self.num_feats)], axis=-1)\n",
    "        \n",
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim=128,\n",
    "        num_heads=2,\n",
    "        width=128,\n",
    "        dropout_mha=0.1,\n",
    "        dropout_ff=0.1,\n",
    "        attention_type=\"efficient\",\n",
    "    ):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "\n",
    "        self.attention_type = attention_type\n",
    "        self.act = nn.ReLU\n",
    "        self.mha = torch.nn.MultiheadAttention(embedding_dim, num_heads, dropout=dropout_mha, batch_first=True)\n",
    "        self.norm0 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.norm1 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.seq = torch.nn.Sequential(\n",
    "            nn.Linear(embedding_dim, width), self.act(), nn.Linear(width, embedding_dim), self.act()\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(dropout_ff)\n",
    "\n",
    "        self.add0 = torch.ao.nn.quantized.FloatFunctional()\n",
    "        self.add1 = torch.ao.nn.quantized.FloatFunctional()\n",
    "        self.mul = torch.ao.nn.quantized.FloatFunctional()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        mha_out = self.mha(x, x, x, need_weights=False)[0]\n",
    "        x = self.add0.add(x, mha_out)\n",
    "        x = self.norm0(x)\n",
    "        x = self.add1.add(x, self.seq(x))\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(x)\n",
    "        # x = self.mul.mul(x, mask.unsqueeze(-1))\n",
    "        return x\n",
    "\n",
    "class RegressionOutput(nn.Module):\n",
    "    def __init__(self, embed_dim, width, act, dropout):\n",
    "        super(RegressionOutput, self).__init__()\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "        self.nn = ffn(embed_dim, 1, width, act, dropout)\n",
    "\n",
    "    def forward(self, elems, x, orig_value):\n",
    "        nn_out = self.nn(x)\n",
    "        nn_out = self.dequant(nn_out)\n",
    "        return orig_value + nn_out\n",
    "\n",
    "def ffn(input_dim, output_dim, width, act, dropout):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, width),\n",
    "        act(),\n",
    "        torch.nn.LayerNorm(width),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(width, output_dim),\n",
    "    )\n",
    "\n",
    "def transform_batch(Xbatch):\n",
    "    Xbatch = Xbatch.clone()\n",
    "    Xbatch[..., 1] = torch.log(Xbatch[..., 1])\n",
    "    Xbatch[..., 5] = torch.log(Xbatch[..., 5])\n",
    "    Xbatch[torch.isnan(Xbatch)] = 0.0\n",
    "    Xbatch[torch.isinf(Xbatch)] = 0.0\n",
    "    return Xbatch\n",
    "    \n",
    "def unpack_target(y):\n",
    "    ret = {}\n",
    "    ret[\"cls_id\"] = y[..., 0].long()\n",
    "\n",
    "    for i, feat in enumerate(Y_FEATURES):\n",
    "        if i >= 2:  # skip the cls and charge as they are defined above\n",
    "            ret[feat] = y[..., i].to(dtype=torch.float32)\n",
    "    ret[\"phi\"] = torch.atan2(ret[\"sin_phi\"], ret[\"cos_phi\"])\n",
    "    \n",
    "    # note ~ momentum = [\"pt\", \"eta\", \"sin_phi\", \"cos_phi\", \"energy\"]\n",
    "    ret[\"momentum\"] = y[..., 2:7].to(dtype=torch.float32)\n",
    "    ret[\"p4\"] = torch.cat(\n",
    "        [ret[\"pt\"].unsqueeze(1), ret[\"eta\"].unsqueeze(1), ret[\"phi\"].unsqueeze(1), ret[\"energy\"].unsqueeze(1)], axis=1\n",
    "    )\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def unpack_predictions(preds):\n",
    "    ret = {}\n",
    "    ret[\"cls_id_onehot\"], ret[\"momentum\"] = preds\n",
    "\n",
    "    ret[\"pt\"] = ret[\"momentum\"][..., 0]\n",
    "    ret[\"eta\"] = ret[\"momentum\"][..., 1]\n",
    "    ret[\"sin_phi\"] = ret[\"momentum\"][..., 2]\n",
    "    ret[\"cos_phi\"] = ret[\"momentum\"][..., 3]\n",
    "    ret[\"energy\"] = ret[\"momentum\"][..., 4]\n",
    "\n",
    "    ret[\"cls_id\"] = torch.argmax(ret[\"cls_id_onehot\"], axis=-1)\n",
    "    ret[\"phi\"] = torch.atan2(ret[\"sin_phi\"], ret[\"cos_phi\"])\n",
    "    ret[\"p4\"] = torch.cat(\n",
    "        [\n",
    "            ret[\"pt\"].unsqueeze(axis=-1),\n",
    "            ret[\"eta\"].unsqueeze(axis=-1),\n",
    "            ret[\"phi\"].unsqueeze(axis=-1),\n",
    "            ret[\"energy\"].unsqueeze(axis=-1),\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "    return ret\n",
    "\n",
    "class MLPF(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=16,\n",
    "        num_classes=6,\n",
    "        num_convs=2,\n",
    "        dropout_ff=0.0,\n",
    "        dropout_conv_reg_mha=0.0,\n",
    "        dropout_conv_reg_ff=0.0,\n",
    "        dropout_conv_id_mha=0.0,\n",
    "        dropout_conv_id_ff=0.0,\n",
    "        num_heads=16,\n",
    "        head_dim=16,\n",
    "        elemtypes=[0,1,2],\n",
    "    ):\n",
    "        super(MLPF, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.act = nn.ReLU\n",
    "        self.elemtypes = elemtypes\n",
    "        self.num_elemtypes = len(self.elemtypes)\n",
    "\n",
    "        embedding_dim = num_heads * head_dim\n",
    "        width = num_heads * head_dim\n",
    "        \n",
    "        self.nn0_id = ffn(self.input_dim, embedding_dim, width, self.act, dropout_ff)\n",
    "        self.nn0_reg = ffn(self.input_dim, embedding_dim, width, self.act, dropout_ff)\n",
    "        \n",
    "        self.conv_id = nn.ModuleList()\n",
    "        self.conv_reg = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_convs):\n",
    "            self.conv_id.append(\n",
    "                SelfAttentionLayer(\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    width=width,\n",
    "                    dropout_mha=dropout_conv_id_mha,\n",
    "                    dropout_ff=dropout_conv_id_ff,\n",
    "                )\n",
    "            )\n",
    "            self.conv_reg.append(\n",
    "                SelfAttentionLayer(\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    width=width,\n",
    "                    dropout_mha=dropout_conv_reg_mha,\n",
    "                    dropout_ff=dropout_conv_reg_ff,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        decoding_dim = self.input_dim + embedding_dim\n",
    "\n",
    "        # DNN that acts on the node level to predict the PID\n",
    "        self.nn_id = ffn(decoding_dim, num_classes, width, self.act, dropout_ff)\n",
    "\n",
    "        # elementwise DNN for node momentum regression\n",
    "        embed_dim = decoding_dim + num_classes\n",
    "        self.nn_pt = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_eta = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_sin_phi = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_cos_phi = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_energy = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        \n",
    "        self.quant = QuantizeFeaturesStub(self.input_dim + len(self.elemtypes))\n",
    "        self.dequant_id = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, X_features, mask):\n",
    "        Xfeat_transformed = transform_batch(X_features)\n",
    "        Xfeat_normed = self.quant(Xfeat_transformed)\n",
    "\n",
    "        embeddings_id, embeddings_reg = [], []\n",
    "        embedding_id = self.nn0_id(Xfeat_normed)\n",
    "        embedding_reg = self.nn0_reg(Xfeat_normed)\n",
    "        for num, conv in enumerate(self.conv_id):\n",
    "            conv_input = embedding_id if num == 0 else embeddings_id[-1]\n",
    "            out_padded = conv(conv_input, mask)\n",
    "            embeddings_id.append(out_padded)\n",
    "        for num, conv in enumerate(self.conv_reg):\n",
    "            conv_input = embedding_reg if num == 0 else embeddings_reg[-1]\n",
    "            out_padded = conv(conv_input, mask)\n",
    "            embeddings_reg.append(out_padded)\n",
    "\n",
    "        final_embedding_id = torch.cat([Xfeat_normed] + [embeddings_id[-1]], axis=-1)\n",
    "        preds_id = self.nn_id(final_embedding_id)\n",
    "\n",
    "        final_embedding_reg = torch.cat([Xfeat_normed] + [embeddings_reg[-1]] + [preds_id], axis=-1)\n",
    "        preds_pt = self.nn_pt(X_features, final_embedding_reg, X_features[..., 1:2])\n",
    "        preds_eta = self.nn_eta(X_features, final_embedding_reg, X_features[..., 2:3])\n",
    "        preds_sin_phi = self.nn_sin_phi(X_features, final_embedding_reg, X_features[..., 3:4])\n",
    "        preds_cos_phi = self.nn_cos_phi(X_features, final_embedding_reg, X_features[..., 4:5])\n",
    "        preds_energy = self.nn_energy(X_features, final_embedding_reg, X_features[..., 5:6])\n",
    "        preds_momentum = torch.cat([preds_pt, preds_eta, preds_sin_phi, preds_cos_phi, preds_energy], axis=-1)\n",
    "        \n",
    "        preds_id = self.dequant_id(preds_id)\n",
    "        return preds_id, preds_momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89c14772",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_events_train = 10000\n",
    "max_events_eval = 10000\n",
    "events_per_batch = 1000  # data points per batch\n",
    "nepochs = 10\n",
    "batch_size = 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "274e5c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# model = MLPF(input_dim=INPUT_DIM, num_classes=NUM_CLASSES).to(device=device)\n",
    "# optimizer = torch.optim.Adam(lr=1e-4, params=model.parameters())\n",
    "\n",
    "# # Define the desired number of features\n",
    "# desired_num_features = 300\n",
    "\n",
    "# # Modify the creation of X_features_padded tensor\n",
    "# X_features_padded = pad_sequence(X_features, batch_first=True)\n",
    "# # Get the current size\n",
    "# current_size = list(X_features_padded.size())\n",
    "# # Update the second dimension to the desired number of features\n",
    "# current_size[2] = desired_num_features\n",
    "# # Expand the tensor to the desired size\n",
    "# X_features_padded = torch.cat([X_features_padded, torch.zeros(current_size)], dim=2)\n",
    "\n",
    "# # Similarly, modify the creation of y_targets_padded tensor if necessary\n",
    "\n",
    "# # Now, X_features_padded tensor will have torch.Size([batch_size, seq_length, desired_num_features])\n",
    "\n",
    "# # Now, X_features_padded tensor will have torch.Size([batch_size, seq_length, desired_num_features])\n",
    "\n",
    "# #  function to stream data (trying to avoid memory error)\n",
    "# def stream_data(dataset, batch_size, events_per_batch, chunk_size=2000):\n",
    "#     iterator = iter(dataset)\n",
    "#     while True:\n",
    "#         batch = []\n",
    "#         try:\n",
    "#             for _ in range(events_per_batch):\n",
    "#                 batch.append(next(iterator))\n",
    "#         except StopIteration:\n",
    "#             break\n",
    "\n",
    "#         for i in range(0, len(batch), batch_size):\n",
    "#             yield batch[i:i + batch_size]\n",
    "\n",
    "# loss_vals_epochs = []\n",
    "# best_loss = float('inf')\n",
    "# patience = 3  \n",
    "# no_improvement = 0\n",
    "\n",
    "# for epoch in range(nepochs):\n",
    "#     loss_vals_steps = []\n",
    "    \n",
    "#     data_stream = stream_data(ds_train, batch_size, events_per_batch)\n",
    "#     data_stream = tqdm.tqdm(data_stream, total=len(ds_train) // (batch_size * events_per_batch))\n",
    "    \n",
    "#     for batch in data_stream:\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         X_features = [torch.tensor(elem[\"X\"]).to(torch.float32) for elem in batch]\n",
    "#         y_targets = [torch.tensor(elem[\"ygen\"]).to(torch.float32) for elem in batch]\n",
    "\n",
    "#         X_features_padded = pad_sequence(X_features, batch_first=True).to(device=device)\n",
    "#         y_targets_padded = pad_sequence(y_targets, batch_first=True).to(device=device)\n",
    "#         mask = X_features_padded[:, :, 0] != 0\n",
    "\n",
    "#         preds = model(X_features_padded, mask)\n",
    "#         preds_unpacked = unpack_predictions(preds)\n",
    "#         targets_unpacked = unpack_target(y_targets_padded)\n",
    "\n",
    "#         loss = mlpf_loss(targets_unpacked, preds_unpacked, mask)\n",
    "#         loss[\"Total\"].backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         loss_vals_steps.append(loss[\"Total\"].detach().cpu().item())\n",
    "\n",
    "#         del X_features, y_targets, X_features_padded, y_targets_padded, mask, preds, preds_unpacked, targets_unpacked, loss\n",
    "#         gc.collect()  #garbage collection\n",
    "\n",
    "#         data_stream.set_description(f\"Epoch {epoch}, Loss: {np.mean(loss_vals_steps):.2f}\")\n",
    "\n",
    "#     epoch_loss = np.mean(loss_vals_steps)\n",
    "#     loss_vals_epochs.append(epoch_loss)\n",
    "#     print(f\"Epoch {epoch}, loss={epoch_loss:.2f}\")\n",
    "\n",
    "#     if epoch_loss < best_loss:\n",
    "#         best_loss = epoch_loss\n",
    "#         no_improvement = 0\n",
    "#     else:\n",
    "#         no_improvement += 1\n",
    "#     if no_improvement >= patience:\n",
    "#         print(\"Early stopping: No improvement for\", patience, \"epochs\")\n",
    "#         break\n",
    "        \n",
    "# torch.save(model.state_dict(), 'trained_model_batch_size_150.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0de25de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Loss=4.10\n",
      "Loss=2.88\n",
      "Loss=2.44\n",
      "Loss=2.31\n",
      "Loss=2.15\n",
      "Loss=2.06\n",
      "Loss=1.96\n",
      "Loss=1.91\n",
      "Loss=1.93\n",
      "Loss=1.81\n",
      "Loss=1.82\n",
      "Loss=1.73\n",
      "Loss=1.72\n",
      "Loss=1.68\n",
      "Loss=1.72\n",
      "Loss=1.69\n",
      "Loss=1.64\n",
      "Loss=1.58\n",
      "Loss=1.64\n",
      "Loss=1.63\n",
      "Loss=1.59\n",
      "Loss=1.59\n",
      "Loss=1.57\n",
      "Loss=1.52\n",
      "Loss=1.52\n",
      "Loss=1.50\n",
      "Loss=1.47\n",
      "Loss=1.41\n",
      "Loss=1.43\n",
      "Loss=1.44\n",
      "Loss=1.37\n",
      "Loss=1.40\n",
      "Loss=1.39\n",
      "Loss=1.38\n",
      "Loss=1.39\n",
      "Loss=1.37\n",
      "Loss=1.29\n",
      "Loss=1.33\n",
      "Loss=1.31\n",
      "Loss=1.29\n",
      "Loss=1.28\n",
      "Loss=1.26\n",
      "Loss=1.28\n",
      "Loss=1.22\n",
      "Loss=1.27\n",
      "Loss=1.25\n",
      "Loss=1.23\n",
      "Loss=1.23\n",
      "Loss=1.22\n",
      "Loss=1.18\n",
      "Loss=1.17\n",
      "Loss=1.20\n",
      "Loss=1.16\n",
      "Loss=1.15\n",
      "Loss=1.15\n",
      "Loss=1.12\n",
      "Loss=1.19\n",
      "Loss=1.15\n",
      "Loss=1.12\n",
      "Loss=1.12\n",
      "Loss=1.11\n",
      "Loss=1.09\n",
      "Loss=1.11\n",
      "Loss=1.11\n",
      "Loss=1.06\n",
      "Loss=1.05\n",
      "Loss=1.07\n",
      "Loss=1.05\n",
      "Loss=1.07\n",
      "Loss=1.05\n",
      "Loss=1.05\n",
      "Loss=1.00\n",
      "Loss=1.04\n",
      "Loss=1.00\n",
      "Loss=1.02\n",
      "Loss=1.04\n",
      "Loss=1.00\n",
      "Loss=1.03\n",
      "Loss=1.01\n",
      "Loss=0.99\n",
      "Loss=1.00\n",
      "Loss=0.97\n",
      "Loss=1.06\n",
      "Loss=0.96\n",
      "Loss=0.95\n",
      "Loss=1.02\n",
      "Loss=1.00\n",
      "Loss=0.95\n",
      "Loss=0.95\n",
      "Loss=0.93\n",
      "Loss=0.98\n",
      "Loss=1.00\n",
      "Loss=0.95\n",
      "Loss=0.97\n",
      "Loss=0.97\n",
      "Loss=0.95\n",
      "Loss=0.99\n",
      "Loss=0.98\n",
      "Loss=0.94\n",
      "Loss=0.93\n",
      "Loss=0.94\n",
      "Loss=0.95\n",
      "Loss=0.91\n",
      "Loss=0.94\n",
      "Loss=0.91\n",
      "Loss=0.93\n",
      "Loss=0.94\n",
      "Loss=0.92\n",
      "Loss=0.91\n",
      "Loss=0.90\n",
      "Loss=0.95\n",
      "Loss=0.91\n",
      "Loss=0.91\n",
      "Loss=0.96\n",
      "Loss=0.89\n",
      "Loss=0.87\n",
      "Loss=0.90\n",
      "Loss=0.89\n",
      "Loss=0.88\n",
      "Loss=0.90\n",
      "Loss=0.85\n",
      "Loss=0.89\n",
      "Loss=0.88\n",
      "Loss=0.87\n",
      "Loss=0.85\n",
      "Loss=0.88\n",
      "Loss=0.85\n",
      "Loss=0.93\n",
      "Loss=0.85\n",
      "Loss=0.81\n",
      "Loss=0.88\n",
      "Loss=0.86\n",
      "Loss=0.83\n",
      "Loss=0.84\n",
      "Loss=0.88\n",
      "Loss=0.89\n",
      "Loss=0.85\n",
      "Loss=0.85\n",
      "Loss=0.86\n",
      "Loss=0.85\n",
      "Loss=0.83\n",
      "Loss=0.84\n",
      "Loss=0.83\n",
      "Loss=0.82\n",
      "Loss=0.85\n",
      "Loss=0.85\n",
      "Loss=0.88\n",
      "Loss=0.82\n",
      "Loss=0.86\n",
      "Loss=0.80\n",
      "Loss=0.79\n",
      "Loss=0.84\n",
      "Loss=0.87\n",
      "Loss=0.82\n",
      "Loss=0.81\n",
      "Loss=0.81\n",
      "Loss=0.82\n",
      "Loss=0.78\n",
      "Loss=0.84\n",
      "Loss=0.78\n",
      "Loss=0.85\n",
      "Loss=0.77\n",
      "Loss=0.81\n",
      "Loss=0.78\n",
      "Loss=0.82\n",
      "Loss=0.79\n",
      "Loss=0.80\n",
      "Loss=0.79\n",
      "Loss=0.78\n",
      "Loss=0.83\n",
      "Loss=0.81\n",
      "Loss=0.81\n",
      "Loss=0.79\n",
      "Loss=0.81\n",
      "Loss=0.78\n",
      "Loss=0.81\n",
      "Loss=0.77\n",
      "Loss=0.81\n",
      "Loss=0.81\n",
      "Loss=0.79\n",
      "Loss=0.76\n",
      "Loss=0.78\n",
      "Loss=0.80\n",
      "Loss=0.86\n",
      "Loss=0.87\n",
      "Loss=0.76\n",
      "Loss=0.78\n",
      "Loss=0.77\n",
      "Loss=0.77\n",
      "Loss=0.79\n",
      "Loss=0.79\n",
      "Loss=0.77\n",
      "Loss=0.87\n",
      "Loss=0.76\n",
      "Loss=0.77\n",
      "Loss=0.80\n",
      "Loss=0.82\n",
      "Loss=0.78\n",
      "Loss=0.77\n",
      "Loss=0.82\n",
      "Average loss for epoch 1: 1.09\n",
      "Epoch 2/10\n",
      "Loss=0.74\n",
      "Loss=0.77\n",
      "Loss=0.78\n",
      "Loss=0.80\n",
      "Loss=0.75\n",
      "Loss=0.80\n",
      "Loss=0.77\n",
      "Loss=0.75\n",
      "Loss=0.80\n",
      "Loss=0.75\n",
      "Loss=0.79\n",
      "Loss=0.76\n",
      "Loss=0.76\n",
      "Loss=0.76\n",
      "Loss=0.80\n",
      "Loss=0.78\n",
      "Loss=0.76\n",
      "Loss=0.76\n",
      "Loss=0.79\n",
      "Loss=0.82\n",
      "Loss=0.75\n",
      "Loss=0.77\n",
      "Loss=0.79\n",
      "Loss=0.75\n",
      "Loss=0.79\n",
      "Loss=0.78\n",
      "Loss=0.77\n",
      "Loss=0.74\n",
      "Loss=0.76\n",
      "Loss=0.77\n",
      "Loss=0.73\n",
      "Loss=0.78\n",
      "Loss=0.78\n",
      "Loss=0.76\n",
      "Loss=0.77\n",
      "Loss=0.77\n",
      "Loss=0.74\n",
      "Loss=0.76\n",
      "Loss=0.76\n",
      "Loss=0.75\n",
      "Loss=0.74\n",
      "Loss=0.74\n",
      "Loss=0.75\n",
      "Loss=0.73\n",
      "Loss=0.76\n",
      "Loss=0.77\n",
      "Loss=0.75\n",
      "Loss=0.76\n",
      "Loss=0.76\n",
      "Loss=0.73\n",
      "Loss=0.74\n",
      "Loss=0.77\n",
      "Loss=0.72\n",
      "Loss=0.72\n",
      "Loss=0.73\n",
      "Loss=0.72\n",
      "Loss=0.77\n",
      "Loss=0.75\n",
      "Loss=0.73\n",
      "Loss=0.73\n",
      "Loss=0.74\n",
      "Loss=0.72\n",
      "Loss=0.74\n",
      "Loss=0.75\n",
      "Loss=0.71\n",
      "Loss=0.70\n",
      "Loss=0.72\n",
      "Loss=0.72\n",
      "Loss=0.73\n",
      "Loss=0.73\n",
      "Loss=0.72\n",
      "Loss=0.69\n",
      "Loss=0.73\n",
      "Loss=0.68\n",
      "Loss=0.72\n",
      "Loss=0.74\n",
      "Loss=0.70\n",
      "Loss=0.74\n",
      "Loss=0.72\n",
      "Loss=0.71\n",
      "Loss=0.72\n",
      "Loss=0.69\n",
      "Loss=0.78\n",
      "Loss=0.70\n",
      "Loss=0.69\n",
      "Loss=0.76\n",
      "Loss=0.73\n",
      "Loss=0.71\n",
      "Loss=0.70\n",
      "Loss=0.69\n",
      "Loss=0.73\n",
      "Loss=0.75\n",
      "Loss=0.70\n",
      "Loss=0.74\n",
      "Loss=0.74\n",
      "Loss=0.71\n",
      "Loss=0.75\n",
      "Loss=0.75\n",
      "Loss=0.71\n",
      "Loss=0.71\n",
      "Loss=0.72\n",
      "Loss=0.73\n",
      "Loss=0.70\n",
      "Loss=0.73\n",
      "Loss=0.70\n",
      "Loss=0.71\n",
      "Loss=0.73\n",
      "Loss=0.71\n",
      "Loss=0.71\n",
      "Loss=0.71\n",
      "Loss=0.75\n",
      "Loss=0.71\n",
      "Loss=0.72\n",
      "Loss=0.77\n",
      "Loss=0.70\n",
      "Loss=0.69\n",
      "Loss=0.71\n",
      "Loss=0.71\n",
      "Loss=0.70\n",
      "Loss=0.71\n",
      "Loss=0.67\n",
      "Loss=0.71\n",
      "Loss=0.71\n",
      "Loss=0.70\n",
      "Loss=0.68\n",
      "Loss=0.71\n",
      "Loss=0.69\n",
      "Loss=0.76\n",
      "Loss=0.68\n",
      "Loss=0.65\n",
      "Loss=0.72\n",
      "Loss=0.70\n",
      "Loss=0.67\n",
      "Loss=0.68\n",
      "Loss=0.72\n",
      "Loss=0.73\n",
      "Loss=0.70\n",
      "Loss=0.70\n",
      "Loss=0.71\n",
      "Loss=0.70\n",
      "Loss=0.69\n",
      "Loss=0.70\n",
      "Loss=0.69\n",
      "Loss=0.68\n",
      "Loss=0.71\n",
      "Loss=0.71\n",
      "Loss=0.73\n",
      "Loss=0.68\n",
      "Loss=0.72\n",
      "Loss=0.66\n",
      "Loss=0.66\n",
      "Loss=0.71\n",
      "Loss=0.73\n",
      "Loss=0.69\n",
      "Loss=0.68\n",
      "Loss=0.68\n",
      "Loss=0.70\n",
      "Loss=0.66\n",
      "Loss=0.71\n",
      "Loss=0.66\n",
      "Loss=0.73\n",
      "Loss=0.65\n",
      "Loss=0.68\n",
      "Loss=0.67\n",
      "Loss=0.70\n",
      "Loss=0.67\n",
      "Loss=0.68\n",
      "Loss=0.68\n",
      "Loss=0.66\n",
      "Loss=0.72\n",
      "Loss=0.70\n",
      "Loss=0.69\n",
      "Loss=0.68\n",
      "Loss=0.70\n",
      "Loss=0.67\n",
      "Loss=0.69\n",
      "Loss=0.66\n",
      "Loss=0.70\n",
      "Loss=0.70\n",
      "Loss=0.67\n",
      "Loss=0.66\n",
      "Loss=0.67\n",
      "Loss=0.70\n",
      "Loss=0.75\n",
      "Loss=0.76\n",
      "Loss=0.65\n",
      "Loss=0.67\n",
      "Loss=0.66\n",
      "Loss=0.67\n",
      "Loss=0.69\n",
      "Loss=0.68\n",
      "Loss=0.67\n",
      "Loss=0.76\n",
      "Loss=0.66\n",
      "Loss=0.67\n",
      "Loss=0.71\n",
      "Loss=0.72\n",
      "Loss=0.69\n",
      "Loss=0.68\n",
      "Loss=0.72\n",
      "Average loss for epoch 2: 0.72\n",
      "Epoch 3/10\n",
      "Loss=0.65\n",
      "Loss=0.68\n",
      "Loss=0.69\n",
      "Loss=0.70\n",
      "Loss=0.66\n",
      "Loss=0.71\n",
      "Loss=0.68\n",
      "Loss=0.66\n",
      "Loss=0.71\n",
      "Loss=0.67\n",
      "Loss=0.69\n",
      "Loss=0.67\n",
      "Loss=0.68\n",
      "Loss=0.68\n",
      "Loss=0.71\n",
      "Loss=0.69\n",
      "Loss=0.67\n",
      "Loss=0.67\n",
      "Loss=0.70\n",
      "Loss=0.74\n",
      "Loss=0.67\n",
      "Loss=0.68\n",
      "Loss=0.70\n",
      "Loss=0.67\n",
      "Loss=0.71\n",
      "Loss=0.69\n",
      "Loss=0.69\n",
      "Loss=0.67\n",
      "Loss=0.68\n",
      "Loss=0.69\n",
      "Loss=0.65\n",
      "Loss=0.70\n",
      "Loss=0.69\n",
      "Loss=0.68\n",
      "Loss=0.69\n",
      "Loss=0.69\n",
      "Loss=0.66\n",
      "Loss=0.68\n",
      "Loss=0.69\n",
      "Loss=0.67\n",
      "Loss=0.66\n",
      "Loss=0.67\n",
      "Loss=0.68\n",
      "Loss=0.66\n",
      "Loss=0.68\n",
      "Loss=0.69\n",
      "Loss=0.68\n",
      "Loss=0.69\n",
      "Loss=0.69\n",
      "Loss=0.66\n",
      "Loss=0.67\n",
      "Loss=0.70\n",
      "Loss=0.64\n",
      "Loss=0.66\n",
      "Loss=0.66\n",
      "Loss=0.66\n",
      "Loss=0.70\n",
      "Loss=0.68\n",
      "Loss=0.67\n",
      "Loss=0.66\n",
      "Loss=0.67\n",
      "Loss=0.66\n",
      "Loss=0.67\n",
      "Loss=0.68\n",
      "Loss=0.65\n",
      "Loss=0.64\n",
      "Loss=0.65\n",
      "Loss=0.66\n",
      "Loss=0.66\n",
      "Loss=0.66\n",
      "Loss=0.66\n",
      "Loss=0.62\n",
      "Loss=0.66\n",
      "Loss=0.62\n",
      "Loss=0.66\n",
      "Loss=0.67\n",
      "Loss=0.64\n",
      "Loss=0.68\n",
      "Loss=0.66\n",
      "Loss=0.65\n",
      "Loss=0.66\n",
      "Loss=0.64\n",
      "Loss=0.72\n",
      "Loss=0.64\n",
      "Loss=0.63\n",
      "Loss=0.70\n",
      "Loss=0.67\n",
      "Loss=0.65\n",
      "Loss=0.64\n",
      "Loss=0.63\n",
      "Loss=0.67\n",
      "Loss=0.69\n",
      "Loss=0.65\n",
      "Loss=0.68\n",
      "Loss=0.68\n",
      "Loss=0.65\n",
      "Loss=0.69\n",
      "Loss=0.69\n",
      "Loss=0.66\n",
      "Loss=0.66\n",
      "Loss=0.67\n",
      "Loss=0.67\n",
      "Loss=0.65\n",
      "Loss=0.67\n",
      "Loss=0.65\n",
      "Loss=0.65\n",
      "Loss=0.67\n",
      "Loss=0.66\n",
      "Loss=0.65\n",
      "Loss=0.65\n",
      "Loss=0.69\n",
      "Loss=0.66\n",
      "Loss=0.67\n",
      "Loss=0.71\n",
      "Loss=0.65\n",
      "Loss=0.63\n",
      "Loss=0.66\n",
      "Loss=0.66\n",
      "Loss=0.65\n",
      "Loss=0.66\n",
      "Loss=0.62\n",
      "Loss=0.66\n",
      "Loss=0.67\n",
      "Loss=0.65\n",
      "Loss=0.63\n",
      "Loss=0.66\n",
      "Loss=0.64\n",
      "Loss=0.71\n",
      "Loss=0.63\n",
      "Loss=0.61\n",
      "Loss=0.67\n",
      "Loss=0.65\n",
      "Loss=0.63\n",
      "Loss=0.63\n",
      "Loss=0.67\n",
      "Loss=0.68\n",
      "Loss=0.65\n",
      "Loss=0.65\n",
      "Loss=0.67\n",
      "Loss=0.65\n",
      "Loss=0.65\n",
      "Loss=0.65\n",
      "Loss=0.64\n",
      "Loss=0.64\n",
      "Loss=0.66\n",
      "Loss=0.66\n",
      "Loss=0.69\n",
      "Loss=0.64\n",
      "Loss=0.67\n",
      "Loss=0.62\n",
      "Loss=0.62\n",
      "Loss=0.66\n",
      "Loss=0.68\n",
      "Loss=0.65\n",
      "Loss=0.64\n",
      "Loss=0.64\n",
      "Loss=0.66\n",
      "Loss=0.62\n",
      "Loss=0.67\n",
      "Loss=0.62\n",
      "Loss=0.68\n",
      "Loss=0.61\n",
      "Loss=0.64\n",
      "Loss=0.63\n",
      "Loss=0.65\n",
      "Loss=0.63\n",
      "Loss=0.64\n",
      "Loss=0.64\n",
      "Loss=0.62\n",
      "Loss=0.67\n",
      "Loss=0.65\n",
      "Loss=0.64\n",
      "Loss=0.64\n",
      "Loss=0.66\n",
      "Loss=0.63\n",
      "Loss=0.65\n",
      "Loss=0.62\n",
      "Loss=0.66\n",
      "Loss=0.66\n",
      "Loss=0.63\n",
      "Loss=0.62\n",
      "Loss=0.63\n",
      "Loss=0.65\n",
      "Loss=0.71\n",
      "Loss=0.72\n",
      "Loss=0.61\n",
      "Loss=0.63\n",
      "Loss=0.62\n",
      "Loss=0.63\n",
      "Loss=0.65\n",
      "Loss=0.65\n",
      "Loss=0.63\n",
      "Loss=0.72\n",
      "Loss=0.62\n",
      "Loss=0.63\n",
      "Loss=0.67\n",
      "Loss=0.68\n",
      "Loss=0.65\n",
      "Loss=0.64\n",
      "Loss=0.69\n",
      "Average loss for epoch 3: 0.66\n",
      "Epoch 4/10\n",
      "Loss=0.61\n",
      "Loss=0.64\n",
      "Loss=0.65\n",
      "Loss=0.67\n",
      "Loss=0.63\n",
      "Loss=0.67\n",
      "Loss=0.64\n",
      "Loss=0.62\n",
      "Loss=0.67\n",
      "Loss=0.63\n",
      "Loss=0.65\n",
      "Loss=0.63\n",
      "Loss=0.64\n",
      "Loss=0.64\n",
      "Loss=0.68\n",
      "Loss=0.65\n",
      "Loss=0.64\n",
      "Loss=0.64\n",
      "Loss=0.66\n",
      "Loss=0.71\n",
      "Loss=0.63\n",
      "Loss=0.65\n",
      "Loss=0.67\n",
      "Loss=0.64\n",
      "Loss=0.67\n",
      "Loss=0.66\n",
      "Loss=0.65\n",
      "Loss=0.63\n",
      "Loss=0.65\n",
      "Loss=0.66\n",
      "Loss=0.61\n",
      "Loss=0.66\n",
      "Loss=0.66\n",
      "Loss=0.65\n",
      "Loss=0.65\n",
      "Loss=0.65\n",
      "Loss=0.63\n",
      "Loss=0.64\n",
      "Loss=0.66\n",
      "Loss=0.64\n",
      "Loss=0.63\n",
      "Loss=0.63\n",
      "Loss=0.65\n",
      "Loss=0.62\n",
      "Loss=0.65\n",
      "Loss=0.66\n",
      "Loss=0.65\n",
      "Loss=0.65\n",
      "Loss=0.65\n",
      "Loss=0.62\n",
      "Loss=0.63\n",
      "Loss=0.66\n",
      "Loss=0.61\n",
      "Loss=0.62\n",
      "Loss=0.63\n",
      "Loss=0.62\n",
      "Loss=0.67\n",
      "Loss=0.65\n",
      "Loss=0.63\n",
      "Loss=0.63\n",
      "Loss=0.64\n",
      "Loss=0.63\n",
      "Loss=0.64\n",
      "Loss=0.65\n",
      "Loss=0.62\n",
      "Loss=0.61\n",
      "Loss=0.62\n",
      "Loss=0.63\n",
      "Loss=0.63\n",
      "Loss=0.63\n",
      "Loss=0.62\n",
      "Loss=0.59\n",
      "Loss=0.63\n",
      "Loss=0.59\n",
      "Loss=0.63\n",
      "Loss=0.64\n",
      "Loss=0.61\n",
      "Loss=0.65\n",
      "Loss=0.63\n",
      "Loss=0.62\n",
      "Loss=0.63\n",
      "Loss=0.61\n",
      "Loss=0.69\n",
      "Loss=0.61\n",
      "Loss=0.60\n",
      "Loss=0.67\n",
      "Loss=0.64\n",
      "Loss=0.62\n",
      "Loss=0.61\n",
      "Loss=0.60\n",
      "Loss=0.64\n",
      "Loss=0.66\n",
      "Loss=0.62\n",
      "Loss=0.65\n",
      "Loss=0.65\n",
      "Loss=0.62\n",
      "Loss=0.66\n",
      "Loss=0.66\n",
      "Loss=0.63\n",
      "Loss=0.63\n",
      "Loss=0.64\n",
      "Loss=0.64\n",
      "Loss=0.62\n",
      "Loss=0.64\n",
      "Loss=0.62\n",
      "Loss=0.63\n",
      "Loss=0.64\n",
      "Loss=0.63\n",
      "Loss=0.62\n",
      "Loss=0.63\n",
      "Loss=0.66\n",
      "Loss=0.63\n",
      "Loss=0.64\n",
      "Loss=0.68\n",
      "Loss=0.62\n",
      "Loss=0.60\n",
      "Loss=0.63\n",
      "Loss=0.64\n",
      "Loss=0.62\n",
      "Loss=0.63\n",
      "Loss=0.59\n",
      "Loss=0.63\n",
      "Loss=0.64\n",
      "Loss=0.63\n",
      "Loss=0.61\n",
      "Loss=0.64\n",
      "Loss=0.62\n",
      "Loss=0.69\n",
      "Loss=0.61\n",
      "Loss=0.58\n",
      "Loss=0.64\n",
      "Loss=0.63\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.65\n",
      "Loss=0.65\n",
      "Loss=0.63\n",
      "Loss=0.62\n",
      "Loss=0.64\n",
      "Loss=0.63\n",
      "Loss=0.62\n",
      "Loss=0.63\n",
      "Loss=0.61\n",
      "Loss=0.61\n",
      "Loss=0.64\n",
      "Loss=0.63\n",
      "Loss=0.66\n",
      "Loss=0.61\n",
      "Loss=0.64\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.64\n",
      "Loss=0.66\n",
      "Loss=0.62\n",
      "Loss=0.62\n",
      "Loss=0.61\n",
      "Loss=0.63\n",
      "Loss=0.59\n",
      "Loss=0.64\n",
      "Loss=0.60\n",
      "Loss=0.66\n",
      "Loss=0.58\n",
      "Loss=0.62\n",
      "Loss=0.61\n",
      "Loss=0.63\n",
      "Loss=0.60\n",
      "Loss=0.62\n",
      "Loss=0.62\n",
      "Loss=0.60\n",
      "Loss=0.64\n",
      "Loss=0.63\n",
      "Loss=0.62\n",
      "Loss=0.62\n",
      "Loss=0.64\n",
      "Loss=0.61\n",
      "Loss=0.63\n",
      "Loss=0.59\n",
      "Loss=0.63\n",
      "Loss=0.64\n",
      "Loss=0.61\n",
      "Loss=0.59\n",
      "Loss=0.61\n",
      "Loss=0.63\n",
      "Loss=0.69\n",
      "Loss=0.70\n",
      "Loss=0.59\n",
      "Loss=0.61\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.63\n",
      "Loss=0.62\n",
      "Loss=0.60\n",
      "Loss=0.70\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.65\n",
      "Loss=0.66\n",
      "Loss=0.63\n",
      "Loss=0.62\n",
      "Loss=0.67\n",
      "Average loss for epoch 4: 0.63\n",
      "Epoch 5/10\n",
      "Loss=0.59\n",
      "Loss=0.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.63\n",
      "Loss=0.64\n",
      "Loss=0.61\n",
      "Loss=0.65\n",
      "Loss=0.62\n",
      "Loss=0.60\n",
      "Loss=0.65\n",
      "Loss=0.61\n",
      "Loss=0.63\n",
      "Loss=0.61\n",
      "Loss=0.62\n",
      "Loss=0.62\n",
      "Loss=0.65\n",
      "Loss=0.63\n",
      "Loss=0.61\n",
      "Loss=0.62\n",
      "Loss=0.64\n",
      "Loss=0.69\n",
      "Loss=0.61\n",
      "Loss=0.63\n",
      "Loss=0.64\n",
      "Loss=0.62\n",
      "Loss=0.65\n",
      "Loss=0.64\n",
      "Loss=0.63\n",
      "Loss=0.61\n",
      "Loss=0.62\n",
      "Loss=0.64\n",
      "Loss=0.59\n",
      "Loss=0.64\n",
      "Loss=0.64\n",
      "Loss=0.63\n",
      "Loss=0.63\n",
      "Loss=0.63\n",
      "Loss=0.61\n",
      "Loss=0.62\n",
      "Loss=0.64\n",
      "Loss=0.62\n",
      "Loss=0.61\n",
      "Loss=0.61\n",
      "Loss=0.63\n",
      "Loss=0.60\n",
      "Loss=0.63\n",
      "Loss=0.64\n",
      "Loss=0.63\n",
      "Loss=0.63\n",
      "Loss=0.63\n",
      "Loss=0.61\n",
      "Loss=0.62\n",
      "Loss=0.64\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.61\n",
      "Loss=0.65\n",
      "Loss=0.64\n",
      "Loss=0.61\n",
      "Loss=0.61\n",
      "Loss=0.62\n",
      "Loss=0.61\n",
      "Loss=0.62\n",
      "Loss=0.63\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.61\n",
      "Loss=0.61\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.61\n",
      "Loss=0.58\n",
      "Loss=0.62\n",
      "Loss=0.63\n",
      "Loss=0.59\n",
      "Loss=0.63\n",
      "Loss=0.62\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.59\n",
      "Loss=0.67\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.65\n",
      "Loss=0.62\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.63\n",
      "Loss=0.65\n",
      "Loss=0.60\n",
      "Loss=0.63\n",
      "Loss=0.63\n",
      "Loss=0.60\n",
      "Loss=0.65\n",
      "Loss=0.64\n",
      "Loss=0.62\n",
      "Loss=0.61\n",
      "Loss=0.62\n",
      "Loss=0.63\n",
      "Loss=0.60\n",
      "Loss=0.62\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.62\n",
      "Loss=0.61\n",
      "Loss=0.61\n",
      "Loss=0.61\n",
      "Loss=0.65\n",
      "Loss=0.61\n",
      "Loss=0.63\n",
      "Loss=0.67\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.61\n",
      "Loss=0.62\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.58\n",
      "Loss=0.62\n",
      "Loss=0.62\n",
      "Loss=0.61\n",
      "Loss=0.59\n",
      "Loss=0.62\n",
      "Loss=0.60\n",
      "Loss=0.67\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.62\n",
      "Loss=0.62\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.63\n",
      "Loss=0.64\n",
      "Loss=0.62\n",
      "Loss=0.61\n",
      "Loss=0.63\n",
      "Loss=0.61\n",
      "Loss=0.61\n",
      "Loss=0.61\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.62\n",
      "Loss=0.62\n",
      "Loss=0.64\n",
      "Loss=0.59\n",
      "Loss=0.63\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.62\n",
      "Loss=0.64\n",
      "Loss=0.61\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.62\n",
      "Loss=0.58\n",
      "Loss=0.63\n",
      "Loss=0.59\n",
      "Loss=0.64\n",
      "Loss=0.57\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.61\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.63\n",
      "Loss=0.62\n",
      "Loss=0.61\n",
      "Loss=0.60\n",
      "Loss=0.62\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.58\n",
      "Loss=0.62\n",
      "Loss=0.62\n",
      "Loss=0.59\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.62\n",
      "Loss=0.68\n",
      "Loss=0.68\n",
      "Loss=0.57\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.60\n",
      "Loss=0.62\n",
      "Loss=0.61\n",
      "Loss=0.59\n",
      "Loss=0.68\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.64\n",
      "Loss=0.64\n",
      "Loss=0.61\n",
      "Loss=0.61\n",
      "Loss=0.65\n",
      "Average loss for epoch 5: 0.62\n",
      "Epoch 6/10\n",
      "Loss=0.58\n",
      "Loss=0.60\n",
      "Loss=0.62\n",
      "Loss=0.63\n",
      "Loss=0.60\n",
      "Loss=0.63\n",
      "Loss=0.61\n",
      "Loss=0.59\n",
      "Loss=0.64\n",
      "Loss=0.60\n",
      "Loss=0.62\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.64\n",
      "Loss=0.62\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.63\n",
      "Loss=0.67\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.63\n",
      "Loss=0.61\n",
      "Loss=0.64\n",
      "Loss=0.62\n",
      "Loss=0.62\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.63\n",
      "Loss=0.58\n",
      "Loss=0.62\n",
      "Loss=0.62\n",
      "Loss=0.61\n",
      "Loss=0.62\n",
      "Loss=0.62\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.62\n",
      "Loss=0.61\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.59\n",
      "Loss=0.62\n",
      "Loss=0.63\n",
      "Loss=0.62\n",
      "Loss=0.62\n",
      "Loss=0.62\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.63\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.64\n",
      "Loss=0.62\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.62\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.56\n",
      "Loss=0.60\n",
      "Loss=0.56\n",
      "Loss=0.60\n",
      "Loss=0.62\n",
      "Loss=0.58\n",
      "Loss=0.62\n",
      "Loss=0.61\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.66\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.64\n",
      "Loss=0.61\n",
      "Loss=0.59\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.62\n",
      "Loss=0.63\n",
      "Loss=0.59\n",
      "Loss=0.62\n",
      "Loss=0.62\n",
      "Loss=0.59\n",
      "Loss=0.63\n",
      "Loss=0.63\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.61\n",
      "Loss=0.59\n",
      "Loss=0.61\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.63\n",
      "Loss=0.60\n",
      "Loss=0.62\n",
      "Loss=0.65\n",
      "Loss=0.59\n",
      "Loss=0.58\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.57\n",
      "Loss=0.61\n",
      "Loss=0.61\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.61\n",
      "Loss=0.59\n",
      "Loss=0.66\n",
      "Loss=0.58\n",
      "Loss=0.56\n",
      "Loss=0.61\n",
      "Loss=0.61\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.62\n",
      "Loss=0.63\n",
      "Loss=0.61\n",
      "Loss=0.60\n",
      "Loss=0.62\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.61\n",
      "Loss=0.61\n",
      "Loss=0.63\n",
      "Loss=0.58\n",
      "Loss=0.62\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.61\n",
      "Loss=0.63\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.61\n",
      "Loss=0.57\n",
      "Loss=0.62\n",
      "Loss=0.58\n",
      "Loss=0.63\n",
      "Loss=0.56\n",
      "Loss=0.59\n",
      "Loss=0.58\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.62\n",
      "Loss=0.61\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.57\n",
      "Loss=0.61\n",
      "Loss=0.61\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.60\n",
      "Loss=0.67\n",
      "Loss=0.67\n",
      "Loss=0.56\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.59\n",
      "Loss=0.61\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.67\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.63\n",
      "Loss=0.63\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.64\n",
      "Average loss for epoch 6: 0.60\n",
      "Epoch 7/10\n",
      "Loss=0.57\n",
      "Loss=0.59\n",
      "Loss=0.61\n",
      "Loss=0.62\n",
      "Loss=0.59\n",
      "Loss=0.62\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.63\n",
      "Loss=0.59\n",
      "Loss=0.61\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.63\n",
      "Loss=0.61\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.62\n",
      "Loss=0.66\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.62\n",
      "Loss=0.60\n",
      "Loss=0.63\n",
      "Loss=0.62\n",
      "Loss=0.61\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.62\n",
      "Loss=0.57\n",
      "Loss=0.61\n",
      "Loss=0.61\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.61\n",
      "Loss=0.58\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.61\n",
      "Loss=0.62\n",
      "Loss=0.61\n",
      "Loss=0.61\n",
      "Loss=0.61\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.61\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.58\n",
      "Loss=0.63\n",
      "Loss=0.62\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.58\n",
      "Loss=0.56\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.58\n",
      "Loss=0.55\n",
      "Loss=0.59\n",
      "Loss=0.55\n",
      "Loss=0.59\n",
      "Loss=0.61\n",
      "Loss=0.57\n",
      "Loss=0.61\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.64\n",
      "Loss=0.57\n",
      "Loss=0.56\n",
      "Loss=0.63\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.56\n",
      "Loss=0.61\n",
      "Loss=0.62\n",
      "Loss=0.59\n",
      "Loss=0.61\n",
      "Loss=0.61\n",
      "Loss=0.58\n",
      "Loss=0.62\n",
      "Loss=0.62\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.62\n",
      "Loss=0.59\n",
      "Loss=0.61\n",
      "Loss=0.64\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.56\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.57\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.65\n",
      "Loss=0.57\n",
      "Loss=0.55\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.56\n",
      "Loss=0.57\n",
      "Loss=0.62\n",
      "Loss=0.62\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.61\n",
      "Loss=0.59\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.62\n",
      "Loss=0.57\n",
      "Loss=0.61\n",
      "Loss=0.56\n",
      "Loss=0.56\n",
      "Loss=0.60\n",
      "Loss=0.62\n",
      "Loss=0.59\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.60\n",
      "Loss=0.56\n",
      "Loss=0.61\n",
      "Loss=0.57\n",
      "Loss=0.62\n",
      "Loss=0.55\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.56\n",
      "Loss=0.61\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.56\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.57\n",
      "Loss=0.56\n",
      "Loss=0.57\n",
      "Loss=0.60\n",
      "Loss=0.66\n",
      "Loss=0.66\n",
      "Loss=0.55\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.66\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.62\n",
      "Loss=0.62\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.63\n",
      "Average loss for epoch 7: 0.59\n",
      "Epoch 8/10\n",
      "Loss=0.56\n",
      "Loss=0.58\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.58\n",
      "Loss=0.61\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.62\n",
      "Loss=0.58\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.62\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.61\n",
      "Loss=0.65\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.61\n",
      "Loss=0.59\n",
      "Loss=0.62\n",
      "Loss=0.61\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.61\n",
      "Loss=0.56\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.56\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.62\n",
      "Loss=0.61\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.57\n",
      "Loss=0.56\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.55\n",
      "Loss=0.58\n",
      "Loss=0.55\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.56\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.56\n",
      "Loss=0.64\n",
      "Loss=0.57\n",
      "Loss=0.56\n",
      "Loss=0.62\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.56\n",
      "Loss=0.55\n",
      "Loss=0.60\n",
      "Loss=0.61\n",
      "Loss=0.58\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.57\n",
      "Loss=0.62\n",
      "Loss=0.61\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.61\n",
      "Loss=0.58\n",
      "Loss=0.60\n",
      "Loss=0.63\n",
      "Loss=0.58\n",
      "Loss=0.56\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.55\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.64\n",
      "Loss=0.56\n",
      "Loss=0.54\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.55\n",
      "Loss=0.56\n",
      "Loss=0.61\n",
      "Loss=0.61\n",
      "Loss=0.59\n",
      "Loss=0.58\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.61\n",
      "Loss=0.56\n",
      "Loss=0.60\n",
      "Loss=0.55\n",
      "Loss=0.55\n",
      "Loss=0.59\n",
      "Loss=0.61\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.59\n",
      "Loss=0.56\n",
      "Loss=0.60\n",
      "Loss=0.56\n",
      "Loss=0.61\n",
      "Loss=0.54\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.59\n",
      "Loss=0.56\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.55\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.60\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.55\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.55\n",
      "Loss=0.56\n",
      "Loss=0.58\n",
      "Loss=0.65\n",
      "Loss=0.65\n",
      "Loss=0.54\n",
      "Loss=0.57\n",
      "Loss=0.56\n",
      "Loss=0.57\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.56\n",
      "Loss=0.65\n",
      "Loss=0.56\n",
      "Loss=0.57\n",
      "Loss=0.62\n",
      "Loss=0.62\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.63\n",
      "Average loss for epoch 8: 0.59\n",
      "Epoch 9/10\n",
      "Loss=0.55\n",
      "Loss=0.57\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.61\n",
      "Loss=0.58\n",
      "Loss=0.56\n",
      "Loss=0.61\n",
      "Loss=0.57\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.61\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.60\n",
      "Loss=0.64\n",
      "Loss=0.57\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.61\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.60\n",
      "Loss=0.55\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.55\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.61\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.55\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.56\n",
      "Loss=0.54\n",
      "Loss=0.57\n",
      "Loss=0.54\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.55\n",
      "Loss=0.59\n",
      "Loss=0.58\n",
      "Loss=0.56\n",
      "Loss=0.58\n",
      "Loss=0.55\n",
      "Loss=0.63\n",
      "Loss=0.56\n",
      "Loss=0.55\n",
      "Loss=0.61\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.55\n",
      "Loss=0.55\n",
      "Loss=0.59\n",
      "Loss=0.60\n",
      "Loss=0.57\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.56\n",
      "Loss=0.61\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.56\n",
      "Loss=0.58\n",
      "Loss=0.56\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.61\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.63\n",
      "Loss=0.57\n",
      "Loss=0.55\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.54\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.58\n",
      "Loss=0.56\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.64\n",
      "Loss=0.55\n",
      "Loss=0.53\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.55\n",
      "Loss=0.55\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.56\n",
      "Loss=0.56\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.60\n",
      "Loss=0.56\n",
      "Loss=0.59\n",
      "Loss=0.54\n",
      "Loss=0.54\n",
      "Loss=0.58\n",
      "Loss=0.60\n",
      "Loss=0.57\n",
      "Loss=0.56\n",
      "Loss=0.56\n",
      "Loss=0.58\n",
      "Loss=0.55\n",
      "Loss=0.59\n",
      "Loss=0.55\n",
      "Loss=0.60\n",
      "Loss=0.53\n",
      "Loss=0.56\n",
      "Loss=0.56\n",
      "Loss=0.58\n",
      "Loss=0.55\n",
      "Loss=0.56\n",
      "Loss=0.57\n",
      "Loss=0.54\n",
      "Loss=0.59\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.54\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.56\n",
      "Loss=0.55\n",
      "Loss=0.55\n",
      "Loss=0.58\n",
      "Loss=0.64\n",
      "Loss=0.64\n",
      "Loss=0.54\n",
      "Loss=0.56\n",
      "Loss=0.55\n",
      "Loss=0.56\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.55\n",
      "Loss=0.64\n",
      "Loss=0.55\n",
      "Loss=0.56\n",
      "Loss=0.61\n",
      "Loss=0.61\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.62\n",
      "Average loss for epoch 9: 0.58\n",
      "Epoch 10/10\n",
      "Loss=0.55\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.56\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.55\n",
      "Loss=0.60\n",
      "Loss=0.56\n",
      "Loss=0.58\n",
      "Loss=0.56\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.56\n",
      "Loss=0.57\n",
      "Loss=0.59\n",
      "Loss=0.64\n",
      "Loss=0.56\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.58\n",
      "Loss=0.56\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.54\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.56\n",
      "Loss=0.57\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.56\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.56\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.58\n",
      "Loss=0.56\n",
      "Loss=0.57\n",
      "Loss=0.59\n",
      "Loss=0.54\n",
      "Loss=0.56\n",
      "Loss=0.56\n",
      "Loss=0.56\n",
      "Loss=0.60\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.56\n",
      "Loss=0.56\n",
      "Loss=0.56\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.56\n",
      "Loss=0.54\n",
      "Loss=0.56\n",
      "Loss=0.56\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.55\n",
      "Loss=0.53\n",
      "Loss=0.56\n",
      "Loss=0.53\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.55\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.55\n",
      "Loss=0.57\n",
      "Loss=0.54\n",
      "Loss=0.62\n",
      "Loss=0.55\n",
      "Loss=0.54\n",
      "Loss=0.60\n",
      "Loss=0.58\n",
      "Loss=0.56\n",
      "Loss=0.54\n",
      "Loss=0.54\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.56\n",
      "Loss=0.59\n",
      "Loss=0.58\n",
      "Loss=0.56\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.55\n",
      "Loss=0.57\n",
      "Loss=0.55\n",
      "Loss=0.56\n",
      "Loss=0.57\n",
      "Loss=0.56\n",
      "Loss=0.56\n",
      "Loss=0.56\n",
      "Loss=0.60\n",
      "Loss=0.57\n",
      "Loss=0.58\n",
      "Loss=0.62\n",
      "Loss=0.56\n",
      "Loss=0.55\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.56\n",
      "Loss=0.57\n",
      "Loss=0.54\n",
      "Loss=0.58\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.55\n",
      "Loss=0.58\n",
      "Loss=0.56\n",
      "Loss=0.63\n",
      "Loss=0.54\n",
      "Loss=0.52\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.54\n",
      "Loss=0.55\n",
      "Loss=0.59\n",
      "Loss=0.59\n",
      "Loss=0.57\n",
      "Loss=0.56\n",
      "Loss=0.58\n",
      "Loss=0.56\n",
      "Loss=0.56\n",
      "Loss=0.56\n",
      "Loss=0.55\n",
      "Loss=0.55\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.59\n",
      "Loss=0.55\n",
      "Loss=0.58\n",
      "Loss=0.53\n",
      "Loss=0.53\n",
      "Loss=0.58\n",
      "Loss=0.59\n",
      "Loss=0.56\n",
      "Loss=0.56\n",
      "Loss=0.56\n",
      "Loss=0.57\n",
      "Loss=0.54\n",
      "Loss=0.59\n",
      "Loss=0.55\n",
      "Loss=0.59\n",
      "Loss=0.52\n",
      "Loss=0.56\n",
      "Loss=0.55\n",
      "Loss=0.57\n",
      "Loss=0.54\n",
      "Loss=0.56\n",
      "Loss=0.56\n",
      "Loss=0.54\n",
      "Loss=0.58\n",
      "Loss=0.57\n",
      "Loss=0.56\n",
      "Loss=0.56\n",
      "Loss=0.58\n",
      "Loss=0.56\n",
      "Loss=0.56\n",
      "Loss=0.53\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.55\n",
      "Loss=0.54\n",
      "Loss=0.55\n",
      "Loss=0.57\n",
      "Loss=0.63\n",
      "Loss=0.63\n",
      "Loss=0.53\n",
      "Loss=0.55\n",
      "Loss=0.54\n",
      "Loss=0.55\n",
      "Loss=0.57\n",
      "Loss=0.57\n",
      "Loss=0.55\n",
      "Loss=0.64\n",
      "Loss=0.55\n",
      "Loss=0.56\n",
      "Loss=0.60\n",
      "Loss=0.60\n",
      "Loss=0.57\n",
      "Loss=0.56\n",
      "Loss=0.61\n",
      "Average loss for epoch 10: 0.57\n"
     ]
    }
   ],
   "source": [
    "max_events_train = 10000\n",
    "max_events_eval = 10000\n",
    "events_per_batch = 50\n",
    "\n",
    "max_epochs = 10\n",
    "losses = []\n",
    "\n",
    "best_loss = float('inf')\n",
    "patience = 3\n",
    "no_improvement = 0\n",
    "\n",
    "model = MLPF(input_dim=INPUT_DIM, num_classes=NUM_CLASSES).to(device=device)\n",
    "optimizer = torch.optim.Adam(lr=1e-4, params=model.parameters())\n",
    "\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{max_epochs}\")\n",
    "    \n",
    "    # Training loop\n",
    "    inds_train = range(0, max_events_train, events_per_batch)\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    for ind in inds_train:\n",
    "        optimizer.zero_grad()\n",
    "        ds_elems = [ds_train[i] for i in range(ind, ind + events_per_batch)]\n",
    "        X_features = [torch.tensor(elem[\"X\"]).to(torch.float32).to(device=device) for elem in ds_elems]\n",
    "        X_features_padded = pad_sequence(X_features, batch_first=True).to(device=device)\n",
    "        y_targets = [torch.tensor(elem[\"ygen\"]).to(torch.float32).to(device=device) for elem in ds_elems]\n",
    "        y_targets_padded = pad_sequence(y_targets, batch_first=True).to(device=device)\n",
    "\n",
    "        mask = X_features_padded[:, :, 0] != 0\n",
    "\n",
    "        preds = model(X_features_padded, mask)\n",
    "        preds_unpacked = unpack_predictions(preds)\n",
    "        targets_unpacked = unpack_target(y_targets_padded)\n",
    "        loss = mlpf_loss(targets_unpacked, preds_unpacked, mask)\n",
    "        loss[\"Total\"].backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss = loss[\"Total\"].detach().item()\n",
    "        total_loss += current_loss\n",
    "        num_batches += 1\n",
    "        losses.append(current_loss)\n",
    "        print(\"Loss={:.2f}\".format(loss[\"Total\"].detach().item()))\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Average loss for epoch {epoch}: {avg_loss:.2f}\")\n",
    "    \n",
    "    # Save model if the current loss is the best so far\n",
    "    if avg_loss < best_loss:\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        best_loss = avg_loss\n",
    "        no_improvement = 0\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "    \n",
    "    # Check for early stopping\n",
    "    if no_improvement >= patience:\n",
    "        print(\"Early stopping: No improvement for\", patience, \"epochs\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "247e0ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model\n",
    "# Load the saved model\n",
    "loaded_model = MLPF(input_dim=INPUT_DIM, num_classes=NUM_CLASSES).to(device=device)\n",
    "loaded_model.load_state_dict(torch.load('trained_model_batch_size_150.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdfd112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(batch_inds)\n",
    "# print(f'X_features',len(X_features))\n",
    "# print(f'y_features',len(y_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da977488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_stream = stream_data(ds_train, batch_size, events_per_batch)\n",
    "# batch = next(data_stream)  \n",
    "# X_features_length = [len(torch.tensor(elem[\"X\"]).to(torch.float32)) for elem in batch]\n",
    "# print(\"Length of X_features:\", X_features_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6e99ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the model back on CPU\n",
    "model = loaded_model.to(device=\"cpu\")\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "ds_elems = [ds_train[i] for i in range(max_events_train, max_events_train + max_events_eval)]\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 3000\n",
    "for i in range(0, len(ds_elems), batch_size):\n",
    "    batch_elems = ds_elems[i:i + batch_size]\n",
    "    # input features\n",
    "    X_features = [torch.tensor(elem[\"X\"]).to(torch.float32) for elem in batch_elems]\n",
    "    X_features_padded = pad_sequence(X_features, batch_first=True)\n",
    "    #  target labels\n",
    "    y_targets = [torch.tensor(elem[\"ygen\"]).to(torch.float32) for elem in batch_elems]\n",
    "    y_targets_padded = pad_sequence(y_targets, batch_first=True)\n",
    "    #  mask for the batch\n",
    "    mask = X_features_padded[:, :, 0] != 0\n",
    "    #  model prediction, loss computation\n",
    "    preds = model(X_features_padded, mask)\n",
    "    preds = preds[0].detach(), preds[1].detach()\n",
    "    # Update mask for the batch\n",
    "    mask = X_features_padded[:, :, 0] != 0\n",
    "    # Unpack predictions and targets for the batch\n",
    "    preds_unpacked = unpack_predictions(preds)\n",
    "    targets_unpacked = unpack_target(y_targets_padded)\n",
    "    # append to a list \n",
    "    all_preds.append(preds_unpacked)\n",
    "    all_targets.append(targets_unpacked)\n",
    "    # Compute loss for the batch\n",
    "    loss = mlpf_loss(targets_unpacked, preds_unpacked, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "136665fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_elems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f26b812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6780953c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b67a0fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_targets_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b3a6633",
   "metadata": {},
   "outputs": [],
   "source": [
    "msk_true_particles = targets_unpacked[\"cls_id\"]!=0\n",
    "msk_pred_particles = preds_unpacked[\"cls_id\"]!=0\n",
    "\n",
    "\n",
    "pt_target = targets_unpacked[\"pt\"][msk_true_particles].numpy()\n",
    "pt_pred = preds_unpacked[\"pt\"][msk_true_particles].numpy()\n",
    "\n",
    "eta_target = targets_unpacked[\"eta\"][msk_true_particles].numpy()\n",
    "eta_pred = preds_unpacked[\"eta\"][msk_true_particles].numpy()\n",
    "\n",
    "sphi_target = targets_unpacked[\"sin_phi\"][msk_true_particles].numpy()\n",
    "sphi_pred = preds_unpacked[\"sin_phi\"][msk_true_particles].numpy()\n",
    "\n",
    "cphi_target = targets_unpacked[\"cos_phi\"][msk_true_particles].numpy()\n",
    "cphi_pred = preds_unpacked[\"cos_phi\"][msk_true_particles].numpy()\n",
    "\n",
    "energy_target = targets_unpacked[\"energy\"][msk_true_particles].numpy()\n",
    "energy_pred = preds_unpacked[\"energy\"][msk_true_particles].numpy()\n",
    "\n",
    "px = preds_unpacked[\"pt\"] * preds_unpacked[\"cos_phi\"] * msk_true_particles\n",
    "py = preds_unpacked[\"pt\"] * preds_unpacked[\"sin_phi\"] * msk_true_particles\n",
    "pred_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)\n",
    "\n",
    "px = targets_unpacked[\"pt\"] * targets_unpacked[\"cos_phi\"] * msk_true_particles\n",
    "py = targets_unpacked[\"pt\"] * targets_unpacked[\"sin_phi\"] * msk_true_particles\n",
    "true_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e0ea333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 277])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msk_pred_particles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de33fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "px = targets_unpacked[\"pt\"] * targets_unpacked[\"cos_phi\"] * msk_true_particles\n",
    "py = targets_unpacked[\"pt\"] * targets_unpacked[\"sin_phi\"] * msk_true_particles\n",
    "pz = targets_unpacked[\"pt\"] * np.sinh(targets_unpacked[\"eta\"]) * msk_true_particles\n",
    "# phi = np.arctan2(targets_unpacked[\"sin_phi\"], targets_unpacked[\"cos_phi\"]) * msk_true_particles\n",
    "\n",
    "px_np = px.detach().cpu().numpy()\n",
    "py_np = py.detach().cpu().numpy()\n",
    "pz_np = pz.detach().cpu().numpy()\n",
    "# phi_np = phi.detach().cpu().numpy()\n",
    "\n",
    "# print(\"px_np\", px_np)\n",
    "# print(\"py_np\", py_np)\n",
    "# print(\"pz_np\", pz_np)\n",
    "# print(\"phi_np\", phi_np)\n",
    "\n",
    "true_mom = np.sqrt(np.sum(px_np, axis=1)**2 + np.sum(py_np, axis=1)**2 + np.sum(pz_np, axis=1)**2)\n",
    "\n",
    "E = np.sqrt(px_np**2 + py_np**2 + pz_np**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b440f931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# four-vectors\n",
    "# px_py_pz_E = np.column_stack((px_np, py_np, pz_np, E)) \n",
    "print(\"E Shape\", E.shape)\n",
    "print(\"px Shape\", px.shape)\n",
    "print(\"py Shape\", py.shape)\n",
    "print(\"pz Shape\", pz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e14d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastjet as fj\n",
    "import numpy as np\n",
    "import awkward\n",
    "\n",
    "# Four momentum \n",
    "px_np = targets_unpacked[\"pt\"] * targets_unpacked[\"cos_phi\"] * msk_true_particles\n",
    "py_np = targets_unpacked[\"pt\"] * targets_unpacked[\"sin_phi\"] * msk_true_particles\n",
    "pz_np = targets_unpacked[\"pt\"] * np.sinh(targets_unpacked[\"eta\"]) * msk_true_particles\n",
    "E_np = np.sqrt(px_np**2 + py_np**2 + pz_np**2)\n",
    "\n",
    "\n",
    "particles = []\n",
    "for ip in range(E.shape[0]):\n",
    "    for ix in range(E.shape[1]):\n",
    "        px_value = float(px[ip, ix])\n",
    "        py_value = float(py[ip, ix])\n",
    "        pz_value = float(pz[ip, ix])\n",
    "        E_value = float(E[ip, ix])\n",
    "        particle = fj.PseudoJet(px_value, py_value, pz_value, E_value)\n",
    "        particles.append(particle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976fcfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(particles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65135113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# num_threads = 20\n",
    "\n",
    "# def cluster_jets(particles):\n",
    "#     jetdef = fj.JetDefinition(fj.antikt_algorithm, 0.08)\n",
    "#     jet_ptcut = 20\n",
    "    \n",
    "#     cluster = fj.ClusterSequence(particles, jetdef)\n",
    "#     jets = cluster.inclusive_jets()\n",
    "    \n",
    "#     return jets\n",
    "\n",
    "# chunks = [particles[i::num_threads] for i in range(num_threads)]\n",
    "\n",
    "# with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "#     futures = [executor.submit(cluster_jets, chunk) for chunk in chunks]\n",
    "\n",
    "# gen_jets = []\n",
    "# for future in futures:\n",
    "#     gen_jets.extend(future.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec34463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_threads = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd02dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mydata = list(range(1000))\n",
    "# [mydata[i::num_threads] for i in range(num_threads)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6b592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_jets(particles):\n",
    "    jetdef = fj.JetDefinition(fj.antikt_algorithm, 0.08)\n",
    "#     jet_ptcut = 20\n",
    "    \n",
    "    # first 1000 events\n",
    "    particles_1000 = particles[:3000]\n",
    "    \n",
    "    cluster = fj.ClusterSequence(particles_1000, jetdef)\n",
    "    jets = cluster.inclusive_jets()\n",
    "    \n",
    "    return jets\n",
    "\n",
    "\n",
    "gen_jets = cluster_jets(particles[:3000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74b3453",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, jet in enumerate(gen_jets):\n",
    "    print(\"Jet\", i+1, \":\", jet.pt(), jet.eta(), jet.phi(), jet.e())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074daf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Jets:\", len(gen_jets))\n",
    "\n",
    "\n",
    "gen_jet_pt = [jet.pt() for jet in gen_jets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c5d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reconstructed particles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da51f2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_px = preds_unpacked[\"pt\"] * preds_unpacked[\"cos_phi\"] * msk_pred_particles\n",
    "reco_py = preds_unpacked[\"pt\"] * preds_unpacked[\"sin_phi\"] * msk_pred_particles\n",
    "reco_pz = preds_unpacked[\"pt\"] * np.sinh(preds_unpacked[\"eta\"]) * msk_pred_particles\n",
    "reco_phi = np.arctan2(preds_unpacked[\"sin_phi\"], preds_unpacked[\"cos_phi\"]) * msk_pred_particles\n",
    "\n",
    "reco_px_np = reco_px.detach().cpu().numpy()\n",
    "reco_py_np = reco_py.detach().cpu().numpy()\n",
    "reco_pz_np = reco_pz.detach().cpu().numpy()\n",
    "reco_phi_np = reco_phi.detach().cpu().numpy()\n",
    "\n",
    "# print(\"reco_px_np\", reco_px_np)\n",
    "# print(\"reco_py_np\", reco_py_np)\n",
    "# print(\"reco_pz_np\", reco_pz_np)\n",
    "# print(\"reco_phi_np\", reco_phi_np)\n",
    "\n",
    "reco_pred_mom = np.sqrt(np.sum(reco_px_np, axis=1)**2 + np.sum(reco_py_np, axis=1)**2 + np.sum(reco_pz_np, axis=1)**2)\n",
    "\n",
    "reco_E_np = np.sqrt(reco_px_np**2 + reco_py_np**2 + reco_pz_np**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad408cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# four-vectors\n",
    "# px_py_pz_E = np.column_stack((px_np, py_np, pz_np, E)) \n",
    "print(\"E Shape\", E_np.shape)\n",
    "print(\"px Shape\", reco_px_np.shape)\n",
    "print(\"py Shape\", reco_py_np.shape)\n",
    "print(\"pz Shape\", reco_pz_np.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dc7d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reco_particles = []   # TODO\n",
    "for ip in range(E_np.shape[0]):\n",
    "    for ix in range(E_np.shape[1]):\n",
    "        px_value = float(reco_px_np[ip, ix])\n",
    "        py_value = float(reco_py_np[ip, ix])\n",
    "        pz_value = float(reco_pz_np[ip, ix])\n",
    "        E_value = float(E_np[ip, ix])\n",
    "        particle = fj.PseudoJet(px_value, py_value, pz_value, E_value)\n",
    "        reco_particles.append(particle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e800914",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# num_threads = 20\n",
    "\n",
    "# # Function to perform jet clustering\n",
    "# def cluster_jets(particles):\n",
    "#     jetdef = fj.JetDefinition(fj.antikt_algorithm, 0.08)\n",
    "# #     jet_ptcut = 20\n",
    "    \n",
    "#     cluster = fj.ClusterSequence(reco_particles, jetdef)\n",
    "#     jets = cluster.inclusive_jets()\n",
    "    \n",
    "#     return jets\n",
    "\n",
    "# # Spliting particles list into chunks to process in parallel\n",
    "# chunks = [particles[i::num_threads] for i in range(num_threads)]\n",
    "\n",
    "# # jet clustering in parallel\n",
    "# with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "#     futures = [executor.submit(cluster_jets, chunk) for chunk in chunks]\n",
    "\n",
    "# reco_jets = []\n",
    "# for future in futures:\n",
    "#     reco_jets.extend(future.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc9e130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_jets(reco_particles):\n",
    "    jetdef = fj.JetDefinition(fj.antikt_algorithm, 0.08)\n",
    "#     jet_ptcut = 20\n",
    "    \n",
    "    # Select only the first 1000 events from particles\n",
    "    particles_1000 = reco_particles[:3000]\n",
    "    \n",
    "    cluster = fj.ClusterSequence(particles_1000, jetdef)\n",
    "    jets = cluster.inclusive_jets()\n",
    "    \n",
    "    return jets\n",
    "\n",
    "\n",
    "reco_jets = cluster_jets(reco_particles[:3000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f2ecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, jet in enumerate(reco_jets):\n",
    "    print(\"Jet\", i+1, \":\", jet.pt(), jet.eta(), jet.phi(), jet.e())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429abf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Jets:\", len(reco_jets))\n",
    "\n",
    "\n",
    "reco_jet_pt = [jet.pt() for jet in reco_jets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8d3e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jets matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2de76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define deltaR function\n",
    "def deltaR(eta1, phi1, eta2, phi2):\n",
    "    deta = eta1 - eta2\n",
    "    dphi = np.abs(phi1 - phi2)\n",
    "    if dphi > np.pi:\n",
    "        dphi -= 2 * np.pi\n",
    "    return np.sqrt(deta**2 + dphi**2)\n",
    "\n",
    "# Define matching criteria\n",
    "deltaR_cut = 0.2 # Adjust as needed\n",
    "\n",
    "# Define bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100]\n",
    "\n",
    "# Iterate over bins\n",
    "for i in range(len(bins) - 1):\n",
    "    lim_low = bins[i]\n",
    "    lim_hi = bins[i + 1]\n",
    "\n",
    "    # Apply selection to gen_jet indices within the bin\n",
    "    selected_indices_gen_jet = [idx for idx, jet in enumerate(gen_jets) if lim_low < jet.pt() <= lim_hi]\n",
    "\n",
    "    # Match selected gen_jet indices to reco_jet values\n",
    "    matched_reco_jet_values = []\n",
    "    for idx in selected_indices_gen_jet:\n",
    "        gen_eta = gen_jets[idx].eta()\n",
    "        gen_phi = gen_jets[idx].phi()\n",
    "        matched_jet = None\n",
    "        min_deltaR = float('inf')\n",
    "        for reco_jet in reco_jets:\n",
    "            reco_eta = reco_jet.eta()\n",
    "            reco_phi = reco_jet.phi()\n",
    "            dR = deltaR(gen_eta, gen_phi, reco_eta, reco_phi)\n",
    "            if dR < deltaR_cut and dR < min_deltaR:\n",
    "                matched_jet = reco_jet.pt()\n",
    "                min_deltaR = dR\n",
    "        if matched_jet is not None:\n",
    "            matched_reco_jet_values.append(matched_jet)\n",
    "\n",
    "    # Calculate the number of events in the bin\n",
    "    num_events_gen_jet = len(selected_indices_gen_jet)\n",
    "    num_events_reco_jet = len(matched_reco_jet_values)\n",
    "\n",
    "    print(f\"Bin {i+1}: Number of events (Gen Jet): {num_events_gen_jet}, Number of events (Reco Jet): {num_events_reco_jet}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bca7eb9",
   "metadata": {},
   "source": [
    "## Jets Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bde406c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastjet as fj\n",
    "import numpy as np\n",
    "import awkward\n",
    "import vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f2bba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deltaR \n",
    "def deltaR(eta1, phi1, eta2, phi2):\n",
    "    deta = eta1 - eta2\n",
    "    dphi = np.abs(phi1 - phi2)\n",
    "    if dphi > np.pi:\n",
    "        dphi -= 2 * np.pi\n",
    "    return np.sqrt(deta**2 + dphi**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "caed7806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching jets\n",
    "def match_jets(jets1, jets2, deltaR_cut):\n",
    "    iev = len(jets1)\n",
    "    jet_inds_1_ev = []\n",
    "    jet_inds_2_ev = []\n",
    "    for ev in range(iev):\n",
    "        j1 = jets1[ev]\n",
    "        j2 = jets2[ev]\n",
    "\n",
    "        jet_inds_1 = []\n",
    "        jet_inds_2 = []\n",
    "        for ij1 in range(len(j1)):\n",
    "            drs = np.zeros(len(j2), dtype=np.float64)\n",
    "            for ij2 in range(len(j2)):\n",
    "                eta1 = j1[ij1].eta\n",
    "                eta2 = j2[ij2].eta\n",
    "                phi1 = j1[ij1].phi\n",
    "                phi2 = j2[ij2].phi\n",
    "\n",
    "                # Workaround for https://github.com/scikit-hep/vector/issues/303\n",
    "                # dr = j1[ij1].deltaR(j2[ij2])\n",
    "                dr = deltaR(eta1, phi1, eta2, phi2)\n",
    "                drs[ij2] = dr\n",
    "            if len(drs) > 0:\n",
    "                min_idx_dr = np.argmin(drs)\n",
    "                if drs[min_idx_dr] < deltaR_cut:\n",
    "                    jet_inds_1.append(ij1)\n",
    "                    jet_inds_2.append(min_idx_dr)\n",
    "        jet_inds_1_ev.append(jet_inds_1)\n",
    "        jet_inds_2_ev.append(jet_inds_2)\n",
    "    return jet_inds_1_ev, jet_inds_2_ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77670c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match two jet collections \n",
    "def match_two_jet_collections(jets_coll, name1, name2, jet_match_dr):\n",
    "    num_events = len(jets_coll[name1])\n",
    "    vec1 = vector.awk(\n",
    "        awkward.zip(\n",
    "            {\n",
    "                \"pt\": jets_coll[name1].pt,\n",
    "                \"eta\": jets_coll[name1].eta,\n",
    "                \"phi\": jets_coll[name1].phi,\n",
    "                \"energy\": jets_coll[name1].energy,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    vec2 = vector.awk(\n",
    "        awkward.zip(\n",
    "            {\n",
    "                \"pt\": jets_coll[name2].pt,\n",
    "                \"eta\": jets_coll[name2].eta,\n",
    "                \"phi\": jets_coll[name2].phi,\n",
    "                \"energy\": jets_coll[name2].energy,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    ret = match_jets(vec1, vec2, jet_match_dr)\n",
    "    j1_idx = awkward.from_iter(ret[0])\n",
    "    j2_idx = awkward.from_iter(ret[1])\n",
    "\n",
    "    num_jets = len(awkward.flatten(j1_idx))\n",
    "\n",
    "    # In case there are no jets matched, create dummy array to ensure correct types\n",
    "    if num_jets > 0:\n",
    "        c1_to_c2 = awkward.Array({name1: j1_idx, name2: j2_idx})\n",
    "    else:\n",
    "        dummy = build_dummy_array(num_events)\n",
    "        c1_to_c2 = awkward.Array({name1: dummy, name2: dummy})\n",
    "\n",
    "    return c1_to_c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e6600f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dummy_array(num_events):\n",
    "    return awkward.Array([[] for _ in range(num_events)])\n",
    "\n",
    "# Four momentum calculation for true particles\n",
    "px = targets_unpacked[\"pt\"] * targets_unpacked[\"cos_phi\"] * msk_true_particles\n",
    "py = targets_unpacked[\"pt\"] * targets_unpacked[\"sin_phi\"] * msk_true_particles\n",
    "pz = targets_unpacked[\"pt\"] * np.sinh(targets_unpacked[\"eta\"]) * msk_true_particles\n",
    "E = np.sqrt(px**2 + py**2 + pz**2)\n",
    "\n",
    "# Convert to numpy\n",
    "px_np = px.detach().cpu().numpy()\n",
    "py_np = py.detach().cpu().numpy()\n",
    "pz_np = pz.detach().cpu().numpy()\n",
    "E_np = E.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbb0524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create particles list\n",
    "particles = []\n",
    "for ip in range(E_np.shape[0]):\n",
    "    event_particles = []\n",
    "    for ix in range(E_np.shape[1]):\n",
    "        px_value = float(px_np[ip, ix])\n",
    "        py_value = float(py_np[ip, ix])\n",
    "        pz_value = float(pz_np[ip, ix])\n",
    "        E_value = float(E_np[ip, ix])\n",
    "        particle = fj.PseudoJet(px_value, py_value, pz_value, E_value)\n",
    "        event_particles.append(particle)\n",
    "    particles.append(event_particles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55d2d127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering function\n",
    "def cluster_jets(event_particles):\n",
    "    jetdef = fj.JetDefinition(fj.antikt_algorithm, 0.08)\n",
    "    cluster = fj.ClusterSequence(event_particles, jetdef)\n",
    "    jets = cluster.inclusive_jets()\n",
    "    return jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0408032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--------------------------------------------------------------------------\n",
      "#                         FastJet release 3.4.1\n",
      "#                 M. Cacciari, G.P. Salam and G. Soyez                  \n",
      "#     A software package for jet finding and analysis at colliders      \n",
      "#                           http://fastjet.fr                           \n",
      "#\t                                                                      \n",
      "# Please cite EPJC72(2012)1896 [arXiv:1111.6097] if you use this package\n",
      "# for scientific work and optionally PLB641(2006)57 [hep-ph/0512210].   \n",
      "#                                                                       \n",
      "# FastJet is provided without warranty under the GNU GPL v2 or higher.  \n",
      "# It uses T. Chan's closest pair algorithm, S. Fortune's Voronoi code,\n",
      "# CGAL and 3rd party plugin jet algorithms. See COPYING file for details.\n",
      "#--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cluster jets for all events\n",
    "gen_jets = [cluster_jets(event) for event in particles]\n",
    "\n",
    "# Four momentum calculation for predicted particles\n",
    "reco_px = preds_unpacked[\"pt\"] * preds_unpacked[\"cos_phi\"] * msk_pred_particles\n",
    "reco_py = preds_unpacked[\"pt\"] * preds_unpacked[\"sin_phi\"] * msk_pred_particles\n",
    "reco_pz = preds_unpacked[\"pt\"] * np.sinh(preds_unpacked[\"eta\"]) * msk_pred_particles\n",
    "reco_E = np.sqrt(reco_px**2 + reco_py**2 + reco_pz**2)\n",
    "\n",
    "# Convert to numpy\n",
    "reco_px_np = reco_px.detach().cpu().numpy()\n",
    "reco_py_np = reco_py.detach().cpu().numpy()\n",
    "reco_pz_np = reco_pz.detach().cpu().numpy()\n",
    "reco_E_np = reco_E.detach().cpu().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ff0dade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reco_particles list\n",
    "reco_particles = []\n",
    "for ip in range(reco_E_np.shape[0]):\n",
    "    event_particles = []\n",
    "    for ix in range(reco_E_np.shape[1]):\n",
    "        px_value = float(reco_px_np[ip, ix])\n",
    "        py_value = float(reco_py_np[ip, ix])\n",
    "        pz_value = float(reco_pz_np[ip, ix])\n",
    "        E_value = float(reco_E_np[ip, ix])\n",
    "        particle = fj.PseudoJet(px_value, py_value, pz_value, E_value)\n",
    "        event_particles.append(particle)\n",
    "    reco_particles.append(event_particles)\n",
    "\n",
    "# Cluster jets for all reco events\n",
    "reco_jets = [cluster_jets(event) for event in reco_particles]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e26ca31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create jet collections\n",
    "gen_jets_coll = {\n",
    "    \"pt\": awkward.from_iter([[jet.pt() for jet in event] for event in gen_jets]),\n",
    "    \"eta\": awkward.from_iter([[jet.eta() for jet in event] for event in gen_jets]),\n",
    "    \"phi\": awkward.from_iter([[jet.phi() for jet in event] for event in gen_jets]),\n",
    "    \"energy\": awkward.from_iter([[jet.e() for jet in event] for event in gen_jets])\n",
    "}\n",
    "\n",
    "reco_jets_coll = {\n",
    "    \"pt\": awkward.from_iter([[jet.pt() for jet in event] for event in reco_jets]),\n",
    "    \"eta\": awkward.from_iter([[jet.eta() for jet in event] for event in reco_jets]),\n",
    "    \"phi\": awkward.from_iter([[jet.phi() for jet in event] for event in reco_jets]),\n",
    "    \"energy\": awkward.from_iter([[jet.e() for jet in event] for event in reco_jets])\n",
    "}\n",
    "\n",
    "# Combine jet collections\n",
    "jets_coll = {\n",
    "    \"gen\": gen_jets_coll,\n",
    "    \"reco\": reco_jets_coll\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2dd1f2e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/sraj/ipykernel_1061930/874354358.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Match the jets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mjet_match_dr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmatched_jets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch_two_jet_collections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjets_coll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gen\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"reco\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjet_match_dr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/sraj/ipykernel_1061930/3431725518.py\u001b[0m in \u001b[0;36mmatch_two_jet_collections\u001b[0;34m(jets_coll, name1, name2, jet_match_dr)\u001b[0m\n\u001b[1;32m      5\u001b[0m         awkward.zip(\n\u001b[1;32m      6\u001b[0m             {\n\u001b[0;32m----> 7\u001b[0;31m                 \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjets_coll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                 \u001b[0;34m\"eta\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjets_coll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0;34m\"phi\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjets_coll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'pt'"
     ]
    }
   ],
   "source": [
    "# Match the jets\n",
    "jet_match_dr = 0.2\n",
    "matched_jets = match_two_jet_collections(jets_coll, \"gen\", \"reco\", jet_match_dr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db75da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85832ef0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/sraj/ipykernel_1061930/3701209707.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;31m# Match the jets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0mjet_match_dr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m \u001b[0mmatched_jets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch_two_jet_collections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjets_coll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gen\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"reco\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjet_match_dr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;31m# Now you can use the matched_jets for further analysis or plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/sraj/ipykernel_1061930/3701209707.py\u001b[0m in \u001b[0;36mmatch_two_jet_collections\u001b[0;34m(jets_coll, name1, name2, jet_match_dr)\u001b[0m\n\u001b[1;32m     67\u001b[0m         )\n\u001b[1;32m     68\u001b[0m     )\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch_jets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjet_match_dr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mj1_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mj2_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/sraj/ipykernel_1061930/3701209707.py\u001b[0m in \u001b[0;36mmatch_jets\u001b[0;34m(jets1, jets2, deltaR_cut)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0meta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mij2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mphi1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mij1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mphi2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mij2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;31m# Workaround for https://github.com/scikit-hep/vector/issues/303\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/vector/_backends/awkward_.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, where)\u001b[0m\n\u001b[1;32m    348\u001b[0m     ) -> typing.Optional[typing.Union[float, ak.Array, ak.Record]]:\n\u001b[1;32m    349\u001b[0m         \u001b[0;31m# \"__getitem__\" undefined in superclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     def _wrap_result(\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/awkward/highlevel.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, where)\u001b[0m\n\u001b[1;32m    989\u001b[0m         \"\"\"\n\u001b[1;32m    990\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_tracers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_behavior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jaxtracers_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/awkward/_util.py\u001b[0m in \u001b[0;36mwrap\u001b[0;34m(content, behavior)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhighlevel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbehavior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbehavior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/awkward/highlevel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, behavior, with_name, check_valid, cache, kernels)\u001b[0m\n\u001b[1;32m   1581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1582\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1583\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehavior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbehavior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1584\u001b[0m         \u001b[0mdocstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpurelist_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__doc__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/awkward/highlevel.py\u001b[0m in \u001b[0;36mbehavior\u001b[0;34m(self, behavior)\u001b[0m\n\u001b[1;32m   1663\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbehavior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbehavior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbehavior\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbehavior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1665\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecordclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbehavior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1666\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_behavior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbehavior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/awkward/_util.py\u001b[0m in \u001b[0;36mrecordclass\u001b[0;34m(layout, behavior)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrecordclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbehavior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0mlayout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0mbehavior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBehavior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbehavior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbehavior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__record__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/awkward/partition.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPartitionedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import fastjet as fj\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import vector\n",
    "\n",
    "# Define deltaR function\n",
    "def deltaR(eta1, phi1, eta2, phi2):\n",
    "    deta = eta1 - eta2\n",
    "    dphi = np.abs(phi1 - phi2)\n",
    "    if dphi > np.pi:\n",
    "        dphi -= 2 * np.pi\n",
    "    return np.sqrt(deta**2 + dphi**2)\n",
    "\n",
    "# Matching jets function\n",
    "def match_jets(jets1, jets2, deltaR_cut):\n",
    "    iev = len(jets1)\n",
    "    jet_inds_1_ev = []\n",
    "    jet_inds_2_ev = []\n",
    "    for ev in range(iev):\n",
    "        j1 = jets1[ev]\n",
    "        j2 = jets2[ev]\n",
    "\n",
    "        jet_inds_1 = []\n",
    "        jet_inds_2 = []\n",
    "        for ij1 in range(len(j1)):\n",
    "            drs = np.zeros(len(j2), dtype=np.float64)\n",
    "            for ij2 in range(len(j2)):\n",
    "                eta1 = j1[ij1].eta\n",
    "                eta2 = j2[ij2].eta\n",
    "                phi1 = j1[ij1].phi\n",
    "                phi2 = j2[ij2].phi\n",
    "\n",
    "                # Workaround for https://github.com/scikit-hep/vector/issues/303\n",
    "                # dr = j1[ij1].deltaR(j2[ij2])\n",
    "                dr = deltaR(eta1, phi1, eta2, phi2)\n",
    "                drs[ij2] = dr\n",
    "            if len(drs) > 0:\n",
    "                min_idx_dr = np.argmin(drs)\n",
    "                if drs[min_idx_dr] < deltaR_cut:\n",
    "                    jet_inds_1.append(ij1)\n",
    "                    jet_inds_2.append(min_idx_dr)\n",
    "        jet_inds_1_ev.append(jet_inds_1)\n",
    "        jet_inds_2_ev.append(jet_inds_2)\n",
    "    return jet_inds_1_ev, jet_inds_2_ev\n",
    "\n",
    "# Match two jet collections function\n",
    "def match_two_jet_collections(jets_coll, name1, name2, jet_match_dr):\n",
    "    num_events = len(jets_coll[name1][\"pt\"])\n",
    "    vec1 = vector.awk(\n",
    "        ak.zip(\n",
    "            {\n",
    "                \"pt\": jets_coll[name1][\"pt\"],\n",
    "                \"eta\": jets_coll[name1][\"eta\"],\n",
    "                \"phi\": jets_coll[name1][\"phi\"],\n",
    "                \"energy\": jets_coll[name1][\"energy\"],\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    vec2 = vector.awk(\n",
    "        ak.zip(\n",
    "            {\n",
    "                \"pt\": jets_coll[name2][\"pt\"],\n",
    "                \"eta\": jets_coll[name2][\"eta\"],\n",
    "                \"phi\": jets_coll[name2][\"phi\"],\n",
    "                \"energy\": jets_coll[name2][\"energy\"],\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    ret = match_jets(vec1, vec2, jet_match_dr)\n",
    "    j1_idx = ak.from_iter(ret[0])\n",
    "    j2_idx = ak.from_iter(ret[1])\n",
    "\n",
    "    num_jets = len(ak.flatten(j1_idx))\n",
    "\n",
    "    # In case there are no jets matched, create dummy array to ensure correct types\n",
    "    if num_jets > 0:\n",
    "        c1_to_c2 = ak.Array({name1: j1_idx, name2: j2_idx})\n",
    "    else:\n",
    "        dummy = build_dummy_array(num_events)\n",
    "        c1_to_c2 = ak.Array({name1: dummy, name2: dummy})\n",
    "\n",
    "    return c1_to_c2\n",
    "\n",
    "# Function to build a dummy array\n",
    "def build_dummy_array(num_events):\n",
    "    return ak.Array([[] for _ in range(num_events)])\n",
    "\n",
    "# Four momentum calculation for true particles\n",
    "px = targets_unpacked[\"pt\"] * targets_unpacked[\"cos_phi\"] * msk_true_particles\n",
    "py = targets_unpacked[\"pt\"] * targets_unpacked[\"sin_phi\"] * msk_true_particles\n",
    "pz = targets_unpacked[\"pt\"] * np.sinh(targets_unpacked[\"eta\"]) * msk_true_particles\n",
    "E = np.sqrt(px**2 + py**2 + pz**2)\n",
    "\n",
    "# Convert to numpy\n",
    "px_np = px.detach().cpu().numpy()\n",
    "py_np = py.detach().cpu().numpy()\n",
    "pz_np = pz.detach().cpu().numpy()\n",
    "E_np = E.detach().cpu().numpy()\n",
    "\n",
    "# Create particles list\n",
    "particles = []\n",
    "for ip in range(E_np.shape[0]):\n",
    "    event_particles = []\n",
    "    for ix in range(E_np.shape[1]):\n",
    "        px_value = float(px_np[ip, ix])\n",
    "        py_value = float(py_np[ip, ix])\n",
    "        pz_value = float(pz_np[ip, ix])\n",
    "        E_value = float(E_np[ip, ix])\n",
    "        particle = fj.PseudoJet(px_value, py_value, pz_value, E_value)\n",
    "        event_particles.append(particle)\n",
    "    particles.append(event_particles)\n",
    "\n",
    "# Clustering function\n",
    "def cluster_jets(event_particles):\n",
    "    jetdef = fj.JetDefinition(fj.antikt_algorithm, 0.08)\n",
    "    cluster = fj.ClusterSequence(event_particles, jetdef)\n",
    "    jets = cluster.inclusive_jets()\n",
    "    return jets\n",
    "\n",
    "# Cluster jets for all events\n",
    "gen_jets = [cluster_jets(event) for event in particles]\n",
    "\n",
    "# Four momentum calculation for predicted particles\n",
    "reco_px = preds_unpacked[\"pt\"] * preds_unpacked[\"cos_phi\"] * msk_pred_particles\n",
    "reco_py = preds_unpacked[\"pt\"] * preds_unpacked[\"sin_phi\"] * msk_pred_particles\n",
    "reco_pz = preds_unpacked[\"pt\"] * np.sinh(preds_unpacked[\"eta\"]) * msk_pred_particles\n",
    "reco_E = np.sqrt(reco_px**2 + reco_py**2 + reco_pz**2)\n",
    "\n",
    "# Convert to numpy\n",
    "reco_px_np = reco_px.detach().cpu().numpy()\n",
    "reco_py_np = reco_py.detach().cpu().numpy()\n",
    "reco_pz_np = reco_pz.detach().cpu().numpy()\n",
    "reco_E_np = reco_E.detach().cpu().numpy()\n",
    "\n",
    "# Create reco_particles list\n",
    "reco_particles = []\n",
    "for ip in range(reco_E_np.shape[0]):\n",
    "    event_particles = []\n",
    "    for ix in range(reco_E_np.shape[1]):\n",
    "        px_value = float(reco_px_np[ip, ix])\n",
    "        py_value = float(reco_py_np[ip, ix])\n",
    "        pz_value = float(reco_pz_np[ip, ix])\n",
    "        E_value = float(reco_E_np[ip, ix])\n",
    "        particle = fj.PseudoJet(px_value, py_value, pz_value, E_value)\n",
    "        event_particles.append(particle)\n",
    "    reco_particles.append(event_particles)\n",
    "\n",
    "# Cluster jets for all reco events\n",
    "reco_jets = [cluster_jets(event) for event in reco_particles]\n",
    "\n",
    "# Create jet collections\n",
    "gen_jets_coll = {\n",
    "    \"pt\": ak.from_iter([[jet.pt() for jet in event] for event in gen_jets]),\n",
    "    \"eta\": ak.from_iter([[jet.eta() for jet in event] for event in gen_jets]),\n",
    "    \"phi\": ak.from_iter([[jet.phi() for jet in event] for event in gen_jets]),\n",
    "    \"energy\": ak.from_iter([[jet.e() for jet in event] for event in gen_jets])\n",
    "}\n",
    "\n",
    "reco_jets_coll = {\n",
    "    \"pt\": ak.from_iter([[jet.pt() for jet in event] for event in reco_jets]),\n",
    "    \"eta\": ak.from_iter([[jet.eta() for jet in event] for event in reco_jets]),\n",
    "    \"phi\": ak.from_iter([[jet.phi() for jet in event] for event in reco_jets]),\n",
    "    \"energy\": ak.from_iter([[jet.e() for jet in event] for event in reco_jets])\n",
    "}\n",
    "\n",
    "# Combine jet collections\n",
    "jets_coll = {\n",
    "    \"gen\": gen_jets_coll,\n",
    "    \"reco\": reco_jets_coll\n",
    "}\n",
    "\n",
    "# Match the jets\n",
    "jet_match_dr = 0.2\n",
    "matched_jets = match_two_jet_collections(jets_coll, \"gen\", \"reco\", jet_match_dr)\n",
    "\n",
    "# Now you can use the matched_jets for further analysis or plotting\n",
    "print(matched_jets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9c3f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779c288c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d12f6660",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Wrong number or type of arguments for overloaded function 'new_PseudoJet'.\n  Possible C/C++ prototypes are:\n    fastjet::PseudoJet::PseudoJet()\n    fastjet::PseudoJet::PseudoJet(double const,double const,double const,double const)\n    fastjet::PseudoJet::PseudoJet(bool)\n    fastjet::PseudoJet::PseudoJet(fastjet::PseudoJet const &)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/sraj/ipykernel_1061930/1761227853.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# Match the jets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m matched_jets = match_two_jet_collections(\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m\"gen\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgen_jets_coll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"reco\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreco_jets_coll\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;34m\"gen\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/sraj/ipykernel_1061930/1761227853.py\u001b[0m in \u001b[0;36mmatch_two_jet_collections\u001b[0;34m(jets_coll, name1, name2, jet_match_dr)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmatch_two_jet_collections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjets_coll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjet_match_dr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mnum_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjets_coll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     vec1 = [[fj.PseudoJet(pt, eta, phi, energy) for pt, eta, phi, energy in zip(\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mjets_coll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mjets_coll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eta\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/sraj/ipykernel_1061930/1761227853.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmatch_two_jet_collections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjets_coll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjet_match_dr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mnum_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjets_coll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     vec1 = [[fj.PseudoJet(pt, eta, phi, energy) for pt, eta, phi, energy in zip(\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mjets_coll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mjets_coll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eta\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/sraj/ipykernel_1061930/1761227853.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmatch_two_jet_collections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjets_coll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjet_match_dr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mnum_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjets_coll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     vec1 = [[fj.PseudoJet(pt, eta, phi, energy) for pt, eta, phi, energy in zip(\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mjets_coll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mjets_coll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eta\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/fastjet/_swig.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         \"\"\"\n\u001b[0;32m--> 927\u001b[0;31m         \u001b[0m_fastjet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPseudoJet_swiginit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fastjet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_PseudoJet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m     \u001b[0m__swig_destroy__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fastjet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_PseudoJet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Wrong number or type of arguments for overloaded function 'new_PseudoJet'.\n  Possible C/C++ prototypes are:\n    fastjet::PseudoJet::PseudoJet()\n    fastjet::PseudoJet::PseudoJet(double const,double const,double const,double const)\n    fastjet::PseudoJet::PseudoJet(bool)\n    fastjet::PseudoJet::PseudoJet(fastjet::PseudoJet const &)\n"
     ]
    }
   ],
   "source": [
    "import fastjet as fj\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "\n",
    "# Define deltaR function\n",
    "def deltaR(eta1, phi1, eta2, phi2):\n",
    "    deta = eta1 - eta2\n",
    "    dphi = np.abs(phi1 - phi2)\n",
    "    if dphi > np.pi:\n",
    "        dphi -= 2 * np.pi\n",
    "    return np.sqrt(deta**2 + dphi**2)\n",
    "\n",
    "# Matching jets function\n",
    "def match_jets(jets1, jets2, deltaR_cut):\n",
    "    iev = len(jets1)\n",
    "    jet_inds_1_ev = []\n",
    "    jet_inds_2_ev = []\n",
    "    for ev in range(iev):\n",
    "        j1 = jets1[ev]\n",
    "        j2 = jets2[ev]\n",
    "\n",
    "        jet_inds_1 = []\n",
    "        jet_inds_2 = []\n",
    "        for ij1 in range(len(j1)):\n",
    "            drs = np.zeros(len(j2), dtype=np.float64)\n",
    "            for ij2 in range(len(j2)):\n",
    "                eta1 = j1[ij1].eta\n",
    "                eta2 = j2[ij2].eta\n",
    "                phi1 = j1[ij1].phi\n",
    "                phi2 = j2[ij2].phi\n",
    "\n",
    "                dr = deltaR(eta1, phi1, eta2, phi2)\n",
    "                drs[ij2] = dr\n",
    "            if len(drs) > 0:\n",
    "                min_idx_dr = np.argmin(drs)\n",
    "                if drs[min_idx_dr] < deltaR_cut:\n",
    "                    jet_inds_1.append(ij1)\n",
    "                    jet_inds_2.append(min_idx_dr)\n",
    "        jet_inds_1_ev.append(jet_inds_1)\n",
    "        jet_inds_2_ev.append(jet_inds_2)\n",
    "    return jet_inds_1_ev, jet_inds_2_ev\n",
    "\n",
    "def match_two_jet_collections(jets_coll, name1, name2, jet_match_dr):\n",
    "    num_events = len(jets_coll[name1][\"pt\"])\n",
    "    vec1 = [[fj.PseudoJet(pt, eta, phi, energy) for pt, eta, phi, energy in zip(\n",
    "                jets_coll[name1][\"pt\"][i],\n",
    "                jets_coll[name1][\"eta\"][i],\n",
    "                jets_coll[name1][\"phi\"][i],\n",
    "                np.sqrt(jets_coll[name1][\"pt\"][i]**2 + np.sinh(jets_coll[name1][\"eta\"][i])**2))]\n",
    "            for i in range(num_events)]\n",
    "    vec2 = [[fj.PseudoJet(pt, eta, phi, energy) for pt, eta, phi, energy in zip(\n",
    "                jets_coll[name2][\"pt\"][i],\n",
    "                jets_coll[name2][\"eta\"][i],\n",
    "                jets_coll[name2][\"phi\"][i],\n",
    "                np.sqrt(jets_coll[name2][\"pt\"][i]**2 + np.sinh(jets_coll[name2][\"eta\"][i])**2))]\n",
    "            for i in range(num_events)]\n",
    "    ret = match_jets(vec1, vec2, jet_match_dr)\n",
    "    j1_idx = ak.from_iter(ret[0])\n",
    "    j2_idx = ak.from_iter(ret[1])\n",
    "\n",
    "    num_jets = len(ak.flatten(j1_idx))\n",
    "\n",
    "    # In case there are no jets matched, create dummy array to ensure correct types\n",
    "    if num_jets > 0:\n",
    "        c1_to_c2 = ak.Array({name1: j1_idx, name2: j2_idx})\n",
    "    else:\n",
    "        dummy = build_dummy_array(num_events)\n",
    "        c1_to_c2 = ak.Array({name1: dummy, name2: dummy})\n",
    "\n",
    "    return c1_to_c2\n",
    "\n",
    "\n",
    "\n",
    "# Function to build a dummy array\n",
    "def build_dummy_array(num_events):\n",
    "    return ak.Array([[] for _ in range(num_events)])\n",
    "\n",
    "# Now, you need to prepare your jet collections in a suitable format\n",
    "gen_jets_coll = {\n",
    "    \"pt\": targets_unpacked[\"pt\"],\n",
    "    \"eta\": targets_unpacked[\"eta\"],\n",
    "    \"phi\": targets_unpacked[\"phi\"],\n",
    "    \"energy\": targets_unpacked[\"energy\"]\n",
    "}\n",
    "\n",
    "reco_jets_coll = {\n",
    "    \"pt\": preds_unpacked[\"pt\"],\n",
    "    \"eta\": preds_unpacked[\"eta\"],\n",
    "    \"phi\": preds_unpacked[\"phi\"],\n",
    "    \"energy\": preds_unpacked[\"energy\"]\n",
    "}\n",
    "\n",
    "# Define the deltaR cut\n",
    "jet_match_dr = 0.2\n",
    "\n",
    "# Match the jets\n",
    "matched_jets = match_two_jet_collections(\n",
    "    {\"gen\": gen_jets_coll, \"reco\": reco_jets_coll},\n",
    "    \"gen\",\n",
    "    \"reco\",\n",
    "    jet_match_dr\n",
    ")\n",
    "\n",
    "# Now you have the matched jets stored in matched_jets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf33ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbfe711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8d3365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3538df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3cd0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a6fde9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611d8a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100]\n",
    "\n",
    "# Iterate over bins\n",
    "for i in range(len(bins) - 1):\n",
    "    lim_low = bins[i]\n",
    "    lim_hi = bins[i + 1]\n",
    "\n",
    "    # Apply selection to gen_jet indices within the bin\n",
    "    selected_indices_gen_jet = [idx for idx, pt in enumerate(gen_jet_pt) if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Match selected gen_jet indices to reco_jet values\n",
    "    matched_reco_jet_values = [reco_jet_pt[idx] for idx in selected_indices_gen_jet]\n",
    "\n",
    "    # Calculate the number of events in the bin\n",
    "    num_events_gen_jet = len(selected_indices_gen_jet)\n",
    "    num_events_reco_jet = len(matched_reco_jet_values)\n",
    "\n",
    "    print(f\"Bin {i+1}: Number of events (Gen Jet): {num_events_gen_jet}, Number of events (Reco Jet): {num_events_reco_jet}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6db405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "import numpy as np\n",
    "\n",
    "# Define bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100]\n",
    "\n",
    "# Initialize lists to store values\n",
    "x_vals = []\n",
    "ratio_iqr_median = []\n",
    "\n",
    "# Iterate over bins\n",
    "for i in range(len(bins) - 1):\n",
    "    lim_low = bins[i]\n",
    "    lim_hi = bins[i + 1]\n",
    "    x_vals.append(np.mean([lim_low, lim_hi]))\n",
    "\n",
    "    # Apply selection to gen_jet indices within the bin\n",
    "    selected_indices = [idx for idx, pt in enumerate(gen_jet_pt) if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Match selected gen_jet indices to reco_jet values\n",
    "    matched_reco_jet_values = [reco_jet_pt[idx] for idx in selected_indices]\n",
    "\n",
    "    # Check if there are any values in both selected gen_jet_pt and matched reco_jet_pt\n",
    "    if len(selected_indices) == 0 or len(matched_reco_jet_values) == 0:\n",
    "        # Append NaN values\n",
    "        ratio_iqr_median.append(np.nan)\n",
    "        continue\n",
    "\n",
    "    # Calculate IQR and median for matched reco_jet_pt / gen_jet_pt values\n",
    "    ratio_values_in_bin = [reco / gen_jet_pt[idx] for reco, idx in zip(matched_reco_jet_values, selected_indices)]\n",
    "    ratio_iqr = np.percentile(ratio_values_in_bin, 75) - np.percentile(ratio_values_in_bin, 25)\n",
    "    ratio_median = np.median(ratio_values_in_bin)\n",
    "    ratio_iqr_median_ratio = ratio_iqr / ratio_median\n",
    "    ratio_iqr_median.append(ratio_iqr_median_ratio)\n",
    "\n",
    "# Filter out NaN values\n",
    "x_vals_filtered = [x for x, y in zip(x_vals, ratio_iqr_median) if not np.isnan(y)]\n",
    "ratio_iqr_median_filtered = [y for y in ratio_iqr_median if not np.isnan(y)]\n",
    "\n",
    "# Create a TGraph with filtered values\n",
    "gr_ratio = ROOT.TGraph(len(x_vals_filtered), np.array(x_vals_filtered), np.array(ratio_iqr_median_filtered))\n",
    "\n",
    "# Set the titles to empty strings\n",
    "gr_ratio.SetTitle(\"\")\n",
    "\n",
    "# Create a canvas\n",
    "canvas = ROOT.TCanvas(\"canvas\", \"Ratio IQR/Median vs Jet pT\", 800, 600)\n",
    "\n",
    "# Draw the graph with connecting lines\n",
    "gr_ratio.SetMarkerStyle(20)\n",
    "gr_ratio.SetMarkerColor(ROOT.kBlue)\n",
    "gr_ratio.SetLineColor(ROOT.kBlue)\n",
    "gr_ratio.GetXaxis().SetTitle(\"Jet PT, Gen (GeV)\")\n",
    "gr_ratio.GetYaxis().SetTitle(\"Response IQR/Median\")\n",
    "gr_ratio.Draw(\"APL\")\n",
    "\n",
    "# Get the X and Y axes\n",
    "xaxis = gr_ratio.GetXaxis()\n",
    "yaxis = gr_ratio.GetYaxis()\n",
    "\n",
    "# Set tick length for all four axes\n",
    "xaxis.SetTickLength(0.03)\n",
    "yaxis.SetTickLength(0.03)\n",
    "\n",
    "# Add legend\n",
    "legend = ROOT.TLegend(0.75, 0.75, 0.9, 0.9)\n",
    "legend.AddEntry(gr_ratio, \"Pred FP\", \"p\")\n",
    "legend.Draw()\n",
    "\n",
    "# Show the canvas\n",
    "canvas.Draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f08b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "import numpy as np\n",
    "\n",
    "# Define jet matching function\n",
    "def match_jets(gen_jet_pt, reco_jet_pt, deltaR_cut):\n",
    "    matched_indices_gen = []\n",
    "    matched_indices_reco = []\n",
    "    \n",
    "    for i, gen_pt in enumerate(gen_jet_pt):\n",
    "        min_deltaR = float('inf')\n",
    "        matched_idx = -1\n",
    "        \n",
    "        for j, reco_pt in enumerate(reco_jet_pt):\n",
    "            deltaR = np.sqrt((gen_pt - reco_pt)**2)\n",
    "            if deltaR < min_deltaR:\n",
    "                min_deltaR = deltaR\n",
    "                matched_idx = j\n",
    "        \n",
    "        if min_deltaR < deltaR_cut:\n",
    "            matched_indices_gen.append(i)\n",
    "            matched_indices_reco.append(matched_idx)\n",
    "    \n",
    "    return matched_indices_gen, matched_indices_reco\n",
    "\n",
    "# Define bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100]\n",
    "\n",
    "# Initialize lists to store values\n",
    "x_vals = []\n",
    "ratio_iqr_median = []\n",
    "\n",
    "# Iterate over bins\n",
    "for i in range(len(bins) - 1):\n",
    "    lim_low = bins[i]\n",
    "    lim_hi = bins[i + 1]\n",
    "    x_vals.append(np.mean([lim_low, lim_hi]))\n",
    "\n",
    "    # Apply selection to gen_jet indices within the bin\n",
    "    selected_indices_gen = [idx for idx, pt in enumerate(gen_jet_pt) if lim_low < pt <= lim_hi]\n",
    "    selected_gen_jet_pt = [gen_jet_pt[idx] for idx in selected_indices_gen]\n",
    "    \n",
    "    # Apply selection to reco_jet indices within the bin\n",
    "    selected_indices_reco = [idx for idx, pt in enumerate(reco_jet_pt) if lim_low < pt <= lim_hi]\n",
    "    selected_reco_jet_pt = [reco_jet_pt[idx] for idx in selected_indices_reco]\n",
    "    \n",
    "    # Match gen and reco jets\n",
    "    matched_indices_gen, matched_indices_reco = match_jets(selected_gen_jet_pt, selected_reco_jet_pt, deltaR_cut=0.4)\n",
    "\n",
    "    # Calculate ratio of reco_jet_pt to gen_jet_pt for matched jets\n",
    "    ratio_values_in_bin = []\n",
    "    for gen_idx, reco_idx in zip(matched_indices_gen, matched_indices_reco):\n",
    "        if gen_jet_pt[gen_idx] != 0:\n",
    "            ratio_values_in_bin.append(reco_jet_pt[reco_idx] / gen_jet_pt[gen_idx])\n",
    "\n",
    "    # Check if ratio_values_in_bin is not empty\n",
    "    if ratio_values_in_bin:\n",
    "        # Calculate IQR and median for ratio_values_in_bin\n",
    "        ratio_iqr = np.percentile(ratio_values_in_bin, 75) - np.percentile(ratio_values_in_bin, 25)\n",
    "        ratio_median = np.median(ratio_values_in_bin)\n",
    "        ratio_iqr_median_ratio = ratio_iqr / ratio_median if ratio_median != 0 else np.nan\n",
    "        ratio_iqr_median.append(ratio_iqr_median_ratio)\n",
    "    else:\n",
    "        # Append NaN if ratio_values_in_bin is empty\n",
    "        ratio_iqr_median.append(np.nan)\n",
    "\n",
    "# Create a TGraph\n",
    "gr_ratio = ROOT.TGraph(len(x_vals), np.array(x_vals), np.array(ratio_iqr_median))\n",
    "\n",
    "# Set titles to empty strings\n",
    "gr_ratio.SetTitle(\"\")\n",
    "\n",
    "# Create a canvas\n",
    "canvas = ROOT.TCanvas(\"canvas\", \"Ratio IQR/Median vs Jet pT\", 800, 600)\n",
    "\n",
    "# Draw the graph\n",
    "gr_ratio.SetMarkerStyle(20)\n",
    "gr_ratio.SetMarkerColor(ROOT.kBlue)\n",
    "gr_ratio.SetLineColor(ROOT.kBlue)\n",
    "gr_ratio.GetXaxis().SetTitle(\"Jet PT, Gen (GeV)\")\n",
    "gr_ratio.GetYaxis().SetTitle(\"Response IQR/Median\")\n",
    "gr_ratio.Draw(\"APL\")\n",
    "\n",
    "# Get the X and Y axes\n",
    "xaxis = gr_ratio.GetXaxis()\n",
    "yaxis = gr_ratio.GetYaxis()\n",
    "\n",
    "# Set tick length for all four axes\n",
    "xaxis.SetTickLength(0.03)\n",
    "yaxis.SetTickLength(0.03)\n",
    "\n",
    "# Add legend\n",
    "legend = ROOT.TLegend(0.75, 0.75, 0.9, 0.9)\n",
    "legend.AddEntry(gr_ratio, \"Pred FP\", \"p\")\n",
    "legend.Draw()\n",
    "\n",
    "# Show the canvas\n",
    "canvas.Draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f10887",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For each bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcde5d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "import numpy as np\n",
    "\n",
    "# Define bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100]\n",
    "\n",
    "# Define the bin index for the first bin (index 0)\n",
    "bin_index = 0\n",
    "\n",
    "# Get the bin limits\n",
    "lim_low = bins[bin_index]\n",
    "lim_hi = bins[bin_index + 1]\n",
    "\n",
    "# Apply selection to gen_jet indices within the bin\n",
    "selected_indices = [idx for idx, pt in enumerate(gen_jet_pt) if lim_low < pt <= lim_hi]\n",
    "\n",
    "# Match selected gen_jet indices to reco_jet values\n",
    "matched_reco_jet_values = [reco_jet_pt[idx] for idx in selected_indices]\n",
    "\n",
    "# Check if there are any values in both selected gen_jet_pt and matched reco_jet_pt\n",
    "if len(selected_indices) == 0 or len(matched_reco_jet_values) == 0:\n",
    "    print(\"No events in the bin.\")\n",
    "else:\n",
    "    # Calculate IQR and median for reco_jet_pt / gen_jet_pt values\n",
    "    ratio_values_in_bin = [reco / gen_jet_pt[idx] for reco, idx in zip(matched_reco_jet_values, selected_indices)]\n",
    "    ratio_iqr = np.percentile(ratio_values_in_bin, 75) - np.percentile(ratio_values_in_bin, 25)\n",
    "    ratio_median = np.median(ratio_values_in_bin)\n",
    "    ratio_iqr_median_ratio = ratio_iqr / ratio_median\n",
    "\n",
    "    # Create a canvas for the bin\n",
    "    canvas = ROOT.TCanvas(f\"canvas_{bin_index}\", f\"Response Distribution in Bin {bin_index + 1}\", 800, 600)\n",
    "\n",
    "    # Create histogram for the response distribution in the bin\n",
    "    hist = ROOT.TH1F(f\"hist_{bin_index}\", f\"  Bin {bin_index + 1}\", 20, 0, 2)  # Adjust binning as needed\n",
    "    for val in ratio_values_in_bin:\n",
    "        hist.Fill(val)\n",
    "\n",
    "    # Draw the histogram\n",
    "    hist.SetLineColor(ROOT.kBlue)\n",
    "    hist.Draw()\n",
    "\n",
    "    # Add legend\n",
    "    legend = ROOT.TLegend(0.75, 0.75, 0.9, 0.9)\n",
    "    legend.AddEntry(hist, f\"Bin {bin_index + 1}\", \"l\")\n",
    "    legend.Draw()\n",
    "\n",
    "    # Show the canvas\n",
    "    canvas.Draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34611cf6",
   "metadata": {},
   "source": [
    "# Jets matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfae73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deltar(eta1, phi1, eta2, phi2):\n",
    "    deta = eta1 - eta2\n",
    "    dphi = deltaphi(phi1, phi2)\n",
    "    return np.sqrt(deta**2 + dphi**2)\n",
    "\n",
    "\n",
    "def match_jets(jets1, jets2, deltaR_cut):\n",
    "    iev = len(jets1)\n",
    "    jet_inds_1_ev = []\n",
    "    jet_inds_2_ev = []\n",
    "    for ev in range(iev):\n",
    "        j1 = jets1[ev]\n",
    "        j2 = jets2[ev]\n",
    "\n",
    "        jet_inds_1 = []\n",
    "        jet_inds_2 = []\n",
    "        for ij1 in range(len(j1)):\n",
    "            drs = np.zeros(len(j2), dtype=np.float64)\n",
    "            for ij2 in range(len(j2)):\n",
    "                eta1 = j1.eta[ij1]\n",
    "                eta2 = j2.eta[ij2]\n",
    "                phi1 = j1.phi[ij1]\n",
    "                phi2 = j2.phi[ij2]\n",
    "\n",
    "                # Workaround for https://github.com/scikit-hep/vector/issues/303\n",
    "                # dr = j1[ij1].deltaR(j2[ij2])\n",
    "                dr = deltar(eta1, phi1, eta2, phi2)\n",
    "                drs[ij2] = dr\n",
    "            if len(drs) > 0:\n",
    "                min_idx_dr = np.argmin(drs)\n",
    "                if drs[min_idx_dr] < deltaR_cut:\n",
    "                    jet_inds_1.append(ij1)\n",
    "                    jet_inds_2.append(min_idx_dr)\n",
    "        jet_inds_1_ev.append(jet_inds_1)\n",
    "        jet_inds_2_ev.append(jet_inds_2)\n",
    "    return jet_inds_1_ev, jet_inds_2_ev\n",
    "\n",
    "\n",
    "def match_two_jet_collections(jets_coll, name1, name2, jet_match_dr):\n",
    "    num_events = len(jets_coll[name1])\n",
    "    vec1 = vector.awk(\n",
    "        awkward.zip(\n",
    "            {\n",
    "                \"pt\": jets_coll[name1].pt,\n",
    "                \"eta\": jets_coll[name1].eta,\n",
    "                \"phi\": jets_coll[name1].phi,\n",
    "                \"energy\": jets_coll[name1].energy,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    vec2 = vector.awk(\n",
    "        awkward.zip(\n",
    "            {\n",
    "                \"pt\": jets_coll[name2].pt,\n",
    "                \"eta\": jets_coll[name2].eta,\n",
    "                \"phi\": jets_coll[name2].phi,\n",
    "                \"energy\": jets_coll[name2].energy,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    ret = match_jets(vec1, vec2, jet_match_dr)\n",
    "    j1_idx = awkward.from_iter(ret[0])\n",
    "    j2_idx = awkward.from_iter(ret[1])\n",
    "\n",
    "    num_jets = len(awkward.flatten(j1_idx))\n",
    "\n",
    "    # In case there are no jets matched, create dummy array to ensure correct types\n",
    "    if num_jets > 0:\n",
    "        c1_to_c2 = awkward.Array({name1: j1_idx, name2: j2_idx})\n",
    "    else:\n",
    "        dummy = build_dummy_array(num_events)\n",
    "        c1_to_c2 = awkward.Array({name1: dummy, name2: dummy})\n",
    "\n",
    "    return c1_to_c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca25ddc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fea7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import fastjet as fj\n",
    "\n",
    "# Define deltaR function\n",
    "def deltaR(eta1, phi1, eta2, phi2):\n",
    "    deta = eta1 - eta2\n",
    "    dphi = np.abs(phi1 - phi2)\n",
    "    if dphi > np.pi:\n",
    "        dphi -= 2 * np.pi\n",
    "    return np.sqrt(deta**2 + dphi**2)\n",
    "\n",
    "# Define matching criteria\n",
    "deltaR_cut = 0.4  # Adjust as needed\n",
    "\n",
    "# Define clustering parameters\n",
    "jet_radius = 0.4\n",
    "jet_ptmin = 20\n",
    "\n",
    "# Define bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100]\n",
    "\n",
    "# Cluster jets function\n",
    "def cluster_jets(particles):\n",
    "    jetdef = fj.JetDefinition(fj.antikt_algorithm, jet_radius)\n",
    "    cluster = fj.ClusterSequence(particles, jetdef)\n",
    "    jets = cluster.inclusive_jets(jet_ptmin)\n",
    "    return jets\n",
    "\n",
    "# Match jets function\n",
    "def match_jets(gen_jets, reco_jets):\n",
    "    matched_reco_jet_values = []\n",
    "    for gen_jet in gen_jets:\n",
    "        matched_jet = None\n",
    "        min_deltaR = float('inf')\n",
    "        for reco_jet in reco_jets:\n",
    "            dR = deltaR(gen_jet.eta(), gen_jet.phi(), reco_jet.eta(), reco_jet.phi())\n",
    "            if dR < deltaR_cut and dR < min_deltaR:\n",
    "                matched_jet = reco_jet\n",
    "                min_deltaR = dR\n",
    "        if matched_jet is not None:\n",
    "            matched_reco_jet_values.append(matched_jet)\n",
    "    return matched_reco_jet_values\n",
    "\n",
    "# Iterate over bins\n",
    "for i in range(len(bins) - 1):\n",
    "    lim_low = bins[i]\n",
    "    lim_hi = bins[i + 1]\n",
    "\n",
    "    # Apply selection to gen_jet indices within the bin\n",
    "    selected_indices_gen_jet = [idx for idx, jet in enumerate(gen_jets) if lim_low < jet.pt() <= lim_hi]\n",
    "\n",
    "    # Cluster jets for reco_particles\n",
    "    reco_jets = cluster_jets(reco_particles)\n",
    "\n",
    "    # Match gen jets to reco jets\n",
    "    matched_reco_jet_values = match_jets(gen_jets, reco_jets)\n",
    "\n",
    "    # Calculate the number of events in the bin\n",
    "    num_events_gen_jet = len(selected_indices_gen_jet)\n",
    "    num_events_reco_jet = len(matched_reco_jet_values)\n",
    "\n",
    "    print(f\"Bin {i+1}: Number of events (Gen Jet): {num_events_gen_jet}, Number of events (Reco Jet): {num_events_reco_jet}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaf262b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e35dab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d7b245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4827d49b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf860219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6201ad7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370fbf2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa076d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b88ca76f",
   "metadata": {},
   "source": [
    "# Quantization INT8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde8e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.eval()\n",
    "loaded_model.qconfig = torch.ao.quantization.get_default_qconfig('onednn')\n",
    "custom_module_config = {\n",
    "        \"float_to_observed_custom_module_class\": {torch.nn.MultiheadAttention: QuantizeableMultiheadAttention},\n",
    "        \"observed_to_quantized_custom_module_class\": {QuantizeableMultiheadAttention: QuantizedMultiheadAttention},\n",
    "}\n",
    "\n",
    "model_prepared = torch.ao.quantization.prepare(model, prepare_custom_config_dict=custom_module_config)\n",
    "\n",
    "#calibrate on data\n",
    "num_events_to_calibrate = 100\n",
    "for ind in range(max_events_train,max_events_train+num_events_to_calibrate):\n",
    "    _X = torch.unsqueeze(torch.tensor(ds_train[ind][\"X\"]).to(torch.float32), 0)\n",
    "    _mask = _X[:, :, 0]!=0\n",
    "    model_prepared(_X, _mask)\n",
    "\n",
    "model_int8 = torch.ao.quantization.convert(model_prepared,convert_custom_config_dict=custom_module_config,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57332ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_int8.quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66badb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_quantized = torch.quantize_per_tensor((X_features_padded[:, :, 0]!=0).to(torch.float32), 1, 0, torch.quint8)\n",
    "preds = model_int8(X_features_padded, mask_quantized)\n",
    "preds = preds[0].detach(), preds[1].detach()\n",
    "preds_unpacked_int8 = unpack_predictions(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66085dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_int8 = mlpf_loss(targets_unpacked, preds_unpacked_int8, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf5d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_pred_int8 = preds_unpacked_int8[\"pt\"][msk_true_particles].numpy()\n",
    "eta_pred_int8 = preds_unpacked_int8[\"eta\"][msk_true_particles].numpy()\n",
    "sphi_pred_int8 = preds_unpacked_int8[\"sin_phi\"][msk_true_particles].numpy()\n",
    "cphi_pred_int8 = preds_unpacked_int8[\"cos_phi\"][msk_true_particles].numpy()\n",
    "energy_pred_int8 = preds_unpacked_int8[\"energy\"][msk_true_particles].numpy()\n",
    "\n",
    "px = preds_unpacked_int8[\"pt\"] * preds_unpacked_int8[\"cos_phi\"] * msk_true_particles\n",
    "py = preds_unpacked_int8[\"pt\"] * preds_unpacked_int8[\"sin_phi\"] * msk_true_particles\n",
    "pred_met_int8 = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d437a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "px_int8 = preds_unpacked_int8[\"pt\"] * preds_unpacked_int8[\"cos_phi\"] * msk_true_particles\n",
    "py_int8 = preds_unpacked_int8[\"pt\"] * preds_unpacked_int8[\"sin_phi\"] * msk_true_particles\n",
    "pz_int8 = preds_unpacked_int8[\"pt\"] * np.sinh(preds_unpacked_int8[\"eta\"]) * msk_true_particles\n",
    "phi_int8 = np.arctan2(preds_unpacked_int8[\"sin_phi\"], preds_unpacked_int8[\"cos_phi\"]) * msk_true_particles\n",
    "\n",
    "px_np_int8 = px_int8.detach().cpu().numpy()\n",
    "py_np_int8 = py_int8.detach().cpu().numpy()\n",
    "pz_np_int8 = pz_int8.detach().cpu().numpy()\n",
    "phi_np_int8 = phi_int8.detach().cpu().numpy()\n",
    "\n",
    "# print(\"px_np\", px_np)\n",
    "# print(\"py_np\", py_np)\n",
    "# print(\"pz_np\", pz_np)\n",
    "# print(\"phi_np\", phi_np)\n",
    "\n",
    "quantized_mom = np.sqrt(np.sum(px_np_int8, axis=1)**2 + np.sum(py_np_int8, axis=1)**2 + np.sum(pz_np_int8, axis=1)**2)\n",
    "int8_E_np = np.sqrt(px_np_int8**2 + py_np_int8**2 + pz_np_int8**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed2aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# four-vectors\n",
    "# px_py_pz_E = np.column_stack((px_np, py_np, pz_np, E)) \n",
    "print(\"int8_E Shape\", int8_E_np.shape)\n",
    "print(\"px Shape\", px_np_int8.shape)\n",
    "print(\"py Shape\", py_np_int8.shape)\n",
    "print(\"pz Shape\", pz_np_int8.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd3c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INT8_Jets_particles = []   # TODO:Change this to reco_jets\n",
    "for ip in range(int8_E_np.shape[0]):\n",
    "    for ix in range(int8_E_np.shape[1]):\n",
    "        px_value = float(px_np_int8[ip, ix])\n",
    "        py_value = float(py_np_int8[ip, ix])\n",
    "        pz_value = float(pz_np_int8[ip, ix])\n",
    "        E_value = float(int8_E_np[ip, ix])\n",
    "        particle = fj.PseudoJet(px_value, py_value, pz_value, E_value)\n",
    "        INT8_Jets_particles.append(particle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc1bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "num_threads = 2 \n",
    "\n",
    "def cluster_jets(particles):\n",
    "    jetdef = fj.JetDefinition(fj.antikt_algorithm, 0.08)\n",
    "#     jet_ptcut = 20\n",
    "    \n",
    "    cluster = fj.ClusterSequence(INT8_Jets_particles, jetdef)\n",
    "    jets = cluster.inclusive_jets()\n",
    "    \n",
    "    return jets\n",
    "\n",
    "chunks = [particles[i::num_threads] for i in range(num_threads)]\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    futures = [executor.submit(cluster_jets, chunk) for chunk in chunks]\n",
    "\n",
    "INT8_reco_jets = []\n",
    "for future in futures:\n",
    "    INT8_reco_jets.extend(future.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15ea033",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, jet in enumerate(INT8_reco_jets):\n",
    "    print(\"Jet\", i+1, \":\", jet.pt(), jet.eta(), jet.phi(), jet.e())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d99ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Jets:\", len(INT8_reco_jets))\n",
    "\n",
    "\n",
    "INT8_jet_pt = [jet.pt() for jet in INT8_reco_jets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "import numpy as np\n",
    "\n",
    "# Define bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100, 200]\n",
    "\n",
    "# Initialize lists to store values\n",
    "x_vals = []\n",
    "reco_ratio_iqr_median = []\n",
    "int8_ratio_iqr_median = []\n",
    "\n",
    "# Iterate over bins\n",
    "for i in range(len(bins) - 1):\n",
    "    lim_low = bins[i]\n",
    "    lim_hi = bins[i + 1]\n",
    "    x_vals.append(np.mean([lim_low, lim_hi]))\n",
    "\n",
    "    # Apply selection to gen_jet indices within the bin\n",
    "    selected_indices = [idx for idx, pt in enumerate(gen_jet_pt) if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Match selected gen_jet indices to reco_jet values\n",
    "    matched_reco_jet_values = [reco_jet_pt[idx] for idx in selected_indices]\n",
    "\n",
    "    # Match selected gen_jet indices to INT8_jet values\n",
    "    matched_int8_jet_values = [INT8_jet_pt[idx] for idx in selected_indices]\n",
    "\n",
    "    # Check if there are any values in both selected gen_jet_pt and matched reco_jet_pt and INT8_jet_pt\n",
    "    if len(selected_indices) == 0 or len(matched_reco_jet_values) == 0 or len(matched_int8_jet_values) == 0:\n",
    "        # Append NaN values\n",
    "        reco_ratio_iqr_median.append(np.nan)\n",
    "        int8_ratio_iqr_median.append(np.nan)\n",
    "        continue\n",
    "\n",
    "    # Calculate IQR and median for matched reco_jet_pt / gen_jet_pt values\n",
    "    reco_ratio_values_in_bin = [reco / gen_jet_pt[idx] for reco, idx in zip(matched_reco_jet_values, selected_indices)]\n",
    "    reco_ratio_iqr = np.percentile(reco_ratio_values_in_bin, 75) - np.percentile(reco_ratio_values_in_bin, 25)\n",
    "    reco_ratio_median = np.median(reco_ratio_values_in_bin)\n",
    "    reco_ratio_iqr_median_ratio = reco_ratio_iqr / reco_ratio_median\n",
    "    reco_ratio_iqr_median.append(reco_ratio_iqr_median_ratio)\n",
    "\n",
    "    # Calculate IQR and median for matched INT8_jet_pt / gen_jet_pt values\n",
    "    int8_ratio_values_in_bin = [int8 / gen_jet_pt[idx] for int8, idx in zip(matched_int8_jet_values, selected_indices)]\n",
    "    int8_ratio_iqr = np.percentile(int8_ratio_values_in_bin, 75) - np.percentile(int8_ratio_values_in_bin, 25)\n",
    "    int8_ratio_median = np.median(int8_ratio_values_in_bin)\n",
    "    int8_ratio_iqr_median_ratio = int8_ratio_iqr / int8_ratio_median\n",
    "    int8_ratio_iqr_median.append(int8_ratio_iqr_median_ratio)\n",
    "\n",
    "# Filter out NaN values\n",
    "x_vals_filtered_reco = [x for x, y in zip(x_vals, reco_ratio_iqr_median) if not np.isnan(y)]\n",
    "reco_ratio_iqr_median_filtered = [y for y in reco_ratio_iqr_median if not np.isnan(y)]\n",
    "\n",
    "x_vals_filtered_int8 = [x for x, y in zip(x_vals, int8_ratio_iqr_median) if not np.isnan(y)]\n",
    "int8_ratio_iqr_median_filtered = [y for y in int8_ratio_iqr_median if not np.isnan(y)]\n",
    "\n",
    "# Create TGraphs with filtered values\n",
    "gr_reco_ratio = ROOT.TGraph(len(x_vals_filtered_reco), np.array(x_vals_filtered_reco), np.array(reco_ratio_iqr_median_filtered))\n",
    "gr_int8_ratio = ROOT.TGraph(len(x_vals_filtered_int8), np.array(x_vals_filtered_int8), np.array(int8_ratio_iqr_median_filtered))\n",
    "\n",
    "# Set the titles to empty strings\n",
    "gr_reco_ratio.SetTitle(\"\")\n",
    "gr_int8_ratio.SetTitle(\"\")\n",
    "\n",
    "# Create a canvas\n",
    "canvas = ROOT.TCanvas(\"canvas\", \"Ratio IQR/Median vs Jet pT\", 800, 600)\n",
    "\n",
    "# Draw the first graph (Reco_jet_pt/gen_jet_pt)\n",
    "gr_reco_ratio.SetMarkerStyle(20)\n",
    "gr_reco_ratio.SetMarkerColor(ROOT.kBlue)\n",
    "gr_reco_ratio.SetLineColor(ROOT.kBlue)\n",
    "gr_reco_ratio.GetXaxis().SetTitle(\"Jet P_{T,gen} (GeV)\")\n",
    "gr_reco_ratio.GetYaxis().SetTitle(\"Response IQR / Median\")\n",
    "gr_reco_ratio.Draw(\"APL\")\n",
    "\n",
    "# Draw the second graph (INT8_jet_pt/gen_jet_pt) on the same canvas\n",
    "gr_int8_ratio.SetMarkerStyle(20)\n",
    "gr_int8_ratio.SetMarkerColor(ROOT.kRed)\n",
    "gr_int8_ratio.SetLineColor(ROOT.kRed)\n",
    "gr_int8_ratio.Draw(\"PL same\")\n",
    "\n",
    "# Get the X and Y axes\n",
    "xaxis = gr_reco_ratio.GetXaxis()\n",
    "yaxis = gr_reco_ratio.GetYaxis()\n",
    "\n",
    "# Set tick length for all four axes\n",
    "xaxis.SetTickLength(0.03)\n",
    "yaxis.SetTickLength(0.03)\n",
    "\n",
    "# Set y-axis range to show both plots\n",
    "min_y = min(min(reco_ratio_iqr_median_filtered), min(int8_ratio_iqr_median_filtered))\n",
    "max_y = max(max(reco_ratio_iqr_median_filtered), max(int8_ratio_iqr_median_filtered))\n",
    "yaxis.SetRangeUser(min_y * 0.9, max_y * 1.1)\n",
    "\n",
    "# Add legend\n",
    "legend = ROOT.TLegend(0.75, 0.75, 0.9, 0.9)\n",
    "legend.AddEntry(gr_reco_ratio, \"Pred_FP32\", \"lp\")\n",
    "legend.AddEntry(gr_int8_ratio, \"Pred_INT8\", \"lp\")\n",
    "legend.Draw()\n",
    "\n",
    "# Show the canvas\n",
    "canvas.Draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7af5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "import numpy as np\n",
    "\n",
    "# Define bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100, 200]\n",
    "\n",
    "# Initialize lists to store values\n",
    "x_vals = []\n",
    "reco_ratio_iqr_median = []\n",
    "int8_ratio_iqr_median = []\n",
    "\n",
    "# Iterate over bins\n",
    "for i in range(len(bins) - 1):\n",
    "    lim_low = bins[i]\n",
    "    lim_hi = bins[i + 1]\n",
    "    x_vals.append(np.mean([lim_low, lim_hi]))\n",
    "\n",
    "    # Apply selection to gen_jet indices within the bin\n",
    "    selected_indices = [idx for idx, pt in enumerate(gen_jet_pt) if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Match selected gen_jet indices to reco_jet values\n",
    "    matched_reco_jet_values = [reco_jet_pt[idx] for idx in selected_indices]\n",
    "\n",
    "    # Match selected gen_jet indices to INT8_jet values\n",
    "    matched_int8_jet_values = [INT8_jet_pt[idx] for idx in selected_indices if idx < len(INT8_jet_pt)]\n",
    "\n",
    "    # Check if there are any values in both selected gen_jet_pt and matched reco_jet_pt and INT8_jet_pt\n",
    "    if len(selected_indices) == 0 or len(matched_reco_jet_values) == 0 or len(matched_int8_jet_values) == 0:\n",
    "        # Append NaN values\n",
    "        reco_ratio_iqr_median.append(np.nan)\n",
    "        int8_ratio_iqr_median.append(np.nan)\n",
    "        continue\n",
    "\n",
    "    # Calculate IQR and median for matched reco_jet_pt / gen_jet_pt values\n",
    "    reco_ratio_values_in_bin = [reco / gen_jet_pt[idx] for reco, idx in zip(matched_reco_jet_values, selected_indices)]\n",
    "    reco_ratio_iqr = np.percentile(reco_ratio_values_in_bin, 75) - np.percentile(reco_ratio_values_in_bin, 25)\n",
    "    reco_ratio_median = np.median(reco_ratio_values_in_bin)\n",
    "    reco_ratio_iqr_median_ratio = reco_ratio_iqr / reco_ratio_median\n",
    "    reco_ratio_iqr_median.append(reco_ratio_iqr_median_ratio)\n",
    "\n",
    "    # Calculate IQR and median for matched INT8_jet_pt / gen_jet_pt values\n",
    "    int8_ratio_values_in_bin = [int8 / gen_jet_pt[idx] for int8, idx in zip(matched_int8_jet_values, selected_indices)]\n",
    "    int8_ratio_iqr = np.percentile(int8_ratio_values_in_bin, 75) - np.percentile(int8_ratio_values_in_bin, 25)\n",
    "    int8_ratio_median = np.median(int8_ratio_values_in_bin)\n",
    "    int8_ratio_iqr_median_ratio = int8_ratio_iqr / int8_ratio_median\n",
    "    int8_ratio_iqr_median.append(int8_ratio_iqr_median_ratio)\n",
    "\n",
    "# Filter out NaN values\n",
    "x_vals_filtered_reco = [x for x, y in zip(x_vals, reco_ratio_iqr_median) if not np.isnan(y)]\n",
    "reco_ratio_iqr_median_filtered = [y for y in reco_ratio_iqr_median if not np.isnan(y)]\n",
    "\n",
    "x_vals_filtered_int8 = [x for x, y in zip(x_vals, int8_ratio_iqr_median) if not np.isnan(y)]\n",
    "int8_ratio_iqr_median_filtered = [y for y in int8_ratio_iqr_median if not np.isnan(y)]\n",
    "\n",
    "# Create TGraphs with filtered values\n",
    "gr_reco_ratio = ROOT.TGraph(len(x_vals_filtered_reco), np.array(x_vals_filtered_reco), np.array(reco_ratio_iqr_median_filtered))\n",
    "gr_int8_ratio = ROOT.TGraph(len(x_vals_filtered_int8), np.array(x_vals_filtered_int8), np.array(int8_ratio_iqr_median_filtered))\n",
    "\n",
    "# Set the titles to empty strings\n",
    "gr_reco_ratio.SetTitle(\"\")\n",
    "gr_int8_ratio.SetTitle(\"\")\n",
    "\n",
    "# Create a canvas\n",
    "canvas = ROOT.TCanvas(\"canvas\", \"Ratio IQR/Median vs Jet pT\", 800, 600)\n",
    "\n",
    "# Draw the first graph (Reco_jet_pt/gen_jet_pt)\n",
    "gr_reco_ratio.SetMarkerStyle(20)\n",
    "gr_reco_ratio.SetMarkerColor(ROOT.kBlue)\n",
    "gr_reco_ratio.SetLineColor(ROOT.kBlue)\n",
    "gr_reco_ratio.GetXaxis().SetTitle(\"Jet P_{T,gen} (GeV)\")\n",
    "gr_reco_ratio.GetYaxis().SetTitle(\"Response IQR / Median\")\n",
    "gr_reco_ratio.Draw(\"APL\")\n",
    "\n",
    "# Draw the second graph (INT8_jet_pt/gen_jet_pt) on the same canvas\n",
    "gr_int8_ratio.SetMarkerStyle(20)\n",
    "gr_int8_ratio.SetMarkerColor(ROOT.kRed)\n",
    "gr_int8_ratio.SetLineColor(ROOT.kRed)\n",
    "gr_int8_ratio.Draw(\"PL same\")\n",
    "\n",
    "# Get the X and Y axes\n",
    "xaxis = gr_reco_ratio.GetXaxis()\n",
    "yaxis = gr_reco_ratio.GetYaxis()\n",
    "\n",
    "# Set tick length for all four axes\n",
    "xaxis.SetTickLength(0.03)\n",
    "yaxis.SetTickLength(0.03)\n",
    "\n",
    "# Set y-axis range to show both plots\n",
    "min_y = min(min(reco_ratio_iqr_median_filtered), min(int8_ratio_iqr_median_filtered))\n",
    "max_y = max(max(reco_ratio_iqr_median_filtered), max(int8_ratio_iqr_median_filtered))\n",
    "yaxis.SetRangeUser(min_y * 0.9, max_y * 1.1)\n",
    "\n",
    "# Add legend\n",
    "legend = ROOT.TLegend(0.75, 0.75, 0.9, 0.9)\n",
    "legend.AddEntry(gr_reco_ratio, \"Pred_FP32\", \"lp\")\n",
    "legend.AddEntry(gr_int8_ratio, \"Pred_INT8\", \"lp\")\n",
    "legend.Draw()\n",
    "\n",
    "# Show the canvas\n",
    "canvas.Draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacb1cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e1952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe5fcaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
