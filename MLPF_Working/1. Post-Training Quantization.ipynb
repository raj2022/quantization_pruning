{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd7b0ee6",
   "metadata": {},
   "source": [
    "# Post-Training Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c41252e",
   "metadata": {},
   "source": [
    "Task as defined by Joosep:\n",
    "``I would perhaps start with post-training quantization``.\n",
    "either in tensorflow: https://www.tensorflow.org/model_optimization/guide/quantization/post_training\n",
    "\n",
    "or on top of the exported ONNX model:\n",
    "https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html\n",
    "\n",
    "``you need to set up a basic code that performs post-training quantization on a trained model, evaluated the speed and loss of physics performance\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e206e79",
   "metadata": {},
   "source": [
    "## Post Training quantization in TensorFlow \n",
    "\n",
    "Post-training quantization is a conversion technique that can reduce model size while also improving CPU and hardware accelerator latency, with little degradation in model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff52fc71",
   "metadata": {},
   "source": [
    "### Optimization methods\n",
    "There are several post-training quantization options to choose from. \n",
    "\n",
    "| Model Technique         | Benefits                            | Hardware                                    |\n",
    "|-------------------------|------------------------------------|---------------------------------------------|\n",
    "| Dynamic range quantization | 4x smaller, 2x-3x speedup       | CPU                                         |\n",
    "| Full integer quantization  | 4x smaller, 3x+ speedup         | CPU, Edge TPU, Microcontrollers             |\n",
    "| Float16 quantization      | 2x smaller, GPU acceleration   | CPU, GPU                                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33b9bc9",
   "metadata": {},
   "source": [
    "The following decision tree can help determine which post-training quantization method is best for your use case:\n",
    "![images_1](https://www.tensorflow.org/static/lite/performance/images/optimization.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ef3b98",
   "metadata": {},
   "source": [
    "We will start with `Dynamic Range Quantization` because it provides reduced memory usage and faster computation without you having to provide a representative dataset for calibration. This type of quantization, statically quantizes only the weights from floating point to integer at conversion time, which provides 8-bits of precision:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n",
    "```\n",
    "To further reduce latency during inference, \"dynamic-range\" operators dynamically quantize activations based on their range to 8-bits and perform computations with 8-bit weights and activations. This optimization provides latencies close to fully fixed-point inferences. However, the outputs are still stored using floating point so the increased speed of dynamic-range ops is less than a full fixed-point computation.\n",
    "\n",
    "\n",
    "**Quantization involves reducing the precision of the weights and activations in a model, typically from 32-bit floating point values to 8-bit integers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "02c6f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import joblib\n",
    "\n",
    "\n",
    "import h5py\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "293635e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-19 12:16:29--  https://huggingface.co/jpata/particleflow/blob/clic_clusters_v1.6/opt-96-5.346523.pkl\n",
      "Resolving huggingface.co (huggingface.co)... 2600:9000:248c:1a00:17:b174:6d00:93a1, 2600:9000:248c:b200:17:b174:6d00:93a1, 2600:9000:248c:1000:17:b174:6d00:93a1, ...\n",
      "Connecting to huggingface.co (huggingface.co)|2600:9000:248c:1a00:17:b174:6d00:93a1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 42096 (41K) [text/html]\n",
      "Saving to: ‘opt-96-5.346523.pkl.1’\n",
      "\n",
      "opt-96-5.346523.pkl 100%[===================>]  41.11K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2023-10-19 12:16:29 (416 KB/s) - ‘opt-96-5.346523.pkl.1’ saved [42096/42096]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/jpata/particleflow/blob/clic_clusters_v1.6/opt-96-5.346523.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aede9de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-19 12:15:39--  https://huggingface.co/jpata/particleflow/blob/clic_clusters_v1.6/weights-96-5.346523.hdf5\n",
      "Resolving huggingface.co (huggingface.co)... 2600:9000:248c:9000:17:b174:6d00:93a1, 2600:9000:248c:7e00:17:b174:6d00:93a1, 2600:9000:248c:ea00:17:b174:6d00:93a1, ...\n",
      "Connecting to huggingface.co (huggingface.co)|2600:9000:248c:9000:17:b174:6d00:93a1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 37791 (37K) [text/html]\n",
      "Saving to: ‘weights-96-5.346523.hdf5’\n",
      "\n",
      "weights-96-5.346523 100%[===================>]  36.91K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2023-10-19 12:15:40 (387 KB/s) - ‘weights-96-5.346523.hdf5’ saved [37791/37791]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/jpata/particleflow/blob/clic_clusters_v1.6/weights-96-5.346523.hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae944c40",
   "metadata": {},
   "source": [
    "### Reading the HDF5 Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cd3986",
   "metadata": {},
   "source": [
    "Method I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0090bf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['cg_id_0', 'cg_id_1', 'cg_id_2', 'cg_id_3', 'cg_id_4', 'cg_id_5', 'cg_reg_0', 'cg_reg_1', 'cg_reg_2', 'cg_reg_3', 'cg_reg_4', 'cg_reg_5', 'input_encoding_clic', 'node_encoding', 'normalization', 'output_decoding', 'top_level_model_weights']>\n",
      "<class 'h5py._hl.group.Group'>\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('weights-96-5.346523.hdf5', 'r') as f:\n",
    "    # Print all root level object names (aka keys) \n",
    "    # these can be group or dataset names \n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    # get first object name/key; may or may NOT be a group\n",
    "    a_group_key = list(f.keys())[0]\n",
    "    \n",
    "    # get the object type for a_group_key: usually group or dataset\n",
    "    print(type(f[a_group_key])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6879d8ae",
   "metadata": {},
   "source": [
    "Method II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a190310a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cg_id_0\n",
      "<class 'h5py._hl.group.Group'>\n",
      "cg_id_1\n",
      "<class 'h5py._hl.group.Group'>\n",
      "cg_id_2\n",
      "<class 'h5py._hl.group.Group'>\n",
      "cg_id_3\n",
      "<class 'h5py._hl.group.Group'>\n",
      "cg_id_4\n",
      "<class 'h5py._hl.group.Group'>\n",
      "cg_id_5\n",
      "<class 'h5py._hl.group.Group'>\n",
      "cg_reg_0\n",
      "<class 'h5py._hl.group.Group'>\n",
      "cg_reg_1\n",
      "<class 'h5py._hl.group.Group'>\n",
      "cg_reg_2\n",
      "<class 'h5py._hl.group.Group'>\n",
      "cg_reg_3\n",
      "<class 'h5py._hl.group.Group'>\n",
      "cg_reg_4\n",
      "<class 'h5py._hl.group.Group'>\n",
      "cg_reg_5\n",
      "<class 'h5py._hl.group.Group'>\n",
      "input_encoding_clic\n",
      "<class 'h5py._hl.group.Group'>\n",
      "node_encoding\n",
      "<class 'h5py._hl.group.Group'>\n",
      "normalization\n",
      "<class 'h5py._hl.group.Group'>\n",
      "output_decoding\n",
      "<class 'h5py._hl.group.Group'>\n",
      "top_level_model_weights\n",
      "<class 'h5py._hl.group.Group'>\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File('weights-96-5.346523.hdf5')\n",
    "\n",
    "# Studying the structure of the file by printing what HDF5 groups are present\n",
    "\n",
    "for key in f.keys():\n",
    "    print(key) #Names of the root level object names in HDF5 file - can be groups or datasets.\n",
    "    print(type(f[key])) # get the object type: usually group or dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30b66137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys within cg_id_0: ['cg_id_0', 'cg_id_0_ffn_dist_dense_0', 'cg_id_0_ffn_dist_dense_1', 'cg_id_0_ffn_dist_dense_2', 'cg_id_0_ffn_dist_dense_3']\n",
      "Keys within cg_id_1: ['cg_id_1', 'cg_id_1_ffn_dist_dense_0', 'cg_id_1_ffn_dist_dense_1', 'cg_id_1_ffn_dist_dense_2', 'cg_id_1_ffn_dist_dense_3']\n",
      "Keys within cg_id_2: ['cg_id_2', 'cg_id_2_ffn_dist_dense_0', 'cg_id_2_ffn_dist_dense_1', 'cg_id_2_ffn_dist_dense_2', 'cg_id_2_ffn_dist_dense_3']\n",
      "Keys within cg_id_3: ['cg_id_3', 'cg_id_3_ffn_dist_dense_0', 'cg_id_3_ffn_dist_dense_1', 'cg_id_3_ffn_dist_dense_2', 'cg_id_3_ffn_dist_dense_3']\n",
      "Keys within cg_id_4: ['cg_id_4', 'cg_id_4_ffn_dist_dense_0', 'cg_id_4_ffn_dist_dense_1', 'cg_id_4_ffn_dist_dense_2', 'cg_id_4_ffn_dist_dense_3']\n",
      "Keys within cg_id_5: ['cg_id_5', 'cg_id_5_ffn_dist_dense_0', 'cg_id_5_ffn_dist_dense_1', 'cg_id_5_ffn_dist_dense_2', 'cg_id_5_ffn_dist_dense_3']\n",
      "Keys within cg_reg_0: ['cg_reg_0', 'cg_reg_0_ffn_dist_dense_0', 'cg_reg_0_ffn_dist_dense_1', 'cg_reg_0_ffn_dist_dense_2', 'cg_reg_0_ffn_dist_dense_3']\n",
      "Keys within cg_reg_1: ['cg_reg_1', 'cg_reg_1_ffn_dist_dense_0', 'cg_reg_1_ffn_dist_dense_1', 'cg_reg_1_ffn_dist_dense_2', 'cg_reg_1_ffn_dist_dense_3']\n",
      "Keys within cg_reg_2: ['cg_reg_2', 'cg_reg_2_ffn_dist_dense_0', 'cg_reg_2_ffn_dist_dense_1', 'cg_reg_2_ffn_dist_dense_2', 'cg_reg_2_ffn_dist_dense_3']\n",
      "Keys within cg_reg_3: ['cg_reg_3', 'cg_reg_3_ffn_dist_dense_0', 'cg_reg_3_ffn_dist_dense_1', 'cg_reg_3_ffn_dist_dense_2', 'cg_reg_3_ffn_dist_dense_3']\n",
      "Keys within cg_reg_4: ['cg_reg_4', 'cg_reg_4_ffn_dist_dense_0', 'cg_reg_4_ffn_dist_dense_1', 'cg_reg_4_ffn_dist_dense_2', 'cg_reg_4_ffn_dist_dense_3']\n",
      "Keys within cg_reg_5: ['cg_reg_5', 'cg_reg_5_ffn_dist_dense_0', 'cg_reg_5_ffn_dist_dense_1', 'cg_reg_5_ffn_dist_dense_2', 'cg_reg_5_ffn_dist_dense_3']\n",
      "Keys within input_encoding_clic: []\n",
      "Keys within node_encoding: ['node_encoding_dense_0', 'node_encoding_dense_1']\n",
      "Keys within normalization: ['normalization']\n",
      "Keys within output_decoding: ['ffn_charge_dense_0', 'ffn_charge_dense_1', 'ffn_cls_dense_0', 'ffn_cls_dense_1', 'ffn_energy_dense_0', 'ffn_energy_dense_1', 'ffn_eta_dense_0', 'ffn_eta_dense_1', 'ffn_phi_dense_0', 'ffn_phi_dense_1', 'ffn_pt_dense_0', 'ffn_pt_dense_1', 'output_decoding']\n",
      "Keys within top_level_model_weights: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Open the HDF5 file\n",
    "with h5py.File('weights-96-5.346523.hdf5', 'r') as file:\n",
    "    # Loop through the keys and read the data\n",
    "    for key in file.keys():\n",
    "#         print(f\"Key: {key}\")\n",
    "\n",
    "        # Check if the key refers to a group\n",
    "        if isinstance(file[key], h5py.Group):\n",
    "            # Access the group and print its keys\n",
    "            group = file[key]\n",
    "            print(f\"Keys within {key}: {list(group.keys())}\")\n",
    "\n",
    "#             for subkey in group.keys():\n",
    "#                 data = group[subkey][()]\n",
    "#                 print(f\"Data in {subkey}:\")\n",
    "#                 print(data)\n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5a7f3d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys within cg_id_0: ['cg_id_0', 'cg_id_0_ffn_dist_dense_0', 'cg_id_0_ffn_dist_dense_1', 'cg_id_0_ffn_dist_dense_2', 'cg_id_0_ffn_dist_dense_3']\n",
      "Keys within cg_id_0: ['cg_id_0_layernorm1', 'cg_id_0_msg_0', 'cg_id_0_msg_1', 'message_building_layer_lsh']\n"
     ]
    }
   ],
   "source": [
    "# Open the HDF5 file\n",
    "with h5py.File('weights-96-5.346523.hdf5', 'r') as file:\n",
    "    # Access the group 'cg_id_0'\n",
    "    group_cg_id_0 = file['cg_id_0']\n",
    "    \n",
    "    # List the keys within the group\n",
    "    keys_in_cg_id_0 = list(group_cg_id_0.keys())\n",
    "    print(f\"Keys within cg_id_0: {keys_in_cg_id_0}\")\n",
    "    \n",
    "    #Choose a subkey\n",
    "    subkey='cg_id_0'\n",
    "    \n",
    "    subgroup = group_cg_id_0[subkey]\n",
    "    \n",
    "#     print(subgroup)\n",
    "    \n",
    "    keys_in_subgroup = list(subgroup.keys())\n",
    "    print(f\"Keys within {subkey}: {keys_in_subgroup}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cca98ad",
   "metadata": {},
   "source": [
    "## Reading files within the subgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4eef387b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys within cg_id_0_ffn_dist_dense_0: ['bias:0', 'kernel:0']\n",
      "Data in bias:0:\n",
      "[-0.42805317  0.9052358  -0.30682716  0.457701   -0.77937907  0.21298797\n",
      " -0.64650166 -0.54155844  0.62915844  0.71274954 -0.852341   -0.42336592\n",
      " -0.5791534   0.4973752   0.6579737  -0.2940757  -0.43727416 -0.3596791\n",
      " -0.53477967  0.2825041   0.52568066  0.38996714  0.349633   -0.9086682\n",
      "  0.38351122  1.346358   -0.7207639   0.28883758  0.70375586  0.8860439\n",
      " -0.41023213 -0.4066803   0.05410811 -0.06734214 -0.56115437 -0.58051896\n",
      " -0.3027428   0.74306834  0.45497474 -0.03278346 -0.24062441 -0.5881686\n",
      "  0.41746095  0.42180288  0.10265758 -0.2534956  -0.15646602 -0.9777575\n",
      "  0.8568099  -0.53954774  0.44354475 -0.4319153   0.63953334  0.18238714\n",
      " -0.7146191  -0.97242945 -0.22551654  0.22472627 -0.4724067  -0.99339795\n",
      " -0.9917985   0.13679531  0.31225142 -0.6768131 ]\n",
      "Data in kernel:0:\n",
      "[[ 0.31398082 -0.5096557  -0.03505608 ...  0.08097732  0.19537257\n",
      "  -0.35680908]\n",
      " [-0.04176841  0.10870415  0.01572619 ...  0.05416466  0.12002812\n",
      "  -0.62530637]\n",
      " [-0.07226437 -0.28455225  0.06761416 ...  0.11163151  0.09915844\n",
      "  -0.23784722]\n",
      " ...\n",
      " [ 0.05049501 -0.03838195  0.08743014 ...  0.05069735  0.1369824\n",
      "   0.58052385]\n",
      " [ 0.1408932   0.16768245 -0.09721825 ...  0.05845559  0.0926805\n",
      "  -0.03045342]\n",
      " [ 0.16930693  0.17753285 -0.04727688 ...  0.1146686  -0.2299448\n",
      "  -0.12432736]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('weights-96-5.346523.hdf5', 'r') as file:\n",
    "    # Access the subgroup 'cg_id_0_ffn_dist_dense_0'\n",
    "    subgroup_cg_id_0_ffn_dist_dense_0 = file['cg_id_0']['cg_id_0_ffn_dist_dense_0']\n",
    "    \n",
    "    # List the keys within the subgroup\n",
    "    keys_in_subgroup = list(subgroup_cg_id_0_ffn_dist_dense_0.keys())\n",
    "    print(f\"Keys within cg_id_0_ffn_dist_dense_0: {keys_in_subgroup}\")\n",
    "    \n",
    "    # Access and print data within the subgroup\n",
    "    for subkey in keys_in_subgroup:\n",
    "        data = subgroup_cg_id_0_ffn_dist_dense_0[subkey][()]\n",
    "        print(f\"Data in {subkey}:\")\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0748c3e4",
   "metadata": {},
   "source": [
    "##### applying the post training quantization on the weights(bais and Kernel) stored in a subsubgroup of the HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "93e8a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the HDF5 file\n",
    "with h5py.File('weights-96-5.346523.hdf5', 'r') as file:\n",
    "    # Access the subgroup 'cg_id_0_ffn_dist_dense_0'\n",
    "    subgroup_cg_id_0_ffn_dist_dense_0 = file['cg_id_0']['cg_id_0_ffn_dist_dense_0']\n",
    "    \n",
    "    # Load the weights (bias and kernel)\n",
    "    bias = subgroup_cg_id_0_ffn_dist_dense_0['bias:0'][()]  # Load bias\n",
    "    kernel = subgroup_cg_id_0_ffn_dist_dense_0['kernel:0'][()]  # Load kernel\n",
    "    \n",
    "    # Apply post-training quantization\n",
    "    # Apply post-training quantization to the weights\n",
    "    quantized_bias = tf.quantization.fake_quant_with_min_max_args(\n",
    "        bias, min=-10, max=10, num_bits=8)\n",
    "    quantized_kernel = tf.quantization.fake_quant_with_min_max_args(\n",
    "        kernel, min=-10, max=10, num_bits=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "76866eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sizes\n",
    "original_bias_size = sys.getsizeof(bias)\n",
    "original_kernel_size = sys.getsizeof(kernel)\n",
    "quantized_bias_size = sys.getsizeof(quantized_bias)\n",
    "quantized_kernel_size = sys.getsizeof(quantized_kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "49349817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output with Fake Quantization\n",
      "=====================================\n",
      "Original Bias Size: 368 bytes\n",
      "Original Kernel Size: 65664 bytes\n",
      "Quantized Bias Size: 168 bytes\n",
      "Quantized Kernel Size: 168 bytes\n"
     ]
    }
   ],
   "source": [
    "# Print the sizes\n",
    "print(f\"Output with Fake Quantization\")\n",
    "print(f\"=====================================\")\n",
    "print(f\"Original Bias Size: {original_bias_size} bytes\")\n",
    "print(f\"Original Kernel Size: {original_kernel_size} bytes\")\n",
    "print(f\"Quantized Bias Size: {quantized_bias_size} bytes\")\n",
    "print(f\"Quantized Kernel Size: {quantized_kernel_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c641a",
   "metadata": {},
   "source": [
    "#### Full integer Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aec6f79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output with Full integer Quantization\n",
      "=====================================\n",
      "Original Bias Size: 368 bytes\n",
      "Original Kernel Size: 65664 bytes\n",
      "Quantized Bias Size: 176 bytes\n",
      "Quantized Kernel Size: 16512 bytes\n",
      "Reduction Factor for Bias: 2.090909090909091\n",
      "Reduction Factor for Kernel: 3.9767441860465116\n"
     ]
    }
   ],
   "source": [
    "# Define a function to perform full integer quantization\n",
    "def full_integer_quantization(weights, num_bits=8):\n",
    "    # Determine the range of values\n",
    "    min_val = np.min(weights)\n",
    "    max_val = np.max(weights)\n",
    "    \n",
    "    # Define the quantization range\n",
    "    q_min = 0\n",
    "    q_max = 2**num_bits - 1\n",
    "    \n",
    "    # Scale and quantize the weights\n",
    "    scale = (max_val - min_val) / (q_max - q_min)\n",
    "    zero_point = q_min - min_val / scale\n",
    "    \n",
    "    quantized_weights = np.round(weights / scale + zero_point)\n",
    "    return quantized_weights.astype(np.uint8)\n",
    "\n",
    "\n",
    "# Open the HDF5 file\n",
    "with h5py.File('weights-96-5.346523.hdf5', 'r') as file:\n",
    "    # Access the subgroup 'cg_id_0_ffn_dist_dense_0'\n",
    "    subgroup_cg_id_0_ffn_dist_dense_0 = file['cg_id_0']['cg_id_0_ffn_dist_dense_0']\n",
    "    \n",
    "    # Load the weights (bias and kernel)\n",
    "    bias = subgroup_cg_id_0_ffn_dist_dense_0['bias:0'][()]  # Load bias\n",
    "    kernel = subgroup_cg_id_0_ffn_dist_dense_0['kernel:0'][()]  # Load kernel\n",
    "    \n",
    "    # Apply full integer quantization to the weights\n",
    "    quantized_bias = full_integer_quantization(bias)\n",
    "    quantized_kernel = full_integer_quantization(kernel)\n",
    "    \n",
    "    \n",
    "# Calculate the sizes\n",
    "original_bias_size = sys.getsizeof(bias)\n",
    "original_kernel_size = sys.getsizeof(kernel)\n",
    "quantized_bias_size = sys.getsizeof(quantized_bias)\n",
    "quantized_kernel_size = sys.getsizeof(quantized_kernel)\n",
    "\n",
    "\n",
    "# Print the sizes\n",
    "print(f\"Output with Full integer Quantization\")\n",
    "print(f\"=====================================\")\n",
    "print(f\"Original Bias Size: {original_bias_size} bytes\")\n",
    "print(f\"Original Kernel Size: {original_kernel_size} bytes\")\n",
    "print(f\"Quantized Bias Size: {quantized_bias_size} bytes\")\n",
    "print(f\"Quantized Kernel Size: {quantized_kernel_size} bytes\")\n",
    "\n",
    "# Calculate the reduction factor\n",
    "reduction_factor_bias = original_bias_size / quantized_bias_size\n",
    "reduction_factor_kernel = original_kernel_size / quantized_kernel_size\n",
    "print(f\"Reduction Factor for Bias: {reduction_factor_bias}\")\n",
    "print(f\"Reduction Factor for Kernel: {reduction_factor_kernel}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4d472502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output with Dynamic Range Quantization\n",
      "=====================================\n",
      "Original Bias Size: 368 bytes\n",
      "Original Kernel Size: 65664 bytes\n",
      "Quantized Bias Size: 168 bytes\n",
      "Quantized Kernel Size: 168 bytes\n",
      "Reduction Factor for Bias: 2.1904761904761907\n",
      "Reduction Factor for Kernel: 390.85714285714283\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a function to apply dynamic range quantization\n",
    "def dynamic_range_quantization(weights):\n",
    "    # Define the quantization range dynamically based on the weights\n",
    "    min_val = tf.reduce_min(weights)\n",
    "    max_val = tf.reduce_max(weights)\n",
    "    num_bits = 8  # Define the number of bits for quantization\n",
    "\n",
    "    # Compute the scale and zero point\n",
    "    scale = (max_val - min_val) / ((2 ** num_bits) - 1)\n",
    "    zero_point = tf.round(-min_val / scale)\n",
    "\n",
    "    # Apply quantization and dequantization\n",
    "    quantized_weights = tf.round(weights / scale) - zero_point\n",
    "    dequantized_weights = (quantized_weights + zero_point) * scale\n",
    "\n",
    "    return dequantized_weights\n",
    "\n",
    "# Open the HDF5 file\n",
    "with h5py.File('weights-96-5.346523.hdf5', 'r') as file:\n",
    "    # Access the subgroup 'cg_id_0_ffn_dist_dense_0'\n",
    "    subgroup_cg_id_0_ffn_dist_dense_0 = file['cg_id_0']['cg_id_0_ffn_dist_dense_0']\n",
    "    \n",
    "    # Load the weights (bias and kernel)\n",
    "    bias = subgroup_cg_id_0_ffn_dist_dense_0['bias:0'][()]  # Load bias\n",
    "    kernel = subgroup_cg_id_0_ffn_dist_dense_0['kernel:0'][()]  # Load kernel\n",
    "\n",
    "    # Apply dynamic range quantization to the weights\n",
    "    quantized_bias = dynamic_range_quantization(bias)\n",
    "    quantized_kernel = dynamic_range_quantization(kernel)\n",
    "\n",
    "    \n",
    "# Calculate the sizes\n",
    "original_bias_size = sys.getsizeof(bias)\n",
    "original_kernel_size = sys.getsizeof(kernel)\n",
    "quantized_bias_size = sys.getsizeof(quantized_bias)\n",
    "quantized_kernel_size = sys.getsizeof(quantized_kernel)\n",
    "\n",
    "\n",
    "# Print the sizes\n",
    "print(f\"Output with Dynamic Range Quantization\")\n",
    "print(f\"=====================================\")\n",
    "print(f\"Original Bias Size: {original_bias_size} bytes\")\n",
    "print(f\"Original Kernel Size: {original_kernel_size} bytes\")\n",
    "print(f\"Quantized Bias Size: {quantized_bias_size} bytes\")\n",
    "print(f\"Quantized Kernel Size: {quantized_kernel_size} bytes\")\n",
    "\n",
    "# Calculate the reduction factor\n",
    "reduction_factor_bias = original_bias_size / quantized_bias_size\n",
    "reduction_factor_kernel = original_kernel_size / quantized_kernel_size\n",
    "print(f\"Reduction Factor for Bias: {reduction_factor_bias}\")\n",
    "print(f\"Reduction Factor for Kernel: {reduction_factor_kernel}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "237d22a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output with Float 16 Quantization\n",
      "=====================================\n",
      "Original Bias Size: 368 bytes\n",
      "Original Kernel Size: 65664 bytes\n",
      "Quantized Bias Size: 168 bytes\n",
      "Quantized Kernel Size: 168 bytes\n",
      "Reduction Factor for Bias: 2.1904761904761907\n",
      "Reduction Factor for Kernel: 390.85714285714283\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a function to apply Float16 quantization\n",
    "def float16_quantization(weights):\n",
    "    return tf.dtypes.cast(weights, tf.float16)\n",
    "\n",
    "# Open the HDF5 file\n",
    "with h5py.File('weights-96-5.346523.hdf5', 'r') as file:\n",
    "    # Access the subgroup 'cg_id_0_ffn_dist_dense_0'\n",
    "    subgroup_cg_id_0_ffn_dist_dense_0 = file['cg_id_0']['cg_id_0_ffn_dist_dense_0']\n",
    "    \n",
    "    # Load the weights (bias and kernel)\n",
    "    bias = subgroup_cg_id_0_ffn_dist_dense_0['bias:0'][()]  # Load bias\n",
    "    kernel = subgroup_cg_id_0_ffn_dist_dense_0['kernel:0'][()]  # Load kernel\n",
    "\n",
    "    # Apply Float16 quantization to the weights\n",
    "    float16_bias = float16_quantization(bias)\n",
    "    float16_kernel = float16_quantization(kernel)\n",
    "\n",
    "    \n",
    "# Calculate the sizes\n",
    "original_bias_size = sys.getsizeof(bias)\n",
    "original_kernel_size = sys.getsizeof(kernel)\n",
    "quantized_bias_size = sys.getsizeof(quantized_bias)\n",
    "quantized_kernel_size = sys.getsizeof(quantized_kernel)\n",
    "\n",
    "\n",
    "# Print the sizes\n",
    "print(f\"Output with Float 16 Quantization\")\n",
    "print(f\"=====================================\")\n",
    "print(f\"Original Bias Size: {original_bias_size} bytes\")\n",
    "print(f\"Original Kernel Size: {original_kernel_size} bytes\")\n",
    "print(f\"Quantized Bias Size: {quantized_bias_size} bytes\")\n",
    "print(f\"Quantized Kernel Size: {quantized_kernel_size} bytes\")\n",
    "\n",
    "# Calculate the reduction factor\n",
    "reduction_factor_bias = original_bias_size / quantized_bias_size\n",
    "reduction_factor_kernel = original_kernel_size / quantized_kernel_size\n",
    "print(f\"Reduction Factor for Bias: {reduction_factor_bias}\")\n",
    "print(f\"Reduction Factor for Kernel: {reduction_factor_kernel}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47887c0d",
   "metadata": {},
   "source": [
    "Issuses:\n",
    "1. how the kernel quantization can give 390x in FP16 and Dynamic Range?\n",
    "2. there are a few files inside the ['cg_id_0']['cg_id_0'], do we need to work on those as well?\n",
    "3. The Files are not organised on this notebook, organise it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b9333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46da19f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7ea73e1",
   "metadata": {},
   "source": [
    "## Applying on the whole Subgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99482a5",
   "metadata": {},
   "source": [
    "### `cg_id_0`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb9eb05",
   "metadata": {},
   "source": [
    "#### Dynamic Range Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a6bc2227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Original Size: 66032 bytes\n",
      "Total Quantized Size: 336 bytes\n",
      "Reduction Factor: 196.52380952380952x\n"
     ]
    }
   ],
   "source": [
    "# Define a function to apply dynamic range quantization\n",
    "def dynamic_range_quantization(weights):\n",
    "    # Define the quantization range dynamically based on the weights\n",
    "    min_val = tf.reduce_min(weights)\n",
    "    max_val = tf.reduce_max(weights)\n",
    "    num_bits = 8  # Define the number of bits for quantization\n",
    "\n",
    "    # Compute the scale and zero point\n",
    "    scale = (max_val - min_val) / ((2 ** num_bits) - 1)\n",
    "    zero_point = tf.round(-min_val / scale)\n",
    "\n",
    "    # Apply quantization and dequantization\n",
    "    quantized_weights = tf.round(weights / scale) - zero_point\n",
    "    dequantized_weights = (quantized_weights + zero_point) * scale\n",
    "\n",
    "    return dequantized_weights\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to get the size of an object in bytes\n",
    "def get_size(obj):\n",
    "    return sys.getsizeof(obj)\n",
    "\n",
    "# Open the HDF5 file\n",
    "with h5py.File('weights-96-5.346523.hdf5', 'r') as file:\n",
    "    # Access the subgroup 'cg_id_0_ffn_dist_dense_0'\n",
    "    subgroup_cg_id_0_ffn_dist_dense_0 = file['cg_id_0']['cg_id_0_ffn_dist_dense_0']\n",
    "    \n",
    "    # Load the weights (bias and kernel)\n",
    "    bias = subgroup_cg_id_0_ffn_dist_dense_0['bias:0'][()]  # Load bias\n",
    "    kernel = subgroup_cg_id_0_ffn_dist_dense_0['kernel:0'][()]  # Load kernel\n",
    "\n",
    "    # Apply dynamic range quantization to the weights\n",
    "    quantized_bias = dynamic_range_quantization(bias)\n",
    "    quantized_kernel = dynamic_range_quantization(kernel)\n",
    "\n",
    "    # Calculate original and quantized sizes\n",
    "    original_bias_size = get_size(bias)\n",
    "    original_kernel_size = get_size(kernel)\n",
    "    quantized_bias_size = get_size(quantized_bias)\n",
    "    quantized_kernel_size = get_size(quantized_kernel)\n",
    "    \n",
    "     print(f\"Output with Dynamic Range Quantization\")\n",
    "        print(f\"=====================================\")\n",
    "        print(f\"Subgroup Name: {subgroup_name}\")\n",
    "        print(f\"Original Bias Size: {original_bias_size} bytes\")\n",
    "        print(f\"Original Kernel Size: {original_kernel_size} bytes\")\n",
    "        print(f\"Quantized Bias Size: {quantized_bias_size} bytes\")\n",
    "        print(f\"Quantized Kernel Size: {quantized_kernel_size} bytes\")\n",
    "        print(f\"Reduction Factor for Bias: {original_bias_size / quantized_bias_size}x\")\n",
    "        print(f\"Reduction Factor for Kernel: {original_kernel_size / quantized_kernel_size}x\")\n",
    "        print('======================')\n",
    "        print()\n",
    "        \n",
    "\n",
    "# Calculate total sizes\n",
    "total_original_size = original_bias_size + original_kernel_size\n",
    "total_quantized_size = quantized_bias_size + quantized_kernel_size\n",
    "\n",
    "print(f\"Total Original Size: {total_original_size} bytes\")\n",
    "print(f\"Total Quantized Size: {total_quantized_size} bytes\")\n",
    "print(f\"Reduction Factor: {total_original_size / total_quantized_size}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4315e6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9124e570",
   "metadata": {},
   "source": [
    "#### Float 16 Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "be10b770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output with Float 16 Quantization\n",
      "=====================================\n",
      "Subgroup Name: cg_id_0_ffn_dist_dense_0\n",
      "Original Bias Size: 368 bytes\n",
      "Original Kernel Size: 65664 bytes\n",
      "Quantized Bias Size: 168 bytes\n",
      "Quantized Kernel Size: 168 bytes\n",
      "Reduction Factor for Bias: 2.1904761904761907x\n",
      "Reduction Factor for Kernel: 390.85714285714283x\n",
      "======================\n",
      "\n",
      "Output with Float 16 Quantization\n",
      "=====================================\n",
      "Subgroup Name: cg_id_0_ffn_dist_dense_1\n",
      "Original Bias Size: 368 bytes\n",
      "Original Kernel Size: 16512 bytes\n",
      "Quantized Bias Size: 168 bytes\n",
      "Quantized Kernel Size: 168 bytes\n",
      "Reduction Factor for Bias: 2.1904761904761907x\n",
      "Reduction Factor for Kernel: 98.28571428571429x\n",
      "======================\n",
      "\n",
      "Output with Float 16 Quantization\n",
      "=====================================\n",
      "Subgroup Name: cg_id_0_ffn_dist_dense_2\n",
      "Original Bias Size: 368 bytes\n",
      "Original Kernel Size: 16512 bytes\n",
      "Quantized Bias Size: 168 bytes\n",
      "Quantized Kernel Size: 168 bytes\n",
      "Reduction Factor for Bias: 2.1904761904761907x\n",
      "Reduction Factor for Kernel: 98.28571428571429x\n",
      "======================\n",
      "\n",
      "Output with Float 16 Quantization\n",
      "=====================================\n",
      "Subgroup Name: cg_id_0_ffn_dist_dense_3\n",
      "Original Bias Size: 624 bytes\n",
      "Original Kernel Size: 32896 bytes\n",
      "Quantized Bias Size: 168 bytes\n",
      "Quantized Kernel Size: 168 bytes\n",
      "Reduction Factor for Bias: 3.7142857142857144x\n",
      "Reduction Factor for Kernel: 195.8095238095238x\n",
      "======================\n",
      "\n",
      "Total Original Size: 33520 bytes\n",
      "Total Quantized Size: 336 bytes\n",
      "Reduction Factor: 99.76190476190476x\n"
     ]
    }
   ],
   "source": [
    "# Define a function to apply Float16 quantization\n",
    "def float16_quantization(weights):\n",
    "    return tf.dtypes.cast(weights, tf.float16)\n",
    "\n",
    "# Define a function to get the size of an object in bytes\n",
    "def get_size(obj):\n",
    "    return sys.getsizeof(obj)\n",
    "\n",
    "# Open the HDF5 file\n",
    "with h5py.File('weights-96-5.346523.hdf5', 'r') as file:\n",
    "    # Access the group 'cg_id_0'\n",
    "    group_cg_id_0 = file['cg_id_0']\n",
    "    \n",
    "    # Define a list of subgroup names\n",
    "    subgroup_names = ['cg_id_0_ffn_dist_dense_0', \n",
    "                      'cg_id_0_ffn_dist_dense_1', \n",
    "                      'cg_id_0_ffn_dist_dense_2', \n",
    "                      'cg_id_0_ffn_dist_dense_3']\n",
    "    \n",
    "    for subgroup_name in subgroup_names:\n",
    "        # Access the subgroup\n",
    "        subgroup = group_cg_id_0[subgroup_name]\n",
    "        \n",
    "        # Load the weights (bias and kernel)\n",
    "        bias = subgroup['bias:0'][()]  # Load bias\n",
    "        kernel = subgroup['kernel:0'][()]  # Load kernel\n",
    "\n",
    "        # Apply Float16 quantization to the weights\n",
    "        float16_bias = float16_quantization(bias)\n",
    "        float16_kernel = float16_quantization(kernel)\n",
    "\n",
    "        # Print the sizes and reduction factors\n",
    "        original_bias_size = get_size(bias)\n",
    "        original_kernel_size = get_size(kernel)\n",
    "        quantized_bias_size = get_size(float16_bias)\n",
    "        quantized_kernel_size = get_size(float16_kernel)\n",
    "        \n",
    "        print(f\"Output with Float 16 Quantization\")\n",
    "        print(f\"=====================================\")\n",
    "        print(f\"Subgroup Name: {subgroup_name}\")\n",
    "        print(f\"Original Bias Size: {original_bias_size} bytes\")\n",
    "        print(f\"Original Kernel Size: {original_kernel_size} bytes\")\n",
    "        print(f\"Quantized Bias Size: {quantized_bias_size} bytes\")\n",
    "        print(f\"Quantized Kernel Size: {quantized_kernel_size} bytes\")\n",
    "        print(f\"Reduction Factor for Bias: {original_bias_size / quantized_bias_size}x\")\n",
    "        print(f\"Reduction Factor for Kernel: {original_kernel_size / quantized_kernel_size}x\")\n",
    "        print('======================')\n",
    "        print()\n",
    "        \n",
    "# Calculate total sizes\n",
    "total_original_size = original_bias_size + original_kernel_size\n",
    "total_quantized_size = quantized_bias_size + quantized_kernel_size\n",
    "    \n",
    "print(f\"Total Original Size: {total_original_size} bytes\")\n",
    "print(f\"Total Quantized Size: {total_quantized_size} bytes\")\n",
    "print(f\"Reduction Factor: {total_original_size / total_quantized_size}x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67daab6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ade682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
