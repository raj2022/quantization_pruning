{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2b4f267",
   "metadata": {},
   "source": [
    "# Quantization Recipe\n",
    "source: https://pytorch.org/tutorials/recipes/quantization.html#workflows\n",
    "\n",
    "* Reduced size and faster inference speed with about the same accuracy as the original model.\n",
    "*  Quantization can be applied to both server and mobile model deployment, but it can be especially important or even critical on mobile, because a non-quantized modelâ€™s size may exceed the limit that an iOS or Android app allows for, cause the deployment or OTA update to take too much time, and make the inference too slow for a good user experience.\n",
    "\n",
    "## Introduction\n",
    "* Quantization is a technique that converts 32-bit floating numbers in the model parameters to 8-bit integers. \n",
    "* With quantization, the model size and memory footprint can be reduced to 1/4 of its original size, and the inference can be made about 2-4 times faster, while the accuracy stays about the same.\n",
    "* There are overall three approaches or workflows to quantize a model: \n",
    "    * Post training dynamic quantization\n",
    "    * Post training static quantization\n",
    "    * Quantization aware training\n",
    "* But if the model you want to use already has a quantized version, you can use it directly without going through any of the three workflows above. For example, the torchvision library already includes quantized versions for models MobileNet v2, ResNet 18, ResNet 50, Inception v3, GoogleNet, among others. So we will make the last approach another workflow, albeit a simple one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeb6efb",
   "metadata": {},
   "source": [
    "### Workflows\n",
    " Use one of the four workflows below to quantize a model.\n",
    "\n",
    "\n",
    "#### 1. Use Pretrained Quantized MobileNet v2\n",
    "\n",
    "To get the MobileNet v2 quantizaed model,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ef814db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sraj/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/sraj/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_QuantizedWeights.IMAGENET1K_QNNPACK_V1`. You can also use `weights=MobileNet_V2_QuantizedWeights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/Users/sraj/anaconda3/lib/python3.11/site-packages/torch/ao/quantization/utils.py:339: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n",
      "/Users/sraj/anaconda3/lib/python3.11/site-packages/torch/_utils.py:383: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "model_quantized = torchvision.models.quantization.mobilenet_v2(pretrained=True, quantize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83414a8d",
   "metadata": {},
   "source": [
    "Compare the size difference of a unquantized MobileNet v2 model with its quantized version:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3caa622b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.24 MB\n",
      "3.62 MB\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def print_model_size(mdl):\n",
    "    torch.save(mdl.state_dict(), \"tmp.pt\")\n",
    "    print(\"%.2f MB\" %(os.path.getsize(\"tmp.pt\")/1e6))\n",
    "    os.remove('tmp.pt')\n",
    "    \n",
    "print_model_size(model)\n",
    "print_model_size(model_quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccce5826",
   "metadata": {},
   "source": [
    "#### 2. Post Training Dynamic Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1812fbe5",
   "metadata": {},
   "source": [
    "* Converts all the weights in a model from 32-bit floating numbers to 8-bit integers.\n",
    "* It doesn't convert the activations to int8 **till just before performing the computation on the activations, simply call `torch.quantization.quantize_dynamic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2c7752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dynamic_quantized = torch.quantization.quantize_dynamic(\n",
    "    model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7a0e65",
   "metadata": {},
   "source": [
    "where qconfig_spec specifies the list of submodule names in model to apply quantization to.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716bdccc",
   "metadata": {},
   "source": [
    "The full documentation of the quantize_dynamic API call is [here](https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic). Three other examples of using the post training dynamic quantization are [the Bert example](https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html), an [LSTM model example](https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html#test-dynamic-quantization), and another [demo LSTM example](https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html#do-the-quantization).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ed1d3a",
   "metadata": {},
   "source": [
    "#### 3. Post Training Static Quantization\n",
    "* Methods convert both the weights and the activations to 8-bit  integers beforehand so there won't be on-the fly conversion on activation during the inference, as the dynamic quantization does, hence improving the performance significantly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b14a285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sraj/anaconda3/lib/python3.11/site-packages/torch/ao/quantization/observer.py:1263: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "backend = \"qnnpack\"\n",
    "model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.backends.quantized.engine = backend\n",
    "model_static_quantized = torch.quantization.prepare(model, inplace=False)\n",
    "model_static_quantized = torch.quantization.convert(model_static_quantized, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb997f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.97 MB\n"
     ]
    }
   ],
   "source": [
    "print_model_size(model_static_quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471fca8e",
   "metadata": {},
   "source": [
    "A complete model definition and static quantization example is [here](https://pytorch.org/docs/stable/quantization.html#quantization-api-summary). A dedicated static quantization tutorial is [here](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72607027",
   "metadata": {},
   "source": [
    "#### Quantization Aware Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ec71a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
