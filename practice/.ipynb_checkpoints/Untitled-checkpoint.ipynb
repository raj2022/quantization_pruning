{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fdbbbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "from torch import nn, Tensor\n",
    "import tensorflow_datasets as tfds\n",
    "import torch_geometric\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14c93a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../mlpf/tensorflow_datasets/\"\n",
    "dataset = \"clic_edm_ttbar_pf\"\n",
    "\n",
    "#Load dataset\n",
    "builder = tfds.builder(dataset, data_dir=data_dir)\n",
    "ds_train = builder.as_data_source(split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c60c9d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_FEATURES_TRK = [\n",
    "    \"elemtype\",\n",
    "    \"pt\",\n",
    "    \"eta\",\n",
    "    \"sin_phi\",\n",
    "    \"cos_phi\",\n",
    "    \"p\",\n",
    "    \"chi2\",\n",
    "    \"ndf\",\n",
    "    \"dEdx\",\n",
    "    \"dEdxError\",\n",
    "    \"radiusOfInnermostHit\",\n",
    "    \"tanLambda\",\n",
    "    \"D0\",\n",
    "    \"omega\",\n",
    "    \"Z0\",\n",
    "    \"time\",\n",
    "]\n",
    "X_FEATURES_CL = [\n",
    "    \"elemtype\",\n",
    "    \"et\",\n",
    "    \"eta\",\n",
    "    \"sin_phi\",\n",
    "    \"cos_phi\",\n",
    "    \"energy\",\n",
    "    \"position.x\",\n",
    "    \"position.y\",\n",
    "    \"position.z\",\n",
    "    \"iTheta\",\n",
    "    \"energy_ecal\",\n",
    "    \"energy_hcal\",\n",
    "    \"energy_other\",\n",
    "    \"num_hits\",\n",
    "    \"sigma_x\",\n",
    "    \"sigma_y\",\n",
    "    \"sigma_z\",\n",
    "]\n",
    "Y_FEATURES = [\"cls_id\", \"charge\", \"pt\", \"eta\", \"sin_phi\", \"cos_phi\", \"energy\"]\n",
    "Y_CLASSES = [0, 211, 130, 22, 11, 13]\n",
    "\n",
    "INPUT_DIM = max(len(X_FEATURES_TRK), len(X_FEATURES_CL))\n",
    "NUM_CLASSES = len(Y_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93110933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df6ff4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss, as described in https://arxiv.org/abs/1708.02002.\n",
    "    It is essentially an enhancement to cross entropy loss and is\n",
    "    useful for classification tasks when there is a large class imbalance.\n",
    "    x is expected to contain raw, unnormalized scores for each class.\n",
    "    y is expected to contain class labels.\n",
    "    Shape:\n",
    "        - x: (batch_size, C) or (batch_size, C, d1, d2, ..., dK), K > 0.\n",
    "        - y: (batch_size,) or (batch_size, d1, d2, ..., dK), K > 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, alpha = None, gamma = 0.0, reduction = \"mean\", ignore_index = -100\n",
    "    ):\n",
    "        \"\"\"Constructor.\n",
    "        Args:\n",
    "            alpha (Tensor, optional): Weights for each class. Defaults to None.\n",
    "            gamma (float, optional): A constant, as described in the paper.\n",
    "                Defaults to 0.\n",
    "            reduction (str, optional): 'mean', 'sum' or 'none'.\n",
    "                Defaults to 'mean'.\n",
    "            ignore_index (int, optional): class label to ignore.\n",
    "                Defaults to -100.\n",
    "        \"\"\"\n",
    "        if reduction not in (\"mean\", \"sum\", \"none\"):\n",
    "            raise ValueError('Reduction must be one of: \"mean\", \"sum\", \"none\".')\n",
    "\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "        self.nll_loss = nn.NLLLoss(weight=alpha, reduction=\"none\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        arg_keys = [\"alpha\", \"gamma\", \"reduction\"]\n",
    "        arg_vals = [self.__dict__[k] for k in arg_keys]\n",
    "        arg_strs = [f\"{k}={v!r}\" for k, v in zip(arg_keys, arg_vals)]\n",
    "        arg_str = \", \".join(arg_strs)\n",
    "        return f\"{type(self).__name__}({arg_str})\"\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        if x.ndim > 2:\n",
    "            # (N, C, d1, d2, ..., dK) --> (N * d1 * ... * dK, C)\n",
    "            c = x.shape[1]\n",
    "            x = x.permute(0, *range(2, x.ndim), 1).reshape(-1, c)\n",
    "            # (N, d1, d2, ..., dK) --> (N * d1 * ... * dK,)\n",
    "            y = y.view(-1)\n",
    "\n",
    "        # compute weighted cross entropy term: -alpha * log(pt)\n",
    "        # (alpha is already part of self.nll_loss)\n",
    "        log_p = F.log_softmax(x, dim=-1)\n",
    "        ce = self.nll_loss(log_p, y)\n",
    "\n",
    "        # get true class column from each row\n",
    "        # this is slow due to indexing\n",
    "        # all_rows = torch.arange(len(x))\n",
    "        # log_pt = log_p[all_rows, y]\n",
    "        log_pt = torch.gather(log_p, 1, y.unsqueeze(axis=-1)).squeeze(axis=-1)\n",
    "\n",
    "        # compute focal term: (1 - pt)^gamma\n",
    "        pt = log_pt.exp()\n",
    "        focal_term = (1 - pt) ** self.gamma\n",
    "\n",
    "        # the full loss: -alpha * ((1 - pt)^gamma) * log(pt)\n",
    "        loss = focal_term * ce\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            loss = loss.sum()\n",
    "\n",
    "        return loss\n",
    "\n",
    "class QuantizeFeaturesStub(torch.nn.Module):\n",
    "    def __init__(self, num_feats):\n",
    "        super().__init__()\n",
    "        self.num_feats = num_feats\n",
    "        self.quants = torch.nn.ModuleList()\n",
    "        for ifeat in range(self.num_feats):\n",
    "            self.quants.append(torch.ao.quantization.QuantStub())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.quants[ifeat](x[..., ifeat:ifeat+1]) for ifeat in range(self.num_feats)], axis=-1)\n",
    "        \n",
    "def mlpf_loss(y, ypred):\n",
    "    loss = {}\n",
    "    loss_obj_id = FocalLoss(gamma=2.0, reduction=\"none\")\n",
    "\n",
    "    msk_true_particle = torch.unsqueeze((y[\"cls_id\"] != 0).to(dtype=torch.float32), axis=-1)\n",
    "    npart = y[\"pt\"].numel()\n",
    "\n",
    "    ypred[\"momentum\"] = ypred[\"momentum\"] * msk_true_particle\n",
    "    y[\"momentum\"] = y[\"momentum\"] * msk_true_particle\n",
    "\n",
    "    ypred[\"cls_id_onehot\"] = ypred[\"cls_id_onehot\"].permute((0, 2, 1))\n",
    "\n",
    "    loss_classification = 100 * loss_obj_id(ypred[\"cls_id_onehot\"], y[\"cls_id\"]).reshape(y[\"cls_id\"].shape)\n",
    "    loss_regression = 10 * torch.nn.functional.huber_loss(ypred[\"momentum\"], y[\"momentum\"], reduction=\"none\")\n",
    "    \n",
    "    # average over all particles\n",
    "    loss[\"Classification\"] = loss_classification.sum() / npart\n",
    "    loss[\"Regression\"] = loss_regression.sum() / npart\n",
    "\n",
    "    loss[\"Total\"] = loss[\"Classification\"] + loss[\"Regression\"]\n",
    "    return loss\n",
    "    \n",
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim=128,\n",
    "        num_heads=2,\n",
    "        width=128,\n",
    "        dropout_mha=0.1,\n",
    "        dropout_ff=0.1,\n",
    "        attention_type=\"efficient\",\n",
    "    ):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "\n",
    "        self.attention_type = attention_type\n",
    "        self.act = nn.ELU\n",
    "        self.mha = torch.nn.MultiheadAttention(embedding_dim, num_heads, dropout=dropout_mha, batch_first=True)\n",
    "        self.norm0 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.norm1 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.seq = torch.nn.Sequential(\n",
    "            nn.Linear(embedding_dim, width), self.act(), nn.Linear(width, embedding_dim), self.act()\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(dropout_ff)\n",
    "\n",
    "        self.add0 = torch.ao.nn.quantized.FloatFunctional()\n",
    "        self.add1 = torch.ao.nn.quantized.FloatFunctional()\n",
    "        self.mul = torch.ao.nn.quantized.FloatFunctional()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        mha_out = self.mha(x, x, x, need_weights=False)[0]\n",
    "        x = self.add0.add(x, mha_out)\n",
    "        x = self.norm0(x)\n",
    "        x = self.add1.add(x, self.seq(x))\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.mul.mul(x, mask.unsqueeze(-1))\n",
    "        return x\n",
    "\n",
    "class RegressionOutput(nn.Module):\n",
    "    def __init__(self, embed_dim, width, act, dropout):\n",
    "        super(RegressionOutput, self).__init__()\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "        self.nn = ffn(embed_dim, 1, width, act, dropout)\n",
    "\n",
    "    def forward(self, elems, x, orig_value):\n",
    "        nn_out = self.nn(x)\n",
    "        nn_out = self.dequant(nn_out)\n",
    "        return orig_value + nn_out\n",
    "\n",
    "def ffn(input_dim, output_dim, width, act, dropout):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, width),\n",
    "        act(),\n",
    "        torch.nn.LayerNorm(width),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(width, output_dim),\n",
    "    )\n",
    "\n",
    "def transform_batch(Xbatch):\n",
    "    Xbatch = Xbatch.clone()\n",
    "    Xbatch[..., 1] = torch.log(Xbatch[..., 1])\n",
    "    Xbatch[..., 5] = torch.log(Xbatch[..., 5])\n",
    "    Xbatch[torch.isnan(Xbatch)] = 0.0\n",
    "    Xbatch[torch.isinf(Xbatch)] = 0.0\n",
    "    return Xbatch\n",
    "    \n",
    "def unpack_target(y):\n",
    "    ret = {}\n",
    "    ret[\"cls_id\"] = y[..., 0].long()\n",
    "\n",
    "    for i, feat in enumerate(Y_FEATURES):\n",
    "        if i >= 2:  # skip the cls and charge as they are defined above\n",
    "            ret[feat] = y[..., i].to(dtype=torch.float32)\n",
    "    ret[\"phi\"] = torch.atan2(ret[\"sin_phi\"], ret[\"cos_phi\"])\n",
    "    \n",
    "    # note ~ momentum = [\"pt\", \"eta\", \"sin_phi\", \"cos_phi\", \"energy\"]\n",
    "    ret[\"momentum\"] = y[..., 2:7].to(dtype=torch.float32)\n",
    "    ret[\"p4\"] = torch.cat(\n",
    "        [ret[\"pt\"].unsqueeze(1), ret[\"eta\"].unsqueeze(1), ret[\"phi\"].unsqueeze(1), ret[\"energy\"].unsqueeze(1)], axis=1\n",
    "    )\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def unpack_predictions(preds):\n",
    "    ret = {}\n",
    "    ret[\"cls_id_onehot\"], ret[\"momentum\"] = preds\n",
    "\n",
    "    ret[\"pt\"] = ret[\"momentum\"][..., 0]\n",
    "    ret[\"eta\"] = ret[\"momentum\"][..., 1]\n",
    "    ret[\"sin_phi\"] = ret[\"momentum\"][..., 2]\n",
    "    ret[\"cos_phi\"] = ret[\"momentum\"][..., 3]\n",
    "    ret[\"energy\"] = ret[\"momentum\"][..., 4]\n",
    "\n",
    "    ret[\"cls_id\"] = torch.argmax(ret[\"cls_id_onehot\"], axis=-1)\n",
    "    ret[\"phi\"] = torch.atan2(ret[\"sin_phi\"], ret[\"cos_phi\"])\n",
    "    ret[\"p4\"] = torch.cat(\n",
    "        [\n",
    "            ret[\"pt\"].unsqueeze(axis=-1),\n",
    "            ret[\"eta\"].unsqueeze(axis=-1),\n",
    "            ret[\"phi\"].unsqueeze(axis=-1),\n",
    "            ret[\"energy\"].unsqueeze(axis=-1),\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "    return ret\n",
    "\n",
    "class MLPF(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=16,\n",
    "        num_classes=6,\n",
    "        num_convs=2,\n",
    "        dropout_ff=0.0,\n",
    "        dropout_conv_reg_mha=0.0,\n",
    "        dropout_conv_reg_ff=0.0,\n",
    "        dropout_conv_id_mha=0.0,\n",
    "        dropout_conv_id_ff=0.0,\n",
    "        num_heads=16,\n",
    "        head_dim=16,\n",
    "        elemtypes=[0,1,2],\n",
    "    ):\n",
    "        super(MLPF, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.act = nn.ELU\n",
    "        self.elemtypes = elemtypes\n",
    "        self.num_elemtypes = len(self.elemtypes)\n",
    "\n",
    "        embedding_dim = num_heads * head_dim\n",
    "        width = num_heads * head_dim\n",
    "        \n",
    "        self.nn0_id = ffn(self.input_dim, embedding_dim, width, self.act, dropout_ff)\n",
    "        self.nn0_reg = ffn(self.input_dim, embedding_dim, width, self.act, dropout_ff)\n",
    "        \n",
    "        self.conv_id = nn.ModuleList()\n",
    "        self.conv_reg = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_convs):\n",
    "            self.conv_id.append(\n",
    "                SelfAttentionLayer(\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    width=width,\n",
    "                    dropout_mha=dropout_conv_id_mha,\n",
    "                    dropout_ff=dropout_conv_id_ff,\n",
    "                )\n",
    "            )\n",
    "            self.conv_reg.append(\n",
    "                SelfAttentionLayer(\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    width=width,\n",
    "                    dropout_mha=dropout_conv_reg_mha,\n",
    "                    dropout_ff=dropout_conv_reg_ff,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        decoding_dim = self.input_dim + embedding_dim\n",
    "\n",
    "        # DNN that acts on the node level to predict the PID\n",
    "        self.nn_id = ffn(decoding_dim, num_classes, width, self.act, dropout_ff)\n",
    "\n",
    "        # elementwise DNN for node momentum regression\n",
    "        embed_dim = decoding_dim + num_classes\n",
    "        self.nn_pt = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_eta = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_sin_phi = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_cos_phi = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_energy = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        \n",
    "        self.quant = QuantizeFeaturesStub(self.input_dim + len(self.elemtypes))\n",
    "        self.dequant_id = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, X_features, mask):\n",
    "        Xfeat_transformed = transform_batch(X_features)\n",
    "        Xfeat_normed = self.quant(Xfeat_transformed)\n",
    "\n",
    "        embeddings_id, embeddings_reg = [], []\n",
    "        embedding_id = self.nn0_id(Xfeat_normed)\n",
    "        embedding_reg = self.nn0_reg(Xfeat_normed)\n",
    "        for num, conv in enumerate(self.conv_id):\n",
    "            conv_input = embedding_id if num == 0 else embeddings_id[-1]\n",
    "            out_padded = conv(conv_input, mask)\n",
    "            embeddings_id.append(out_padded)\n",
    "        for num, conv in enumerate(self.conv_reg):\n",
    "            conv_input = embedding_reg if num == 0 else embeddings_reg[-1]\n",
    "            out_padded = conv(conv_input, mask)\n",
    "            embeddings_reg.append(out_padded)\n",
    "\n",
    "        final_embedding_id = torch.cat([Xfeat_normed] + [embeddings_id[-1]], axis=-1)\n",
    "        preds_id = self.nn_id(final_embedding_id)\n",
    "\n",
    "        final_embedding_reg = torch.cat([Xfeat_normed] + [embeddings_reg[-1]] + [preds_id], axis=-1)\n",
    "        preds_pt = self.nn_pt(X_features, final_embedding_reg, X_features[..., 1:2])\n",
    "        preds_eta = self.nn_eta(X_features, final_embedding_reg, X_features[..., 2:3])\n",
    "        preds_sin_phi = self.nn_sin_phi(X_features, final_embedding_reg, X_features[..., 3:4])\n",
    "        preds_cos_phi = self.nn_cos_phi(X_features, final_embedding_reg, X_features[..., 4:5])\n",
    "        preds_energy = self.nn_energy(X_features, final_embedding_reg, X_features[..., 5:6])\n",
    "        preds_momentum = torch.cat([preds_pt, preds_eta, preds_sin_phi, preds_cos_phi, preds_energy], axis=-1)\n",
    "        \n",
    "        preds_id = self.dequant_id(preds_id)\n",
    "        return preds_id, preds_momentum\n",
    "\n",
    "model = MLPF(input_dim=INPUT_DIM, num_classes=NUM_CLASSES)\n",
    "optimizer = torch.optim.Adam(lr=1e-4, params=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23160670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=121.42\n",
      "Loss=111.61\n",
      "Loss=101.22\n",
      "Loss=104.25\n",
      "Loss=95.12\n",
      "Loss=95.04\n",
      "Loss=99.22\n",
      "Loss=93.12\n",
      "Loss=87.64\n",
      "Loss=89.39\n",
      "Loss=91.22\n",
      "Loss=85.96\n",
      "Loss=84.93\n",
      "Loss=97.82\n",
      "Loss=81.12\n",
      "Loss=81.69\n",
      "Loss=74.96\n",
      "Loss=67.72\n",
      "Loss=77.18\n",
      "Loss=80.63\n",
      "Loss=58.58\n",
      "Loss=70.32\n",
      "Loss=62.06\n",
      "Loss=69.05\n",
      "Loss=72.75\n",
      "Loss=68.87\n",
      "Loss=63.52\n",
      "Loss=65.84\n",
      "Loss=60.83\n",
      "Loss=71.01\n",
      "Loss=62.42\n",
      "Loss=52.04\n",
      "Loss=61.76\n",
      "Loss=57.31\n",
      "Loss=56.35\n",
      "Loss=55.22\n",
      "Loss=51.46\n",
      "Loss=59.43\n",
      "Loss=57.48\n",
      "Loss=47.46\n",
      "Loss=58.68\n",
      "Loss=51.30\n",
      "Loss=50.72\n",
      "Loss=43.92\n",
      "Loss=54.27\n",
      "Loss=49.71\n",
      "Loss=54.25\n",
      "Loss=57.08\n",
      "Loss=52.15\n",
      "Loss=49.46\n",
      "Loss=51.25\n",
      "Loss=46.69\n",
      "Loss=36.60\n",
      "Loss=43.56\n",
      "Loss=47.97\n",
      "Loss=44.91\n",
      "Loss=35.85\n",
      "Loss=39.92\n",
      "Loss=45.46\n",
      "Loss=40.15\n",
      "Loss=49.05\n",
      "Loss=43.80\n",
      "Loss=44.11\n",
      "Loss=44.59\n",
      "Loss=43.46\n",
      "Loss=40.41\n",
      "Loss=47.69\n",
      "Loss=41.09\n",
      "Loss=53.59\n",
      "Loss=38.87\n",
      "Loss=45.14\n",
      "Loss=41.35\n",
      "Loss=46.07\n",
      "Loss=40.84\n",
      "Loss=42.02\n",
      "Loss=44.14\n",
      "Loss=39.37\n",
      "Loss=36.60\n",
      "Loss=36.87\n",
      "Loss=34.95\n",
      "Loss=35.67\n",
      "Loss=35.30\n",
      "Loss=35.24\n",
      "Loss=37.51\n",
      "Loss=36.41\n",
      "Loss=37.23\n",
      "Loss=36.06\n",
      "Loss=34.64\n",
      "Loss=36.16\n",
      "Loss=38.82\n",
      "Loss=37.69\n",
      "Loss=32.20\n",
      "Loss=28.28\n",
      "Loss=38.29\n",
      "Loss=34.59\n",
      "Loss=42.56\n",
      "Loss=35.73\n",
      "Loss=33.54\n",
      "Loss=31.90\n",
      "Loss=30.04\n"
     ]
    }
   ],
   "source": [
    "max_events_train = 1000\n",
    "events_per_batch = 10\n",
    "\n",
    "losses = []\n",
    "\n",
    "#Training loop\n",
    "inds_train = range(0,max_events_train,events_per_batch)\n",
    "for ind in inds_train:\n",
    "    optimizer.zero_grad()\n",
    "    ds_elems = [ds_train[i] for i in range(ind,ind+events_per_batch)]\n",
    "    X_features = [torch.tensor(elem[\"X\"]).to(torch.float32) for elem in ds_elems]\n",
    "    X_features_padded = pad_sequence(X_features, batch_first=True)\n",
    "    y_targets = [torch.tensor(elem[\"ygen\"]).to(torch.float32) for elem in ds_elems]\n",
    "    y_targets_padded = pad_sequence(y_targets, batch_first=True)\n",
    "    \n",
    "    mask = X_features_padded[:, :, 0]!=0\n",
    "\n",
    "    preds = model(X_features_padded, mask)\n",
    "    preds_unpacked = unpack_predictions(preds)\n",
    "    targets_unpacked = unpack_target(y_targets_padded)\n",
    "    loss = mlpf_loss(targets_unpacked, preds_unpacked)\n",
    "    loss[\"Total\"].backward()\n",
    "    optimizer.step()\n",
    "    current_loss = loss[\"Total\"].detach().item()\n",
    "    losses.append(current_loss)\n",
    "    print(\"Loss={:.2f}\".format(loss[\"Total\"].detach().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f8e7e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curve\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52afb2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "163fd552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/sft.cern.ch/lcg/views/LCG_105/x86_64-el9-gcc13-opt/lib/python3.9/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/cvmfs/sft.cern.ch/lcg/views/LCG_105/x86_64-el9-gcc13-opt/lib/python3.9/site-packages/torch/ao/quantization/observer.py:1207: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "model_prepared = torch.ao.quantization.prepare(model)\n",
    "\n",
    "#calibrate on data\n",
    "num_events_to_calibrate = 1\n",
    "for ind in range(1000,1000+num_events_to_calibrate):\n",
    "    X = torch.unsqueeze(torch.tensor(ds_train[ind][\"X\"]).to(torch.float32), 0)\n",
    "    mask = X[:, :, 0]!=0\n",
    "    model_prepared(X, mask)\n",
    "\n",
    "model_int8 = torch.ao.quantization.convert(model_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2d42e0",
   "metadata": {},
   "source": [
    "## Training on the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aebc43da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim=128,\n",
    "        num_heads=2,\n",
    "        width=128,\n",
    "        dropout_mha=0.1,\n",
    "        dropout_ff=0.1,\n",
    "        attention_type=\"efficient\",\n",
    "    ):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "\n",
    "        self.attention_type = attention_type\n",
    "        self.act = nn.ELU\n",
    "        self.mha = torch.nn.MultiheadAttention(embedding_dim, num_heads, dropout=dropout_mha, batch_first=True)\n",
    "        self.norm0 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.norm1 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.seq = torch.nn.Sequential(\n",
    "            nn.Linear(embedding_dim, width), self.act(), nn.Linear(width, embedding_dim), self.act()\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(dropout_ff)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        mha_out = self.mha(x, x, x, need_weights=False)[0]\n",
    "        x = x + mha_out\n",
    "        x = self.norm0(x)\n",
    "        x = x + self.seq(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Perform the multiplication\n",
    "        x = x * mask.unsqueeze(-1)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75d6c212",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Mul operands should have same data type.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/sraj/ipykernel_1548229/4074242495.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Quantize mask to int8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mpreds_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_momentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_int8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_features_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlpf_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_targets_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munpack_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_momentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105/x86_64-el9-gcc13-opt/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105/x86_64-el9-gcc13-opt/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/sraj/ipykernel_1548229/1516836056.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X_features, mask)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0mconv_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_id\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0membeddings_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0mout_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0membeddings_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105/x86_64-el9-gcc13-opt/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105/x86_64-el9-gcc13-opt/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/sraj/ipykernel_1548229/1516836056.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105/x86_64-el9-gcc13-opt/lib/python3.9/site-packages/torch/ao/nn/quantized/modules/functional_modules.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;34mr\"\"\"Operation equivalent to ``torch.ops.quantized.mul(Tensor, Tensor)``\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_post_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105/x86_64-el9-gcc13-opt/lib/python3.9/site-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# We save the function ptr as the `op` attribute on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;31m# OpOverloadPacket to access it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Mul operands should have same data type."
     ]
    }
   ],
   "source": [
    "# Training loop with quantized model\n",
    "for ind in inds_train:\n",
    "    optimizer.zero_grad()\n",
    "    ds_elems = [ds_train[i] for i in range(ind, ind + events_per_batch)]\n",
    "    X_features = [torch.tensor(elem[\"X\"]).to(torch.float32) for elem in ds_elems]\n",
    "    X_features_padded = pad_sequence(X_features, batch_first=True)\n",
    "    y_targets = [torch.tensor(elem[\"ygen\"]).to(torch.float32) for elem in ds_elems]\n",
    "    y_targets_padded = pad_sequence(y_targets, batch_first=True)\n",
    "    \n",
    "    mask = X_features_padded[:, :, 0] != 0\n",
    "    mask = mask.to(torch.int8)  # Quantize mask to int8\n",
    "    \n",
    "    preds_id, preds_momentum = model_int8(X_features_padded, mask)\n",
    "\n",
    "    loss = mlpf_loss(unpack_target(y_targets_padded), unpack_predictions((preds_id, preds_momentum)))\n",
    "    loss[\"Total\"].backward()\n",
    "    optimizer.step()\n",
    "    print(\"Loss={:.2f}\".format(loss[\"Total\"].detach().item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75a0edd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_features_padded.dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
