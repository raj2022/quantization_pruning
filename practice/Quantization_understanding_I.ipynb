{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d1fc985",
   "metadata": {},
   "source": [
    "Exmaple to simulate the quantiztion matrix multiplication of Y = XW + b using random matrix X,W, and b.\n",
    "\n",
    "1. https://leimao.github.io/article/Neural-Networks-Quantization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d236cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e61716a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantization(x,s,z,alpha_q, beta_q):\n",
    "    \n",
    "    x_q = np.round(1/s * x +z, decimals=0)\n",
    "    x_q = np.clip(x_q, a_min=alpha_q, a_max=beta_q)\n",
    "    return x_q\n",
    "\n",
    "\n",
    "def quantization_int8(x, s, z):\n",
    "\n",
    "    x_q = quantization(x, s, z, alpha_q=-128, beta_q=127)\n",
    "    x_q = x_q.astype(np.int8)\n",
    "\n",
    "    return x_q\n",
    "\n",
    "def dequantization(x_q, s, z):\n",
    "\n",
    "    # x_q - z might go outside the quantization range.\n",
    "    x_q = x_q.astype(np.int32)\n",
    "    x = s * (x_q - z)\n",
    "    x = x.astype(np.float32)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def generate_quantization_constants(alpha, beta, alpha_q, beta_q):\n",
    "\n",
    "    # Affine quantization mapping\n",
    "    s = (beta - alpha) / (beta_q - alpha_q)\n",
    "    z = int((beta * alpha_q - alpha * beta_q) / (beta - alpha))\n",
    "\n",
    "    return s, z\n",
    "\n",
    "\n",
    "def generate_quantization_int8_constants(alpha, beta):\n",
    "\n",
    "    b = 8\n",
    "    alpha_q = -2**(b - 1)\n",
    "    beta_q = 2**(b - 1) - 1\n",
    "\n",
    "    s, z = generate_quantization_constants(alpha=alpha,\n",
    "                                           beta=beta,\n",
    "                                           alpha_q=alpha_q,\n",
    "                                           beta_q=beta_q)\n",
    "\n",
    "    return s, z\n",
    "\n",
    "\n",
    "\n",
    "def quantization_matrix_multiplication_int8(X_q, W_q, b_q, s_X, z_X, s_W, z_W,\n",
    "                                            s_b, z_b, s_Y, z_Y):\n",
    "\n",
    "    p = W_q.shape[0]\n",
    "\n",
    "    # Y_q_simulated is FP32\n",
    "    Y_q_simulated = (z_Y + (s_b / s_Y * (b_q.astype(np.int32) - z_b)) + (\n",
    "        (s_X * s_W / s_Y) *\n",
    "        (np.matmul(X_q.astype(np.int32), W_q.astype(np.int32)) -\n",
    "         z_W * np.sum(X_q.astype(np.int32), axis=1, keepdims=True) - z_X *\n",
    "         np.sum(W_q.astype(np.int32), axis=0, keepdims=True) + p * z_X * z_W)))\n",
    "\n",
    "    Y_q_simulated = np.round(Y_q_simulated, decimals=0)\n",
    "    Y_q_simulated = np.clip(Y_q_simulated, a_min=-128, a_max=127)\n",
    "    Y_q_simulated = Y_q_simulated.astype(np.int8)\n",
    "\n",
    "    return Y_q_simulated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "193902ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    random_seed = 0\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Random matrices\n",
    "    m = 2\n",
    "    p = 3\n",
    "    n = 4\n",
    "\n",
    "    # X\n",
    "    alpha_X = -100.0\n",
    "    beta_X = 80.0\n",
    "    s_X, z_X = generate_quantization_int8_constants(alpha=alpha_X, beta=beta_X)\n",
    "    X = np.random.uniform(low=alpha_X, high=beta_X,\n",
    "                          size=(m, p)).astype(np.float32)\n",
    "    X_q = quantization_int8(x=X, s=s_X, z=z_X)\n",
    "    X_q_dq = dequantization(x_q=X_q, s=s_X, z=z_X)\n",
    "\n",
    "    # W\n",
    "    alpha_W = -20.0\n",
    "    beta_W = 10.0\n",
    "    s_W, z_W = generate_quantization_int8_constants(alpha=alpha_W, beta=beta_W)\n",
    "    W = np.random.uniform(low=alpha_W, high=beta_W,\n",
    "                          size=(p, n)).astype(np.float32)\n",
    "    W_q = quantization_int8(x=W, s=s_W, z=z_W)\n",
    "    W_q_dq = dequantization(x_q=W_q, s=s_W, z=z_W)\n",
    "\n",
    "    # b\n",
    "    alpha_b = -500.0\n",
    "    beta_b = 500.0\n",
    "    s_b, z_b = generate_quantization_int8_constants(alpha=alpha_b, beta=beta_b)\n",
    "    b = np.random.uniform(low=alpha_b, high=beta_b,\n",
    "                          size=(1, n)).astype(np.float32)\n",
    "    b_q = quantization_int8(x=b, s=s_b, z=z_b)\n",
    "    b_q_dq = dequantization(x_q=b_q, s=s_b, z=z_b)\n",
    "\n",
    "    # Y\n",
    "    alpha_Y = -3000.0\n",
    "    beta_Y = 3000.0\n",
    "    s_Y, z_Y = generate_quantization_int8_constants(alpha=alpha_Y, beta=beta_Y)\n",
    "    Y_expected = np.matmul(X, W) + b\n",
    "    Y_q_expected = quantization_int8(x=Y_expected, s=s_Y, z=z_Y)\n",
    "\n",
    "    Y_expected_prime = np.matmul(X_q_dq, W_q_dq) + b_q_dq\n",
    "    Y_expected_prime_q = quantization_int8(x=Y_expected_prime, s=s_Y, z=z_Y)\n",
    "    Y_expected_prime_q_dq = dequantization(x_q=Y_expected_prime_q,\n",
    "                                           s=s_Y,\n",
    "                                           z=z_Y)\n",
    "\n",
    "    print(\"Expected FP32 Y:\")\n",
    "    print(Y_expected)\n",
    "    print(\"Expected FP32 Y Quantized:\")\n",
    "    print(Y_q_expected)\n",
    "\n",
    "    Y_q_simulated = quantization_matrix_multiplication_int8(X_q=X_q,\n",
    "                                                            W_q=W_q,\n",
    "                                                            b_q=b_q,\n",
    "                                                            s_X=s_X,\n",
    "                                                            z_X=z_X,\n",
    "                                                            s_W=s_W,\n",
    "                                                            z_W=z_W,\n",
    "                                                            s_b=s_b,\n",
    "                                                            z_b=z_b,\n",
    "                                                            s_Y=s_Y,\n",
    "                                                            z_Y=z_Y)\n",
    "    Y_simulated = dequantization(x_q=Y_q_simulated, s=s_Y, z=z_Y)\n",
    "\n",
    "    print(\"Expected Quantized Y_q from Quantized Matrix Multiplication:\")\n",
    "    print(Y_q_simulated)\n",
    "    print(\n",
    "        \"Expected Quantized Y_q from Quantized Matrix Multiplication Dequantized:\"\n",
    "    )\n",
    "    print(Y_simulated)\n",
    "\n",
    "    # Ensure the algorithm implementation is correct\n",
    "    assert (np.array_equal(Y_simulated, Y_expected_prime_q_dq))\n",
    "    assert (np.array_equal(Y_q_simulated, Y_expected_prime_q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51736429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected FP32 Y:\n",
      "[[242.46051  95.31735 217.99707 574.97864]\n",
      " [-88.28122 172.45425 216.39151 212.0112 ]]\n",
      "Expected FP32 Y Quantized:\n",
      "[[10  4  9 24]\n",
      " [-4  7  9  9]]\n",
      "Expected Quantized Y_q from Quantized Matrix Multiplication:\n",
      "[[10  4  9 25]\n",
      " [-4  7  9  9]]\n",
      "Expected Quantized Y_q from Quantized Matrix Multiplication Dequantized:\n",
      "[[235.29411   94.117645 211.76471  588.2353  ]\n",
      " [-94.117645 164.70589  211.76471  211.76471 ]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
