{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "259b8401",
   "metadata": {},
   "source": [
    "\n",
    "Given the notebook, https://github.com/jpata/particleflow/blob/nb_clic_evaluate/notebooks/mlpf-clic-evaluate.ipynb as it loads the model, sets the weights, run the inference on a  set of events and compare the true vs. Predicted value for pT on a smaller subset.\n",
    "\n",
    "*Task:*\n",
    "Do the quantization of the model, and the conversion of the data, all in one notebook along with the model inference. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a1bf01",
   "metadata": {},
   "source": [
    "Load the quantized model, load the quantized weights, and convert all of the data using quantization and then mesaure the inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81940f5a",
   "metadata": {},
   "source": [
    "some point you need to use something like tf.lite.TFLiteConverter?\n",
    "https://www.tensorflow.org/lite/performance/post_training_integer_quant\n",
    "or is there another way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59fe0600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 13:23:37.886265: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83b1516f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-14 13:33:26--  https://huggingface.co/jpata/particleflow/blob/clic_clusters_v1.6/weights-96-5.346523.hdf5\n",
      "Resolving huggingface.co (huggingface.co)... 2600:9000:24bf:7c00:17:b174:6d00:93a1, 2600:9000:24bf:7200:17:b174:6d00:93a1, 2600:9000:24bf:8a00:17:b174:6d00:93a1, ...\n",
      "Connecting to huggingface.co (huggingface.co)|2600:9000:24bf:7c00:17:b174:6d00:93a1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 40417 (39K) [text/html]\n",
      "Saving to: ‘weights-96-5.346523.hdf5’\n",
      "\n",
      "100%[======================================>] 40,417      --.-K/s   in 0.001s  \n",
      "\n",
      "2023-11-14 13:33:30 (36.3 MB/s) - ‘weights-96-5.346523.hdf5’ saved [40417/40417]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/jpata/particleflow/blob/clic_clusters_v1.6/weights-96-5.346523.hdf5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47659ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:horovod not found, ignoring\n",
      "WARNING:root:horovod not found, ignoring\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path += [\"../../../particleflow/mlpf/\"]\n",
    "from tfmodel.model_setup import make_model\n",
    "from tfmodel.utils import parse_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e4b05d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, _ = parse_config(\"../../../particleflow/parameters/clic.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b19e9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 13:27:57.934874: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-14 13:27:57.940916: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "model = make_model(config, tf.float32)\n",
    "model.build((1, None, config[\"dataset\"][\"num_input_features\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a179b4f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"pf_net_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " node_encoding (Sequential)  (1, None, 256)            71462     \n",
      "                                                                 \n",
      " input_encoding_clic (InputE  multiple                 0         \n",
      " ncodingCLIC)                                                    \n",
      "                                                                 \n",
      " cg_id_0 (CombinedGraphLayer  multiple                 441024    \n",
      " )                                                               \n",
      "                                                                 \n",
      " cg_id_1 (CombinedGraphLayer  multiple                 441024    \n",
      " )                                                               \n",
      "                                                                 \n",
      " cg_id_2 (CombinedGraphLayer  multiple                 441024    \n",
      " )                                                               \n",
      "                                                                 \n",
      " cg_id_3 (CombinedGraphLayer  multiple                 441024    \n",
      " )                                                               \n",
      "                                                                 \n",
      " cg_id_4 (CombinedGraphLayer  multiple                 441024    \n",
      " )                                                               \n",
      "                                                                 \n",
      " cg_id_5 (CombinedGraphLayer  multiple                 441024    \n",
      " )                                                               \n",
      "                                                                 \n",
      " cg_reg_0 (CombinedGraphLaye  multiple                 441024    \n",
      " r)                                                              \n",
      "                                                                 \n",
      " cg_reg_1 (CombinedGraphLaye  multiple                 441024    \n",
      " r)                                                              \n",
      "                                                                 \n",
      " cg_reg_2 (CombinedGraphLaye  multiple                 441024    \n",
      " r)                                                              \n",
      "                                                                 \n",
      " cg_reg_3 (CombinedGraphLaye  multiple                 441024    \n",
      " r)                                                              \n",
      "                                                                 \n",
      " cg_reg_4 (CombinedGraphLaye  multiple                 441024    \n",
      " r)                                                              \n",
      "                                                                 \n",
      " cg_reg_5 (CombinedGraphLaye  multiple                 441024    \n",
      " r)                                                              \n",
      "                                                                 \n",
      " output_decoding (OutputDeco  multiple                 404284    \n",
      " ding)                                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,768,034\n",
      "Trainable params: 5,614,434\n",
      "Non-trainable params: 153,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "331f3cee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Weight count mismatch for layer #1 (named node_encoding). Layer expects 8 weight(s). Received 4 saved weight(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11273/2270990826.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../weights-96-5.346523.hdf5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/nobackup/miniconda3/envs/envym/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nobackup/miniconda3/envs/envym/lib/python3.7/site-packages/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group_by_name\u001b[0;34m(f, model, skip_mismatch)\u001b[0m\n\u001b[1;32m    922\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                 raise ValueError(\n\u001b[0;32m--> 924\u001b[0;31m                     \u001b[0;34mf\"Weight count mismatch for layer #{k} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m                     \u001b[0;34mf\"(named {layer.name}). \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m                     \u001b[0;34mf\"Layer expects {len(symbolic_weights)} weight(s). \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Weight count mismatch for layer #1 (named node_encoding). Layer expects 8 weight(s). Received 4 saved weight(s)"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"../weights-96-5.346523.hdf5\", skip_mismatch=False, by_name=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c896d51b",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset clic_edm_qq_pf not found.\nAvailable datasets:\n\t- abstract_reasoning\n\t- accentdb\n\t- aeslc\n\t- aflw2k3d\n\t- ag_news_subset\n\t- ai2_arc\n\t- ai2_arc_with_ir\n\t- amazon_us_reviews\n\t- anli\n\t- answer_equivalence\n\t- arc\n\t- asqa\n\t- asset\n\t- assin2\n\t- bair_robot_pushing_small\n\t- bccd\n\t- beans\n\t- bee_dataset\n\t- beir\n\t- big_patent\n\t- bigearthnet\n\t- billsum\n\t- binarized_mnist\n\t- binary_alpha_digits\n\t- ble_wind_field\n\t- blimp\n\t- booksum\n\t- bool_q\n\t- bucc\n\t- c4\n\t- c4_wsrs\n\t- caltech101\n\t- caltech_birds2010\n\t- caltech_birds2011\n\t- cardiotox\n\t- cars196\n\t- cassava\n\t- cats_vs_dogs\n\t- celeb_a\n\t- celeb_a_hq\n\t- cfq\n\t- cherry_blossoms\n\t- chexpert\n\t- cifar10\n\t- cifar100\n\t- cifar100_n\n\t- cifar10_1\n\t- cifar10_corrupted\n\t- cifar10_n\n\t- citrus_leaves\n\t- cityscapes\n\t- civil_comments\n\t- clevr\n\t- clic\n\t- clinc_oos\n\t- cmaterdb\n\t- cnn_dailymail\n\t- coco\n\t- coco_captions\n\t- coil100\n\t- colorectal_histology\n\t- colorectal_histology_large\n\t- common_voice\n\t- conll2002\n\t- conll2003\n\t- controlled_noisy_web_labels\n\t- coqa\n\t- cos_e\n\t- cosmos_qa\n\t- covid19\n\t- covid19sum\n\t- crema_d\n\t- criteo\n\t- cs_restaurants\n\t- curated_breast_imaging_ddsm\n\t- cycle_gan\n\t- d4rl_adroit_door\n\t- d4rl_adroit_hammer\n\t- d4rl_adroit_pen\n\t- d4rl_adroit_relocate\n\t- d4rl_antmaze\n\t- d4rl_mujoco_ant\n\t- d4rl_mujoco_halfcheetah\n\t- d4rl_mujoco_hopper\n\t- d4rl_mujoco_walker2d\n\t- dart\n\t- davis\n\t- deep1b\n\t- deep_weeds\n\t- definite_pronoun_resolution\n\t- dementiabank\n\t- diabetic_retinopathy_detection\n\t- diamonds\n\t- div2k\n\t- dmlab\n\t- doc_nli\n\t- dolphin_number_word\n\t- domainnet\n\t- downsampled_imagenet\n\t- drop\n\t- dsprites\n\t- dtd\n\t- duke_ultrasound\n\t- e2e_cleaned\n\t- efron_morris75\n\t- emnist\n\t- eraser_multi_rc\n\t- esnli\n\t- eurosat\n\t- fashion_mnist\n\t- flic\n\t- flores\n\t- food101\n\t- forest_fires\n\t- fuss\n\t- gap\n\t- geirhos_conflict_stimuli\n\t- gem\n\t- genomics_ood\n\t- german_credit_numeric\n\t- gigaword\n\t- glove100_angular\n\t- glue\n\t- goemotions\n\t- gov_report\n\t- gpt3\n\t- gref\n\t- groove\n\t- grounded_scan\n\t- gsm8k\n\t- gtzan\n\t- gtzan_music_speech\n\t- hellaswag\n\t- higgs\n\t- hillstrom\n\t- horses_or_humans\n\t- howell\n\t- i_naturalist2017\n\t- i_naturalist2018\n\t- i_naturalist2021\n\t- imagenet2012\n\t- imagenet2012_corrupted\n\t- imagenet2012_fewshot\n\t- imagenet2012_multilabel\n\t- imagenet2012_real\n\t- imagenet2012_subset\n\t- imagenet_a\n\t- imagenet_lt\n\t- imagenet_r\n\t- imagenet_resized\n\t- imagenet_sketch\n\t- imagenet_v2\n\t- imagenette\n\t- imagewang\n\t- imdb_reviews\n\t- irc_disentanglement\n\t- iris\n\t- istella\n\t- kddcup99\n\t- kitti\n\t- kmnist\n\t- laion400m\n\t- lambada\n\t- lfw\n\t- librispeech\n\t- librispeech_lm\n\t- libritts\n\t- ljspeech\n\t- lm1b\n\t- locomotion\n\t- lost_and_found\n\t- lsun\n\t- lvis\n\t- malaria\n\t- math_dataset\n\t- math_qa\n\t- mctaco\n\t- media_sum\n\t- mlqa\n\t- mnist\n\t- mnist_corrupted\n\t- movie_lens\n\t- movie_rationales\n\t- movielens\n\t- moving_mnist\n\t- mrqa\n\t- mslr_web\n\t- mt_opt\n\t- mtnt\n\t- multi_news\n\t- multi_nli\n\t- multi_nli_mismatch\n\t- natural_instructions\n\t- natural_questions\n\t- natural_questions_open\n\t- newsroom\n\t- nsynth\n\t- nyu_depth_v2\n\t- ogbg_molpcba\n\t- omniglot\n\t- open_images_challenge2019_detection\n\t- open_images_v4\n\t- openbookqa\n\t- opinion_abstracts\n\t- opinosis\n\t- opus\n\t- oxford_flowers102\n\t- oxford_iiit_pet\n\t- para_crawl\n\t- pass\n\t- patch_camelyon\n\t- paws_wiki\n\t- paws_x_wiki\n\t- penguins\n\t- pet_finder\n\t- pg19\n\t- piqa\n\t- places365_small\n\t- placesfull\n\t- plant_leaves\n\t- plant_village\n\t- plantae_k\n\t- protein_net\n\t- q_re_cc\n\t- qa4mre\n\t- qasc\n\t- quac\n\t- quality\n\t- quickdraw_bitmap\n\t- race\n\t- radon\n\t- reddit\n\t- reddit_disentanglement\n\t- reddit_tifu\n\t- ref_coco\n\t- resisc45\n\t- rlu_atari\n\t- rlu_atari_checkpoints\n\t- rlu_atari_checkpoints_ordered\n\t- rlu_control_suite\n\t- rlu_dmlab_explore_object_rewards_few\n\t- rlu_dmlab_explore_object_rewards_many\n\t- rlu_dmlab_rooms_select_nonmatching_object\n\t- rlu_dmlab_rooms_watermaze\n\t- rlu_dmlab_seekavoid_arena01\n\t- rlu_locomotion\n\t- rlu_rwrl\n\t- robomimic_ph\n\t- robonet\n\t- robosuite_panda_pick_place_can\n\t- rock_paper_scissors\n\t- rock_you\n\t- s3o4d\n\t- salient_span_wikipedia\n\t- samsum\n\t- savee\n\t- scan\n\t- scene_parse150\n\t- schema_guided_dialogue\n\t- sci_tail\n\t- scicite\n\t- scientific_papers\n\t- scrolls\n\t- sentiment140\n\t- shapes3d\n\t- sift1m\n\t- simpte\n\t- siscore\n\t- smallnorb\n\t- smartwatch_gestures\n\t- snli\n\t- so2sat\n\t- speech_commands\n\t- spoken_digit\n\t- squad\n\t- squad_question_generation\n\t- stanford_dogs\n\t- stanford_online_products\n\t- star_cfq\n\t- starcraft_video\n\t- stl10\n\t- story_cloze\n\t- summscreen\n\t- sun397\n\t- super_glue\n\t- svhn_cropped\n\t- symmetric_solids\n\t- tao\n\t- tatoeba\n\t- ted_hrlr_translate\n\t- ted_multi_translate\n\t- tedlium\n\t- tf_flowers\n\t- the300w_lp\n\t- tiny_shakespeare\n\t- titanic\n\t- trec\n\t- trivia_qa\n\t- tydi_qa\n\t- uc_merced\n\t- ucf101\n\t- unified_qa\n\t- universal_dependencies\n\t- unnatural_instructions\n\t- user_libri_audio\n\t- user_libri_text\n\t- vctk\n\t- visual_domain_decathlon\n\t- voc\n\t- voxceleb\n\t- voxforge\n\t- waymo_open_dataset\n\t- web_graph\n\t- web_nlg\n\t- web_questions\n\t- wider_face\n\t- wiki40b\n\t- wiki_auto\n\t- wiki_bio\n\t- wiki_dialog\n\t- wiki_table_questions\n\t- wiki_table_text\n\t- wikiann\n\t- wikihow\n\t- wikipedia\n\t- wikipedia_toxicity_subtypes\n\t- wine_quality\n\t- winogrande\n\t- wit\n\t- wit_kaggle\n\t- wmt13_translate\n\t- wmt14_translate\n\t- wmt15_translate\n\t- wmt16_translate\n\t- wmt17_translate\n\t- wmt18_translate\n\t- wmt19_translate\n\t- wmt_t2t_translate\n\t- wmt_translate\n\t- wordnet\n\t- wsc273\n\t- xnli\n\t- xquad\n\t- xsum\n\t- xtreme_pawsx\n\t- xtreme_pos\n\t- xtreme_s\n\t- xtreme_xnli\n\t- yahoo_ltrc\n\t- yelp_polarity_reviews\n\t- yes_no\n\t- youtube_vis\n\nCheck that:\n    - if dataset was added recently, it may only be available\n      in `tfds-nightly`\n    - the dataset name is spelled correctly\n    - dataset class defines all base class abstract methods\n    - the module defining the dataset class is imported\n\nThe builder directory ../../../tensorflow_datasets/clic_edm_qq_pf doesn't contain any versions.\nNo builder could be found in the directory: ../../../tensorflow_datasets/ for the builder: clic_edm_qq_pf.\nNo registered data_dirs were found in:\n\t- ../../../tensorflow_datasets/\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19700/3610595399.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Reading the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mds_builder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"clic_edm_qq_pf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../../../tensorflow_datasets/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_data_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nobackup/miniconda3/envs/envym/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nobackup/miniconda3/envs/envym/lib/python3.7/site-packages/tensorflow_datasets/core/logging/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m       \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nobackup/miniconda3/envs/envym/lib/python3.7/site-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mbuilder\u001b[0;34m(name, try_gcs, **builder_kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m   \u001b[0;31m# If neither the code nor the files are found, raise DatasetNotFoundError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mnot_found_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nobackup/miniconda3/envs/envym/lib/python3.7/site-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mbuilder\u001b[0;34m(name, try_gcs, **builder_kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m   \u001b[0;31m# First check whether we can find the corresponding dataset builder code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mregistered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Class not found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nobackup/miniconda3/envs/envym/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nobackup/miniconda3/envs/envym/lib/python3.7/site-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mbuilder_cls\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m       \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimported_builder_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m       \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nobackup/miniconda3/envs/envym/lib/python3.7/site-packages/tensorflow_datasets/core/registered.py\u001b[0m in \u001b[0;36mimported_builder_cls\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_DATASET_REGISTRY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Dataset {name} not found.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m   \u001b[0mbuilder_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_DATASET_REGISTRY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset clic_edm_qq_pf not found.\nAvailable datasets:\n\t- abstract_reasoning\n\t- accentdb\n\t- aeslc\n\t- aflw2k3d\n\t- ag_news_subset\n\t- ai2_arc\n\t- ai2_arc_with_ir\n\t- amazon_us_reviews\n\t- anli\n\t- answer_equivalence\n\t- arc\n\t- asqa\n\t- asset\n\t- assin2\n\t- bair_robot_pushing_small\n\t- bccd\n\t- beans\n\t- bee_dataset\n\t- beir\n\t- big_patent\n\t- bigearthnet\n\t- billsum\n\t- binarized_mnist\n\t- binary_alpha_digits\n\t- ble_wind_field\n\t- blimp\n\t- booksum\n\t- bool_q\n\t- bucc\n\t- c4\n\t- c4_wsrs\n\t- caltech101\n\t- caltech_birds2010\n\t- caltech_birds2011\n\t- cardiotox\n\t- cars196\n\t- cassava\n\t- cats_vs_dogs\n\t- celeb_a\n\t- celeb_a_hq\n\t- cfq\n\t- cherry_blossoms\n\t- chexpert\n\t- cifar10\n\t- cifar100\n\t- cifar100_n\n\t- cifar10_1\n\t- cifar10_corrupted\n\t- cifar10_n\n\t- citrus_leaves\n\t- cityscapes\n\t- civil_comments\n\t- clevr\n\t- clic\n\t- clinc_oos\n\t- cmaterdb\n\t- cnn_dailymail\n\t- coco\n\t- coco_captions\n\t- coil100\n\t- colorectal_histology\n\t- colorectal_histology_large\n\t- common_voice\n\t- conll2002\n\t- conll2003\n\t- controlled_noisy_web_labels\n\t- coqa\n\t- cos_e\n\t- cosmos_qa\n\t- covid19\n\t- covid19sum\n\t- crema_d\n\t- criteo\n\t- cs_restaurants\n\t- curated_breast_imaging_ddsm\n\t- cycle_gan\n\t- d4rl_adroit_door\n\t- d4rl_adroit_hammer\n\t- d4rl_adroit_pen\n\t- d4rl_adroit_relocate\n\t- d4rl_antmaze\n\t- d4rl_mujoco_ant\n\t- d4rl_mujoco_halfcheetah\n\t- d4rl_mujoco_hopper\n\t- d4rl_mujoco_walker2d\n\t- dart\n\t- davis\n\t- deep1b\n\t- deep_weeds\n\t- definite_pronoun_resolution\n\t- dementiabank\n\t- diabetic_retinopathy_detection\n\t- diamonds\n\t- div2k\n\t- dmlab\n\t- doc_nli\n\t- dolphin_number_word\n\t- domainnet\n\t- downsampled_imagenet\n\t- drop\n\t- dsprites\n\t- dtd\n\t- duke_ultrasound\n\t- e2e_cleaned\n\t- efron_morris75\n\t- emnist\n\t- eraser_multi_rc\n\t- esnli\n\t- eurosat\n\t- fashion_mnist\n\t- flic\n\t- flores\n\t- food101\n\t- forest_fires\n\t- fuss\n\t- gap\n\t- geirhos_conflict_stimuli\n\t- gem\n\t- genomics_ood\n\t- german_credit_numeric\n\t- gigaword\n\t- glove100_angular\n\t- glue\n\t- goemotions\n\t- gov_report\n\t- gpt3\n\t- gref\n\t- groove\n\t- grounded_scan\n\t- gsm8k\n\t- gtzan\n\t- gtzan_music_speech\n\t- hellaswag\n\t- higgs\n\t- hillstrom\n\t- horses_or_humans\n\t- howell\n\t- i_naturalist2017\n\t- i_naturalist2018\n\t- i_naturalist2021\n\t- imagenet2012\n\t- imagenet2012_corrupted\n\t- imagenet2012_fewshot\n\t- imagenet2012_multilabel\n\t- imagenet2012_real\n\t- imagenet2012_subset\n\t- imagenet_a\n\t- imagenet_lt\n\t- imagenet_r\n\t- imagenet_resized\n\t- imagenet_sketch\n\t- imagenet_v2\n\t- imagenette\n\t- imagewang\n\t- imdb_reviews\n\t- irc_disentanglement\n\t- iris\n\t- istella\n\t- kddcup99\n\t- kitti\n\t- kmnist\n\t- laion400m\n\t- lambada\n\t- lfw\n\t- librispeech\n\t- librispeech_lm\n\t- libritts\n\t- ljspeech\n\t- lm1b\n\t- locomotion\n\t- lost_and_found\n\t- lsun\n\t- lvis\n\t- malaria\n\t- math_dataset\n\t- math_qa\n\t- mctaco\n\t- media_sum\n\t- mlqa\n\t- mnist\n\t- mnist_corrupted\n\t- movie_lens\n\t- movie_rationales\n\t- movielens\n\t- moving_mnist\n\t- mrqa\n\t- mslr_web\n\t- mt_opt\n\t- mtnt\n\t- multi_news\n\t- multi_nli\n\t- multi_nli_mismatch\n\t- natural_instructions\n\t- natural_questions\n\t- natural_questions_open\n\t- newsroom\n\t- nsynth\n\t- nyu_depth_v2\n\t- ogbg_molpcba\n\t- omniglot\n\t- open_images_challenge2019_detection\n\t- open_images_v4\n\t- openbookqa\n\t- opinion_abstracts\n\t- opinosis\n\t- opus\n\t- oxford_flowers102\n\t- oxford_iiit_pet\n\t- para_crawl\n\t- pass\n\t- patch_camelyon\n\t- paws_wiki\n\t- paws_x_wiki\n\t- penguins\n\t- pet_finder\n\t- pg19\n\t- piqa\n\t- places365_small\n\t- placesfull\n\t- plant_leaves\n\t- plant_village\n\t- plantae_k\n\t- protein_net\n\t- q_re_cc\n\t- qa4mre\n\t- qasc\n\t- quac\n\t- quality\n\t- quickdraw_bitmap\n\t- race\n\t- radon\n\t- reddit\n\t- reddit_disentanglement\n\t- reddit_tifu\n\t- ref_coco\n\t- resisc45\n\t- rlu_atari\n\t- rlu_atari_checkpoints\n\t- rlu_atari_checkpoints_ordered\n\t- rlu_control_suite\n\t- rlu_dmlab_explore_object_rewards_few\n\t- rlu_dmlab_explore_object_rewards_many\n\t- rlu_dmlab_rooms_select_nonmatching_object\n\t- rlu_dmlab_rooms_watermaze\n\t- rlu_dmlab_seekavoid_arena01\n\t- rlu_locomotion\n\t- rlu_rwrl\n\t- robomimic_ph\n\t- robonet\n\t- robosuite_panda_pick_place_can\n\t- rock_paper_scissors\n\t- rock_you\n\t- s3o4d\n\t- salient_span_wikipedia\n\t- samsum\n\t- savee\n\t- scan\n\t- scene_parse150\n\t- schema_guided_dialogue\n\t- sci_tail\n\t- scicite\n\t- scientific_papers\n\t- scrolls\n\t- sentiment140\n\t- shapes3d\n\t- sift1m\n\t- simpte\n\t- siscore\n\t- smallnorb\n\t- smartwatch_gestures\n\t- snli\n\t- so2sat\n\t- speech_commands\n\t- spoken_digit\n\t- squad\n\t- squad_question_generation\n\t- stanford_dogs\n\t- stanford_online_products\n\t- star_cfq\n\t- starcraft_video\n\t- stl10\n\t- story_cloze\n\t- summscreen\n\t- sun397\n\t- super_glue\n\t- svhn_cropped\n\t- symmetric_solids\n\t- tao\n\t- tatoeba\n\t- ted_hrlr_translate\n\t- ted_multi_translate\n\t- tedlium\n\t- tf_flowers\n\t- the300w_lp\n\t- tiny_shakespeare\n\t- titanic\n\t- trec\n\t- trivia_qa\n\t- tydi_qa\n\t- uc_merced\n\t- ucf101\n\t- unified_qa\n\t- universal_dependencies\n\t- unnatural_instructions\n\t- user_libri_audio\n\t- user_libri_text\n\t- vctk\n\t- visual_domain_decathlon\n\t- voc\n\t- voxceleb\n\t- voxforge\n\t- waymo_open_dataset\n\t- web_graph\n\t- web_nlg\n\t- web_questions\n\t- wider_face\n\t- wiki40b\n\t- wiki_auto\n\t- wiki_bio\n\t- wiki_dialog\n\t- wiki_table_questions\n\t- wiki_table_text\n\t- wikiann\n\t- wikihow\n\t- wikipedia\n\t- wikipedia_toxicity_subtypes\n\t- wine_quality\n\t- winogrande\n\t- wit\n\t- wit_kaggle\n\t- wmt13_translate\n\t- wmt14_translate\n\t- wmt15_translate\n\t- wmt16_translate\n\t- wmt17_translate\n\t- wmt18_translate\n\t- wmt19_translate\n\t- wmt_t2t_translate\n\t- wmt_translate\n\t- wordnet\n\t- wsc273\n\t- xnli\n\t- xquad\n\t- xsum\n\t- xtreme_pawsx\n\t- xtreme_pos\n\t- xtreme_s\n\t- xtreme_xnli\n\t- yahoo_ltrc\n\t- yelp_polarity_reviews\n\t- yes_no\n\t- youtube_vis\n\nCheck that:\n    - if dataset was added recently, it may only be available\n      in `tfds-nightly`\n    - the dataset name is spelled correctly\n    - dataset class defines all base class abstract methods\n    - the module defining the dataset class is imported\n\nThe builder directory ../../../tensorflow_datasets/clic_edm_qq_pf doesn't contain any versions.\nNo builder could be found in the directory: ../../../tensorflow_datasets/ for the builder: clic_edm_qq_pf.\nNo registered data_dirs were found in:\n\t- ../../../tensorflow_datasets/\n"
     ]
    }
   ],
   "source": [
    "## Reading the dataset\n",
    "ds_builder = tfds.builder(\"clic_edm_qq_pf\", data_dir = '../../../tensorflow_datasets/')\n",
    "dss = ds_builder.as_data_source(\"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221dd7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Check the error:\n",
    "    1. Read data directly from the lxplus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d5e4dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_from_ds():\n",
    "    for elem in dss:\n",
    "        yield {\"X\": elem[\"X\"], \"ygen\": elem[\"ygen\"], \"ycand\": elem[\"ycand\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1479b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_signature = {k: tf.TensorSpec(shape=(None, v.shape[1])) for (k, v) in dss.dataset_info.features.items()}\n",
    "tf_dataset = tf.data.Dataset.from_generator(yield_from_ds, output_signature=output_signature).take(100).padded_batch(batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fd80d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(tfds.as_numpy(tf_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b4acdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = [d[\"X\"] for d in data]\n",
    "ys = [d[\"ygen\"] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "689eef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pts = []\n",
    "pred_pts = []\n",
    "\n",
    "for ibatch in range(len(Xs)):\n",
    "    ret = model(Xs[ibatch])\n",
    "\n",
    "    mask_true_particles = ys[ibatch][..., 0]!=0\n",
    "    \n",
    "    true_pt = ys[ibatch][mask_true_particles, 2]\n",
    "    pred_pt = ret[\"pt\"][mask_true_particles][..., 0].numpy()\n",
    "\n",
    "    true_pts.append(true_pt)\n",
    "    pred_pts.append(pred_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bcd5ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pt = np.concatenate(true_pts)\n",
    "pred_pt = np.concatenate(pred_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "716b68b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(pred_pt/true_pt, bins=np.linspace(0,3,100));\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c32a2ee",
   "metadata": {},
   "source": [
    "## Quantized data into the unquantized model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4817027",
   "metadata": {},
   "source": [
    "Before feeding the input data to the model, we need to quantize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63219f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize input data\n",
    "Xs_quantized = [np.round(X * 127).astype(np.int8) for X in Xs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a63aed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_quantized = [np.round(X * 127).astype(np.int8) for X in Xs]\n",
    "Xs_tf = [tf.convert_to_tensor(X_q, dtype=tf.float32) for X_q in Xs_quantized]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ab11cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_quantized = [np.round(y * 127).astype(np.int8) for y in ys]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "744809e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pts = []\n",
    "pred_pts = []\n",
    "\n",
    "for ibatch in range(len(Xs)):\n",
    "    ret = model(Xs_tf[ibatch])\n",
    "\n",
    "    mask_true_particles = ys_quantized[ibatch][..., 0]!=0\n",
    "    \n",
    "    true_pt = ys[ibatch][mask_true_particles, 2]\n",
    "    pred_pt = ret[\"pt\"][mask_true_particles][..., 0].numpy()\n",
    "\n",
    "    true_pts.append(true_pt)\n",
    "    pred_pts.append(pred_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8840038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pt = np.concatenate(true_pts)\n",
    "pred_pt = np.concatenate(pred_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1730822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(pred_pt/true_pt, bins=np.linspace(0,3,100));\n",
    "plt.title(\"After INT8 Quantization\")\n",
    "plt.yscale(\"log\")\n",
    "plt.savefig('Output_after_INT8_quantization.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63be5752",
   "metadata": {},
   "source": [
    "Above one is the output with the quantized dataset but not quantized model. We were wokring with the Dynamic Range Quantization. \n",
    "Dynamic range quantization is a recommended starting point because it provides reduced memory usage and faster computation without you having to provide a representative dataset for calibration. This type of quantization, statically quantizes only the weights from floating point to integer at conversion time, which provides 8-bits of precision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5391ebdd",
   "metadata": {},
   "source": [
    "## Quantized Model with the Quantized Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77582ebf",
   "metadata": {},
   "source": [
    "Let's try with **[quantized model](https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization_of_weights_and_activations)**, Post-training qunatization method, Dynamic range quantization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f439ec",
   "metadata": {},
   "source": [
    "## Dynamic Range Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b108b047",
   "metadata": {},
   "source": [
    "Converting the model using dynamic range quantization(https://www.tensorflow.org/lite/performance/post_training_integer_quant):\n",
    "Let's enable the default optimizations flag to quantize all fixed parameters (such as weights):\n",
    "The model is now a bit smaller with quantized weights, but other variable data is still in float format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3790f748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-07 16:12:09.994015: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-11-07 16:12:09.994315: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-11-07 16:12:09.995947: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpwcmc0uml\n",
      "2023-11-07 16:12:10.044333: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-11-07 16:12:10.044366: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpwcmc0uml\n",
      "2023-11-07 16:12:10.193891: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-11-07 16:12:10.812208: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpwcmc0uml\n",
      "2023-11-07 16:12:11.173575: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 1177887 microseconds.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_tflite_model = converter.convert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc83ead0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the quantized model: 5960536 bytes\n"
     ]
    }
   ],
   "source": [
    "# Measuring the size of the quantized tflite model\n",
    "# Saving the model\n",
    "import os\n",
    "\n",
    "with open('quantized_tflite_model.tflite', 'wb') as f:\n",
    "    f.write(quantized_tflite_model)\n",
    "    \n",
    "model_size = os.path.getsize('quantized_tflite_model.tflite')\n",
    "\n",
    "print(f\"Size of the quantized model: {model_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de3a2f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Load the quantized TFlite model\n",
    "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
    "interpreter.allocate_tensors()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04e936d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa32d90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'serving_default_input_1:0',\n",
       "  'index': 0,\n",
       "  'shape': array([  1, 111,  17], dtype=int32),\n",
       "  'shape_signature': array([ -1, 111,  17], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d3e1ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data Shape: (10, 147, 17)\n",
      "Expected Input Shape: [111  17]\n"
     ]
    }
   ],
   "source": [
    "# Assuming Xs_quantized is a list of quantized input data\n",
    "input_data = Xs_quantized[ibatch]\n",
    "\n",
    "# Print shapes for debugging\n",
    "print(\"Input Data Shape:\", input_data.shape)\n",
    "print(\"Expected Input Shape:\", input_details[0]['shape'][1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71712e16",
   "metadata": {},
   "source": [
    "Run the inference on the quantized TFLite model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7783559",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot set tensor: Got value of type INT8 but expected type FLOAT32 for input 0, name: serving_default_input_1:0 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Set input tensor\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_details\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[1;32m      5\u001b[0m interpreter\u001b[38;5;241m.\u001b[39minvoke()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:720\u001b[0m, in \u001b[0;36mInterpreter.set_tensor\u001b[0;34m(self, tensor_index, value)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_tensor\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor_index, value):\n\u001b[1;32m    705\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Sets the value of the input tensor.\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \n\u001b[1;32m    707\u001b[0m \u001b[38;5;124;03m  Note this copies data in `value`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;124;03m    ValueError: If the interpreter could not set the tensor.\u001b[39;00m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 720\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSetTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot set tensor: Got value of type INT8 but expected type FLOAT32 for input 0, name: serving_default_input_1:0 "
     ]
    }
   ],
   "source": [
    "# Set input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e0d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f7dd95a",
   "metadata": {},
   "source": [
    "Let's try with the Full integer quantization \n",
    "## Full integer Quantization\n",
    "Reductions in peak memory usage, and compatibility with integer only hardware devices or accelerators by making sure all model math is integer quantized.\n",
    "\n",
    "For full integer quantization, you need to calibrate or estimate the range, i.e, (min, max) of all floating-point tensors in the model. Unlike constant tensors such as weights and biases, variable tensors such as model input, activations (outputs of intermediate layers) and model output cannot be calibrated unless we run a few inference cycles. As a result, the converter requires a representative dataset to calibrate them. This dataset can be a small subset (around ~100-500 samples) of the training or validation data. Refer to the `representative_dataset()`function below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc71158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply Full Integer quantization\n",
    "def full_integer_quantization(model):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_model = converter.convert()\n",
    "    return tflite_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d69c34e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the dataset\n",
    "ds_builder = tfds.builder(\"clic_edm_qq_pf\", data_dir = '../../../tensorflow_datasets/')\n",
    "dss = ds_builder.as_data_source(\"test\")\n",
    "\n",
    "\n",
    "# Define a function to preprocess the data\n",
    "def preprocess_data(data):\n",
    "    # Implement your preprocessing steps here\n",
    "    return preprocessed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b2ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataset and run inference with quantized model\n",
    "true_pts = []\n",
    "pred_pts = []\n",
    "\n",
    "for data_batch in tfds.as_numpy(dss):\n",
    "    X = preprocess_data(data_batch[\"X\"])\n",
    "    ys = data_batch[\"ygen\"]\n",
    "    \n",
    "    # Run inference with the quantized model\n",
    "    try:\n",
    "        interpreter = tf.lite.Interpreter(model_content=quantized_model)\n",
    "        interpreter.allocate_tensors()\n",
    "        \n",
    "        input_details = interpreter.get_input_details()\n",
    "        output_details = interpreter.get_output_details()\n",
    "        \n",
    "        interpreter.set_tensor(input_details[0]['index'], [X])\n",
    "        interpreter.invoke()\n",
    "        quantized_output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        \n",
    "        mask_true_particles = ys[..., 0] != 0\n",
    "        true_pt = ys[mask_true_particles, 2]\n",
    "        pred_pt = quantized_output[0][mask_true_particles][..., 0]\n",
    "        \n",
    "        true_pts.extend(true_pt)\n",
    "        pred_pts.extend(pred_pt)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41053f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
