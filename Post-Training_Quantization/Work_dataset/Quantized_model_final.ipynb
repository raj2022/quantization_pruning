{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd29169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 23:11:48.884649: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-22 23:11:49.088206: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-22 23:11:49.088249: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-22 23:11:49.089517: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-22 23:11:49.217677: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-22 23:11:49.218536: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-22 23:11:50.202518: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76db36c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:horovod not found, ignoring\n",
      "WARNING:root:horovod not found, ignoring\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path += [\"../../../../particleflow/mlpf/\"]\n",
    "from tfmodel.model_setup import make_model\n",
    "from tfmodel.utils import parse_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "068163e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, _ = parse_config(\"../../../../particleflow/parameters/clic.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "573e563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(config, tf.float32)\n",
    "model.build((1, None, config[\"dataset\"][\"num_input_features\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6aa3018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"pf_net_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  multiple                  33        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " node_encoding (Sequential)  (1, None, 256)            70912     \n",
      "                                                                 \n",
      " input_encoding_clic (Input  multiple                  0         \n",
      " EncodingCLIC)                                                   \n",
      "                                                                 \n",
      " cg_id_0 (CombinedGraphLaye  multiple                  440128    \n",
      " r)                                                              \n",
      "                                                                 \n",
      " cg_id_1 (CombinedGraphLaye  multiple                  440128    \n",
      " r)                                                              \n",
      "                                                                 \n",
      " cg_id_2 (CombinedGraphLaye  multiple                  440128    \n",
      " r)                                                              \n",
      "                                                                 \n",
      " cg_id_3 (CombinedGraphLaye  multiple                  440128    \n",
      " r)                                                              \n",
      "                                                                 \n",
      " cg_id_4 (CombinedGraphLaye  multiple                  440128    \n",
      " r)                                                              \n",
      "                                                                 \n",
      " cg_id_5 (CombinedGraphLaye  multiple                  440128    \n",
      " r)                                                              \n",
      "                                                                 \n",
      " cg_reg_0 (CombinedGraphLay  multiple                  440128    \n",
      " er)                                                             \n",
      "                                                                 \n",
      " cg_reg_1 (CombinedGraphLay  multiple                  440128    \n",
      " er)                                                             \n",
      "                                                                 \n",
      " cg_reg_2 (CombinedGraphLay  multiple                  440128    \n",
      " er)                                                             \n",
      "                                                                 \n",
      " cg_reg_3 (CombinedGraphLay  multiple                  440128    \n",
      " er)                                                             \n",
      "                                                                 \n",
      " cg_reg_4 (CombinedGraphLay  multiple                  440128    \n",
      " er)                                                             \n",
      "                                                                 \n",
      " cg_reg_5 (CombinedGraphLay  multiple                  440128    \n",
      " er)                                                             \n",
      "                                                                 \n",
      " output_decoding (OutputDec  multiple                  269967    \n",
      " oding)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5622448 (21.45 MB)\n",
      "Trainable params: 5468815 (20.86 MB)\n",
      "Non-trainable params: 153633 (600.13 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f432bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"weights-96-5.346523.hdf5\", skip_mismatch=False, by_name=True)\n",
    "## These files hosted at https://huggingface.co/jpata/particleflow/tree/clic_clusters_v1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e0b4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the dataset\n",
    "ds_builder = tfds.builder(\"clic_edm_qq_pf\", data_dir = '../../../tensorflow_datasets/')\n",
    "dss = ds_builder.as_data_source(\"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2a6f103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_from_ds():\n",
    "    for elem in dss:\n",
    "        yield {\"X\": elem[\"X\"], \"ygen\": elem[\"ygen\"], \"ycand\": elem[\"ycand\"]}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d36e85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_signature = {k: tf.TensorSpec(shape=(None, v.shape[1])) for (k, v) in dss.dataset_info.features.items()}\n",
    "tf_dataset = tf.data.Dataset.from_generator(yield_from_ds, output_signature=output_signature).take(100).padded_batch(batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0371c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(tfds.as_numpy(tf_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67de4693",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = [d[\"X\"] for d in data]\n",
    "ys = [d[\"ygen\"] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86638326",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pts = []\n",
    "pred_pts = []\n",
    "\n",
    "for ibatch in range(len(Xs)):\n",
    "    ret = model(Xs[ibatch])\n",
    "\n",
    "    mask_true_particles = ys[ibatch][..., 0]!=0\n",
    "    \n",
    "    true_pt = ys[ibatch][mask_true_particles, 2]\n",
    "    pred_pt = ret[\"pt\"][mask_true_particles][..., 0].numpy()\n",
    "\n",
    "    true_pts.append(true_pt)\n",
    "    pred_pts.append(pred_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97e1fba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pt = np.concatenate(true_pts)\n",
    "pred_pt = np.concatenate(pred_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "615d0df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(pred_pt/true_pt, bins=np.linspace(0,3,100));\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3292af2",
   "metadata": {},
   "source": [
    "## Post-Training Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6a1493",
   "metadata": {},
   "source": [
    "### Quantize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f26b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d566c09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 23:16:09.075555: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-11-22 23:16:09.075578: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-11-22 23:16:09.075758: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpsnwxp3y6\n",
      "2023-11-22 23:16:09.130094: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-11-22 23:16:09.130146: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpsnwxp3y6\n",
      "2023-11-22 23:16:09.331507: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-11-22 23:16:09.984260: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpsnwxp3y6\n",
      "2023-11-22 23:16:10.348586: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 1272829 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# Convert the new model to a TensorFlow Lite model\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_tflite_model = converter.convert()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47e98f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the TensorFlow Lite model to a file\n",
    "with open(\"quantized_model.tflite\", \"wb\") as f:\n",
    "    f.write(quantized_tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257b1314",
   "metadata": {},
   "source": [
    "### Quantize the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a6132a",
   "metadata": {},
   "source": [
    "To quantize the dataset, you can apply quantization to the features and labels before feeding them to the model. You can achieve this by normalizing the values to a quantized range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7aebd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_quantized = [np.round(X * 127).astype(np.int8) for X in Xs]\n",
    "Xs_tf = [tf.convert_to_tensor(X_q, dtype=tf.float32) for X_q in Xs_quantized]\n",
    "\n",
    "ys_quantized = [np.round(y * 127).astype(np.int8) for y in ys]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bab64926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert quantized features to TensorFlow tensors\n",
    "Xs_tf = [tf.convert_to_tensor(X_q, dtype=tf.float32) for X_q in Xs_quantized]\n",
    "\n",
    "# Convert quantized labels to TensorFlow tensors\n",
    "ys_tf = [tf.convert_to_tensor(y_q, dtype=tf.float32) for y_q in ys_quantized]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeea08d8",
   "metadata": {},
   "source": [
    "After converting the data to TensorFlow tensors, you can perform inference using your quantized model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f69cd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a TensorFlow Lite quantized model (quantized_tflite_model)\n",
    "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
    "interpreter.allocate_tensors()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb420cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cabbaf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected input shape: [  1 111  17]\n"
     ]
    }
   ],
   "source": [
    "# Checking the expected shape of the input tensor\n",
    "expected_input_shape = input_details[0]['shape']\n",
    "print(\"Expected input shape:\", expected_input_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb9d37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20334ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create an interpreter for the quantized model\n",
    "interpreter_quantized = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
    "interpreter_quantized.allocate_tensors()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef12468a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_q_padded: (10, 111, 17)\n",
      "Shape of X_q_reshaped: (1, 111, 17)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'inference_times_quantized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m interpreter_quantized\u001b[38;5;241m.\u001b[39minvoke()\n\u001b[1;32m     39\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 40\u001b[0m \u001b[43minference_times_quantized\u001b[49m\u001b[38;5;241m.\u001b[39mappend(end_time \u001b[38;5;241m-\u001b[39m start_time)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Get output tensor\u001b[39;00m\n\u001b[1;32m     43\u001b[0m output_tensor_quantized \u001b[38;5;241m=\u001b[39m interpreter_quantized\u001b[38;5;241m.\u001b[39mget_tensor(output_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inference_times_quantized' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store true and predicted values\n",
    "true_pts_quantized = []\n",
    "pred_pts_quantized = []\n",
    "\n",
    "# Perform inference on the quantized dataset\n",
    "for X_q in Xs_tf:\n",
    "    # Assuming X_q is a NumPy array\n",
    "    target_shape = (1, 111, 17)\n",
    "\n",
    "    # Determine the length of the second dimension in X_q\n",
    "    current_length = X_q.shape[1]\n",
    "\n",
    "    # Pad or truncate X_q to match the target length\n",
    "    if current_length < target_shape[1]:\n",
    "        # Pad with zeros\n",
    "        X_q_padded = np.pad(X_q, ((0, 0), (0, target_shape[1] - current_length), (0, 0)), mode='constant')\n",
    "    elif current_length > target_shape[1]:\n",
    "        # Truncate\n",
    "        X_q_padded = X_q[:, :target_shape[1], :]\n",
    "    else:\n",
    "        # No need to change the shape\n",
    "        X_q_padded = X_q\n",
    "\n",
    "    # Print the shapes for debugging\n",
    "    print(f\"Shape of X_q_padded: {X_q_padded.shape}\")\n",
    "\n",
    "    # Reshape to match the expected shape\n",
    "    X_q_reshaped = np.resize(X_q_padded, target_shape)\n",
    "\n",
    "    # Print the shape for debugging\n",
    "    print(f\"Shape of X_q_reshaped: {X_q_reshaped.shape}\")\n",
    "\n",
    "    # Set input tensor\n",
    "    interpreter_quantized.set_tensor(input_details[0]['index'], X_q_reshaped)\n",
    "\n",
    "    # Run inference\n",
    "    start_time = time.time()\n",
    "    interpreter_quantized.invoke()\n",
    "    end_time = time.time()\n",
    "    inference_times_quantized.append(end_time - start_time)\n",
    "\n",
    "    # Get output tensor\n",
    "    output_tensor_quantized = interpreter_quantized.get_tensor(output_details[0]['index'])\n",
    "\n",
    "    # Process the output as needed\n",
    "    # Assuming a classification task with softmax output\n",
    "    predicted_probabilities_quantized = output_tensor_quantized[0]\n",
    "\n",
    "    # Get the confidence score for the true class\n",
    "    confidence_score_quantized = predicted_probabilities_quantized[ mask_true_particles[0, :111]]\n",
    "\n",
    "    # Get the predicted class (index with the highest probability)\n",
    "    predicted_class_quantized = np.argmax(predicted_probabilities_quantized, axis=1)\n",
    "    \n",
    "    # Extract the first row for boolean indexing\n",
    "    mask_true_particles_row = mask_true_particles[0, :10]\n",
    "    \n",
    "    # Append true and predicted pt values\n",
    "    true_pt_quantized = ys[ibatch][mask_true_particles_row, 2]\n",
    "    pred_pt_quantized = confidence_score_quantized  # Adjust based on your output structure\n",
    "    true_pts_quantized.append(true_pt_quantized)\n",
    "    pred_pts_quantized.append(pred_pt_quantized)\n",
    "    \n",
    "# After the loop, concatenate true and predicted pt values\n",
    "true_pt_quantized = np.concatenate(true_pts_quantized)\n",
    "pred_pt_quantized = np.concatenate(pred_pts_quantized)\n",
    "\n",
    "# Print the final shapes for debugging\n",
    "print(f\"Final shape of true_pt_quantized: {true_pt_quantized.shape}\")\n",
    "print(f\"Final shape of pred_pt_quantized: {pred_pt_quantized.shape}\")\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(pred_pt_quantized[:80] / true_pt_quantized[:, 0], bins=np.linspace(0, 3, 100))\n",
    "plt.title(\"After INT8 Quantization\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
