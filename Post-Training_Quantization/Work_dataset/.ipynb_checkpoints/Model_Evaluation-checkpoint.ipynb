{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "259b8401",
   "metadata": {},
   "source": [
    "\n",
    "Given the notebook, https://github.com/jpata/particleflow/blob/nb_clic_evaluate/notebooks/mlpf-clic-evaluate.ipynb as it loads the model, sets the weights, run the inference on a  set of events and compare the true vs. Predicted value for pT on a smaller subset.\n",
    "\n",
    "*Task:*\n",
    "Do the quantization of the model, and the conversion of the data, all in one notebook along with the model inference. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a1bf01",
   "metadata": {},
   "source": [
    "Load the quantized model, load the quantized weights, and convert all of the data using quantization and then mesaure the inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81940f5a",
   "metadata": {},
   "source": [
    "some point you need to use something like tf.lite.TFLiteConverter?\n",
    "https://www.tensorflow.org/lite/performance/post_training_integer_quant\n",
    "or is there another way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59fe0600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 07:48:43.667248: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f0554eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement yaml (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for yaml\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47659ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:horovod not found, ignoring\n",
      "WARNING:root:horovod not found, ignoring\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17233/1713518727.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"../../../particleflow/mlpf/\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtfmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_setup\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtfmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/uscms/homes/s/sraj/private/particleflow/mlpf/tfmodel/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmixed_precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yaml'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path += [\"../../../particleflow/mlpf/\"]\n",
    "from tfmodel.model_setup import make_model\n",
    "from tfmodel.utils import parse_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e4b05d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, _ = parse_config(\"../../../../particleflow/parameters/clic.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b19e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(config, tf.float32)\n",
    "model.build((1, None, config[\"dataset\"][\"num_input_features\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a179b4f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"pf_net_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " normalization (Normalizati  multiple                  33        \n",
      " on)                                                             \n",
      "                                                                 \n",
      " node_encoding (Sequential)  (1, None, 256)            70912     \n",
      "                                                                 \n",
      " input_encoding_clic (Input  multiple                  0         \n",
      " EncodingCLIC)                                                   \n",
      "                                                                 \n",
      " cg_id_0 (CombinedGraphLaye  multiple                  440128    \n",
      " r)                                                              \n",
      "                                                                 \n",
      " cg_id_1 (CombinedGraphLaye  multiple                  440128    \n",
      " r)                                                              \n",
      "                                                                 \n",
      " cg_id_2 (CombinedGraphLaye  multiple                  440128    \n",
      " r)                                                              \n",
      "                                                                 \n",
      " cg_id_3 (CombinedGraphLaye  multiple                  440128    \n",
      " r)                                                              \n",
      "                                                                 \n",
      " cg_id_4 (CombinedGraphLaye  multiple                  440128    \n",
      " r)                                                              \n",
      "                                                                 \n",
      " cg_id_5 (CombinedGraphLaye  multiple                  440128    \n",
      " r)                                                              \n",
      "                                                                 \n",
      " cg_reg_0 (CombinedGraphLay  multiple                  440128    \n",
      " er)                                                             \n",
      "                                                                 \n",
      " cg_reg_1 (CombinedGraphLay  multiple                  440128    \n",
      " er)                                                             \n",
      "                                                                 \n",
      " cg_reg_2 (CombinedGraphLay  multiple                  440128    \n",
      " er)                                                             \n",
      "                                                                 \n",
      " cg_reg_3 (CombinedGraphLay  multiple                  440128    \n",
      " er)                                                             \n",
      "                                                                 \n",
      " cg_reg_4 (CombinedGraphLay  multiple                  440128    \n",
      " er)                                                             \n",
      "                                                                 \n",
      " cg_reg_5 (CombinedGraphLay  multiple                  440128    \n",
      " er)                                                             \n",
      "                                                                 \n",
      " output_decoding (OutputDec  multiple                  269967    \n",
      " oding)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5622448 (21.45 MB)\n",
      "Trainable params: 5468815 (20.86 MB)\n",
      "Non-trainable params: 153633 (600.13 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "331f3cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"../weights-96-5.346523.hdf5\", skip_mismatch=False, by_name=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c896d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the dataset\n",
    "ds_builder = tfds.builder(\"clic_edm_qq_pf\", data_dir = '../../../tensorflow_datasets/')\n",
    "dss = ds_builder.as_data_source(\"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d5e4dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_from_ds():\n",
    "    for elem in dss:\n",
    "        yield {\"X\": elem[\"X\"], \"ygen\": elem[\"ygen\"], \"ycand\": elem[\"ycand\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1479b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_signature = {k: tf.TensorSpec(shape=(None, v.shape[1])) for (k, v) in dss.dataset_info.features.items()}\n",
    "tf_dataset = tf.data.Dataset.from_generator(yield_from_ds, output_signature=output_signature).take(100).padded_batch(batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fd80d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(tfds.as_numpy(tf_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b4acdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = [d[\"X\"] for d in data]\n",
    "ys = [d[\"ygen\"] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "689eef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pts = []\n",
    "pred_pts = []\n",
    "\n",
    "for ibatch in range(len(Xs)):\n",
    "    ret = model(Xs[ibatch])\n",
    "\n",
    "    mask_true_particles = ys[ibatch][..., 0]!=0\n",
    "    \n",
    "    true_pt = ys[ibatch][mask_true_particles, 2]\n",
    "    pred_pt = ret[\"pt\"][mask_true_particles][..., 0].numpy()\n",
    "\n",
    "    true_pts.append(true_pt)\n",
    "    pred_pts.append(pred_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bcd5ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pt = np.concatenate(true_pts)\n",
    "pred_pt = np.concatenate(pred_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "716b68b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAGdCAYAAADJ6dNTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAclElEQVR4nO3df2xVZ/0H8E8B24rSOkQLHWX4E+2mrQIlTBdhVgkS5mbU6R+zos4f6cyWJpruH4nRhBmVoV9vRF0YRmPEHxkmYz+tY+jEwGAoEzWibMHNFomuHdUUbc/3j2XVQmG95fbe5/a+Xsn945773HM+fXZ27pvnnPOcqizLsgAASMSMUhcAAPC/hBMAICnCCQCQFOEEAEiKcAIAJEU4AQCSIpwAAEkRTgCApMwqdQH5GhkZiSeffDLmzJkTVVVVpS4HAJiALMvi6aefjsbGxpgx4/xjI2UXTp588sloamoqdRkAwCQcP348Fi5ceN42ZRdO5syZExHP/HF1dXUlrgYAmIiBgYFoamoa/R0/n7ILJ8+eyqmrqxNOAKDMTOSSDBfEAgBJEU4AgKQIJwBAUoQTACApwgkAkBThBABIinACACRFOAEAkiKcAABJEU4AgKQIJwBAUoQTACApRQ8nTz31VCxbtixaW1vjsssui29961vFLgEASFjRn0o8Z86c2LNnT8yePTsGBwfjsssui3e9613x4he/uNilwJRY3L3rrGWP3bKuBJUAlKeij5zMnDkzZs+eHRERQ0NDkWVZZFlW7DIAgETlHU727NkT69evj8bGxqiqqoqdO3ee1SaXy8XixYujtrY2VqxYEfv27Rvz+VNPPRUtLS2xcOHC+NSnPhXz5s2b9B8AAEwveYeTwcHBaGlpiVwuN+7nO3bsiK6urti4cWMcPHgwWlpaYs2aNXHixInRNi960Yvi17/+dRw7diy+973vRV9f3+T/AgBgWsk7nKxduzY+//nPxzXXXDPu55s3b47rr78+NmzYEM3NzbF169aYPXt2bNu27ay2DQ0N0dLSEj//+c/Pub2hoaEYGBgY8wIApq+CXnNy+vTpOHDgQLS3t/93AzNmRHt7e+zduzciIvr6+uLpp5+OiIj+/v7Ys2dPLFmy5Jzr3LRpU9TX14++mpqaClkyAJCYgoaTkydPxvDwcDQ0NIxZ3tDQEL29vRER8fjjj8cVV1wRLS0tccUVV8QnP/nJeN3rXnfOdd58883R398/+jp+/HghSwYAElP0W4nb2tri0KFDE25fU1MTNTU1U1cQFMGZtxe7tRjg3Ao6cjJv3ryYOXPmWRe49vX1xfz58wu5KQBgmipoOKmuro6lS5dGT0/P6LKRkZHo6emJlStXFnJTAMA0lfdpnVOnTsXRo0dH3x87diwOHToUc+fOjUWLFkVXV1d0dHTEsmXLoq2tLbZs2RKDg4OxYcOGCyo0l8tFLpeL4eHhC1oPAJC2qizP6Vl3794dq1evPmt5R0dHbN++PSIivva1r8UXv/jF6O3tjdbW1vjqV78aK1asKEjBAwMDUV9fH/39/VFXV1eQdUIhjTd9/ZlccwJUmnx+v/MOJ6UmnJA64QTgbPn8fhf92ToAAOcjnAAASRFOAICklE04yeVy0dzcHMuXLy91KQDAFCqbcNLZ2RlHjhyJ/fv3l7oUAGAKlU04AQAqg3ACACRFOAEAkiKcAABJKZtw4m4dAKgMZRNO3K0DAJWhbMIJAFAZhBMAICnCCQCQFOEEAEiKcAIAJKVswolbiQGgMpRNOHErMQBUhrIJJwBAZRBOAICkCCcAQFKEEwAgKcIJAJAU4QQASIpwAgAkpWzCiUnYAKAylE04MQkbAFSGsgknAEBlEE4AgKQIJwBAUoQTACApwgkAkBThBABIinACACRFOAEAkiKcAABJKZtwYvp6AKgMZRNOTF8PAJWhbMIJAFAZhBMAICnCCQCQFOEEAEiKcAIAJEU4AQCSIpwAAEkRTgCApAgnAEBShBMAICnCCQCQFOEEAEhK2YQTTyUGgMpQNuHEU4kBoDKUTTgBACqDcAIAJEU4AQCSIpwAAEkRTgCApAgnAEBSZpW6AKhEi7t3nbXssVvWlaASgPQYOQEAkiKcAABJEU4AgKQIJwBAUoQTACApwgkAkBThBABIinACACRFOAEAkiKcAABJKZtwksvlorm5OZYvX17qUgCAKVSVZVlW6iLyMTAwEPX19dHf3x91dXWlLgfGfU7OZHi2DjCd5fP7XTYjJwBAZRBOAICkCCcAQFKEEwAgKcIJAJAU4QQASIpwAgAkRTgBAJIinAAASRFOAICkCCcAQFKEEwAgKcIJAJAU4QQASIpwAgAkRTgBAJIyq9QFQDlZ3L2r1CUATHtGTgCApAgnAEBShBMAICnCCQCQFOEEAEiKcAIAJMWtxHAexbx1eLxtPXbLuqJtHyAVRk4AgKQIJwBAUooeTo4fPx6rVq2K5ubmeP3rXx8//OEPi10CAJCwol9zMmvWrNiyZUu0trZGb29vLF26NN7xjnfEC17wgmKXAgAkqOjhZMGCBbFgwYKIiJg/f37Mmzcv/v73vwsnAEBETOK0zp49e2L9+vXR2NgYVVVVsXPnzrPa5HK5WLx4cdTW1saKFSti3759467rwIEDMTw8HE1NTXkXDgBMT3mHk8HBwWhpaYlcLjfu5zt27Iiurq7YuHFjHDx4MFpaWmLNmjVx4sSJMe3+/ve/xwc+8IH45je/ed7tDQ0NxcDAwJgXADB95R1O1q5dG5///OfjmmuuGffzzZs3x/XXXx8bNmyI5ubm2Lp1a8yePTu2bds22mZoaCiuvvrq6O7ujssvv/y829u0aVPU19ePvoyyAMD0VtC7dU6fPh0HDhyI9vb2/25gxoxob2+PvXv3RkRElmXxwQ9+MK688sq47rrrnnOdN998c/T394++jh8/XsiSAYDEFDScnDx5MoaHh6OhoWHM8oaGhujt7Y2IiIceeih27NgRO3fujNbW1mhtbY3Dhw+fc501NTVRV1c35gUATF9Fv1vnzW9+c4yMjBR7swBAmSjoyMm8efNi5syZ0dfXN2Z5X19fzJ8/v5CbAgCmqYKGk+rq6li6dGn09PSMLhsZGYmenp5YuXLlBa07l8tFc3NzLF++/ELLBAASlvdpnVOnTsXRo0dH3x87diwOHToUc+fOjUWLFkVXV1d0dHTEsmXLoq2tLbZs2RKDg4OxYcOGCyq0s7MzOjs7Y2BgIOrr6y9oXQBAuvIOJw8//HCsXr169H1XV1dERHR0dMT27dvj2muvjb/97W/xmc98Jnp7e6O1tTXuueeesy6SBQAYT1WWZVmpi8jHsyMn/f397txhyi3u3lXqEsZ47JZ1pS4BYFLy+f0u+lOJAQDORzgBAJJSNuHE3ToAUBnKJpx0dnbGkSNHYv/+/aUuBQCYQmUTTgCAyiCcAABJEU4AgKQIJwBAUoQTACApZRNO3EoMAJXB9PVwHqlNXz8eU9oD5SCf3++8H/wH00U5BA+ASlQ2p3UAgMognAAASRFOAICkCCcAQFLKJpy4lRgAKkPZhBNPJQaAylA24QQAqAzCCQCQFOEEAEiKcAIAJMX09VSE6TxV/Zl/m2ftAOXOyAkAkBThBABIStmEE5OwAUBlKJtwYhI2AKgMZRNOAIDKIJwAAEkRTgCApAgnAEBShBMAICnCCQCQFOEEAEiKcAIAJKVswokZYgGgMpRNODFDLABUhrIJJwBAZRBOAICkzCp1AUxPi7t3PWebx25ZNyXbKtR6y9V4fV/pfQKUFyMnAEBShBMAIClO61BWJnK6CIDyJpyQNGEEoPI4rQMAJMXICSUzVXfZGG0BKG9GTgCApAgnAEBSnNYhLyb4Kk8mqgPKSdmEk1wuF7lcLoaHh0tdCpQ9IRNIWdmc1vFUYgCoDGUzckJpuPMFgGIrm5ETAKAyGDkhGUZpAIgQTiqaOzgASJHTOgBAUoycMMppFQBSYOQEAEiKkZNpwIRaAEwnRk4AgKQYOXkORiWem2tVACgkIycAQFKMnJTQVM4zYg4TAMqVkRMAIClGToBxud4KKBUjJwBAUoycJM6/XkmJ/REoBiMnAEBShBMAICllc1onl8tFLpeL4eHhKd2OCcWoVPZ9IBVlM3LS2dkZR44cif3795e6FABgCpVNOAEAKoNwAgAkRTgBAJIinAAASRFOAICklM2txFwYt4kCUC6MnAAASRFOAICkOK0zCWeeIvHgMwAoHCMnAEBShBMAIClO6yTEHTVUivH2dadHgWcZOQEAkiKcAABJEU4AgKQIJwBAUoQTACApwgkAkBS3EgNJMPMy8CwjJwBAUoQTACApwgkAkBTXnBTARKbiNjU9AEyMkRMAICnCCQCQlJKEk2uuuSYuuuiiePe7312KzQMACSvJNSc33nhjfOhDH4pvf/vbpdh8UUzlNSauXyEl5icBCq0kIyerVq2KOXPmlGLTAEDi8g4ne/bsifXr10djY2NUVVXFzp07z2qTy+Vi8eLFUVtbGytWrIh9+/YVolYAoALkHU4GBwejpaUlcrncuJ/v2LEjurq6YuPGjXHw4MFoaWmJNWvWxIkTJy64WABg+sv7mpO1a9fG2rVrz/n55s2b4/rrr48NGzZERMTWrVtj165dsW3btuju7s67wKGhoRgaGhp9PzAwkPc6AIDyUdBrTk6fPh0HDhyI9vb2/25gxoxob2+PvXv3TmqdmzZtivr6+tFXU1NTocoFABJU0HBy8uTJGB4ejoaGhjHLGxoaore3d/R9e3t7vOc974m77rorFi5ceN7gcvPNN0d/f//o6/jx44UsGQBITEluJf7pT3864bY1NTVRU1MzhdUAACkp6MjJvHnzYubMmdHX1zdmeV9fX8yfP7+QmwIApqmChpPq6upYunRp9PT0jC4bGRmJnp6eWLlyZSE3BQBMU3mf1jl16lQcPXp09P2xY8fi0KFDMXfu3Fi0aFF0dXVFR0dHLFu2LNra2mLLli0xODg4evfOZOVyucjlcjE8PHxB6wGmlhmMgQtVlWVZls8Xdu/eHatXrz5reUdHR2zfvj0iIr72ta/FF7/4xejt7Y3W1tb46le/GitWrChIwQMDA1FfXx/9/f1RV1dXkHX+LwdWSINp8GF6yef3O+9wUmrCCVQG4QSml3x+v0vybB0AgHMRTgCApAgnAEBSyiac5HK5aG5ujuXLl5e6FABgCpVNOOns7IwjR47E/v37S10KADCFyiacAACVQTgBAJIinAAASRFOAICk5P1snVLxbB2oLBOZrdkssjA9lc3Iibt1AKAylE04AQAqg3ACACRFOAEAkiKcAABJEU4AgKS4lRgoW+Pdbuz2Yih/ZTNy4lZiAKgMZRNOAIDKIJwAAEkRTgCApAgnAEBShBMAICnCCQCQFOEEAEiKSdgAKJgzJ8YzKR6TUTYjJyZhA4DKUDbhBACoDMIJAJAU4QQASIpwAgAkRTgBAJIinAAASRFOAICkCCcAQFKEEwAgKaavB5ikM6dqjyj9dO2mj2c6KJuRE9PXA0BlKJtwAgBUBuEEAEiKcAIAJEU4AQCSIpwAAEkRTgCApAgnAEBShBMAICnCCQCQFOEEAEiKcAIAJEU4AQCS4qnEwLQ33tODz1QOT++dqicOp/h0ZZ7bdH4CddmMnHgqMQBUhrIJJwBAZRBOAICkCCcAQFKEEwAgKcIJAJAU4QQASIpwAgAkRTgBAJIinAAASRFOAICkCCcAQFKEEwAgKcIJAJAU4QQASIpwAgAkRTgBAJIinAAASZlV6gImKpfLRS6Xi+Hh4VKXAiRscfeuKfveY7esy3s9E/lOsU22j6aLifw3KlSbydRzIeuaLspm5KSzszOOHDkS+/fvL3UpAMAUKptwAgBUBuEEAEiKcAIAJEU4AQCSIpwAAEkRTgCApAgnAEBShBMAICnCCQCQFOEEAEiKcAIAJEU4AQCSIpwAAEkRTgCApAgnAEBShBMAICnCCQCQFOEEAEiKcAIAJEU4AQCSIpwAAEkRTgCApAgnAEBShBMAICnCCQCQFOEEAEhKScLJnXfeGUuWLIlXvepVcdttt5WiBAAgUbOKvcH//Oc/0dXVFQ888EDU19fH0qVL45prrokXv/jFxS4FAEhQ0UdO9u3bF5deemlcfPHF8cIXvjDWrl0b9913X7HLAAASlXc42bNnT6xfvz4aGxujqqoqdu7ceVabXC4Xixcvjtra2lixYkXs27dv9LMnn3wyLr744tH3F198cTzxxBOTqx4AmHbyDieDg4PR0tISuVxu3M937NgRXV1dsXHjxjh48GC0tLTEmjVr4sSJE5MqcGhoKAYGBsa8AIDpK+9rTtauXRtr16495+ebN2+O66+/PjZs2BAREVu3bo1du3bFtm3boru7OxobG8eMlDzxxBPR1tZ2zvVt2rQpPvvZz+ZbJkDBLe7eVZTvFHLdk93+md977JZ1ya17vO9NZj0TMdm+nkiNhdr+RL5XqHqmWkGvOTl9+nQcOHAg2tvb/7uBGTOivb099u7dGxERbW1t8eijj8YTTzwRp06dirvvvjvWrFlzznXefPPN0d/fP/o6fvx4IUsGABJT0Lt1Tp48GcPDw9HQ0DBmeUNDQ/z+979/ZoOzZsWXv/zlWL16dYyMjMSnP/3p896pU1NTEzU1NYUsEwBIWNFvJY6IuOqqq+Kqq64qxaYBgMQV9LTOvHnzYubMmdHX1zdmeV9fX8yfP7+QmwIApqmChpPq6upYunRp9PT0jC4bGRmJnp6eWLlyZSE3BQBMU3mf1jl16lQcPXp09P2xY8fi0KFDMXfu3Fi0aFF0dXVFR0dHLFu2LNra2mLLli0xODg4evfOZOVyucjlcjE8PHxB6wEA0pZ3OHn44Ydj9erVo++7uroiIqKjoyO2b98e1157bfztb3+Lz3zmM9Hb2xutra1xzz33nHWRbL46Ozujs7MzBgYGor6+/oLWBQCkK+9wsmrVqsiy7LxtbrjhhrjhhhsmXRQAULlK8lRiAIBzEU4AgKSUTTjJ5XLR3Nwcy5cvL3UpAMAUKptw0tnZGUeOHIn9+/eXuhQAYAqVTTgBACqDcAIAJEU4AQCSUpIH/12IZ+dYGRgYmJL1jwz9c0rWC1DOxjvmTuR4OZHvFXLdhVpPoX4LJlvjmd+byr4ulme3/VxzpUVEVGUTaZWQv/zlL9HU1FTqMgCASTh+/HgsXLjwvG3KLpyMjIzEk08+GXPmzImqqqqCrntgYCCampri+PHjUVdXV9B1Tzf6auL01cTpq4nTVxOnryZuKvsqy7J4+umno7GxMWbMOP9VJWV3WmfGjBnPmbguVF1dnR14gvTVxOmridNXE6evJk5fTdxU9dVEn43nglgAICnCCQCQFOHkf9TU1MTGjRujpqam1KUkT19NnL6aOH01cfpq4vTVxKXSV2V3QSwAML0ZOQEAkiKcAABJEU4AgKQIJwBAUiounORyuVi8eHHU1tbGihUrYt++fedt/8Mf/jBe85rXRG1tbbzuda+Lu+66q0iVll4+fbV9+/aoqqoa86qtrS1itaWxZ8+eWL9+fTQ2NkZVVVXs3LnzOb+ze/fueOMb3xg1NTXxyle+MrZv3z7ldaYi3/7avXv3WftVVVVV9Pb2FqfgEtm0aVMsX7485syZEy996Uvj6quvjj/84Q/P+b1KPF5Npq8q9XgVEfH1r389Xv/6149OsrZy5cq4++67z/udUuxXFRVOduzYEV1dXbFx48Y4ePBgtLS0xJo1a+LEiRPjtv/lL38Z73//++PDH/5wPPLII3H11VfH1VdfHY8++miRKy++fPsq4pkZBf/617+Ovh5//PEiVlwag4OD0dLSErlcbkLtjx07FuvWrYvVq1fHoUOH4qabboqPfOQjce+9905xpWnIt7+e9Yc//GHMvvXSl750iipMw4MPPhidnZ3xq1/9Ku6///7497//HW9/+9tjcHDwnN+p1OPVZPoqojKPVxERCxcujFtuuSUOHDgQDz/8cFx55ZXxzne+M37729+O275k+1VWQdra2rLOzs7R98PDw1ljY2O2adOmcdu/973vzdatWzdm2YoVK7KPfexjU1pnCvLtq9tvvz2rr68vUnVpiojsjjvuOG+bT3/609mll146Ztm1116brVmzZgorS9NE+uuBBx7IIiL7xz/+UZSaUnXixIksIrIHH3zwnG0q+Xj1vybSV45XY1100UXZbbfdNu5npdqvKmbk5PTp03HgwIFob28fXTZjxoxob2+PvXv3jvudvXv3jmkfEbFmzZpztp8uJtNXERGnTp2KSy65JJqams6bxCtZpe5TF6q1tTUWLFgQb3vb2+Khhx4qdTlF19/fHxERc+fOPWcb+9YzJtJXEY5XERHDw8Px/e9/PwYHB2PlypXjtinVflUx4eTkyZMxPDwcDQ0NY5Y3NDSc8/x1b29vXu2ni8n01ZIlS2Lbtm3xk5/8JL773e/GyMhIXH755fGXv/ylGCWXjXPtUwMDA/Gvf/2rRFWla8GCBbF169b48Y9/HD/+8Y+jqakpVq1aFQcPHix1aUUzMjISN910U7zpTW+Kyy677JztKvV49b8m2leVfrw6fPhwvPCFL4yampr4+Mc/HnfccUc0NzeP27ZU+1XZPZWYNK1cuXJM8r788svjta99bXzjG9+Iz33ucyWsjHK2ZMmSWLJkyej7yy+/PP70pz/FrbfeGt/5zndKWFnxdHZ2xqOPPhq/+MUvSl1K8ibaV5V+vFqyZEkcOnQo+vv740c/+lF0dHTEgw8+eM6AUgoVM3Iyb968mDlzZvT19Y1Z3tfXF/Pnzx/3O/Pnz8+r/XQxmb460/Oe97x4wxveEEePHp2KEsvWufapurq6eP7zn1+iqspLW1tbxexXN9xwQ9x5553xwAMPxMKFC8/btlKPV8/Kp6/OVGnHq+rq6njlK18ZS5cujU2bNkVLS0t85StfGbdtqfarigkn1dXVsXTp0ujp6RldNjIyEj09Pec817Zy5cox7SMi7r///nO2ny4m01dnGh4ejsOHD8eCBQumqsyyVKn7VCEdOnRo2u9XWZbFDTfcEHfccUf87Gc/i5e97GXP+Z1K3bcm01dnqvTj1cjISAwNDY37Wcn2qym93DYx3//+97Oampps+/bt2ZEjR7KPfvSj2Yte9KKst7c3y7Isu+6667Lu7u7R9g899FA2a9as7Etf+lL2u9/9Ltu4cWP2vOc9Lzt8+HCp/oSiybevPvvZz2b33ntv9qc//Sk7cOBA9r73vS+rra3Nfvvb35bqTyiKp59+OnvkkUeyRx55JIuIbPPmzdkjjzySPf7441mWZVl3d3d23XXXjbb/85//nM2ePTv71Kc+lf3ud7/LcrlcNnPmzOyee+4p1Z9QVPn216233prt3Lkz++Mf/5gdPnw4u/HGG7MZM2ZkP/3pT0v1JxTFJz7xiay+vj7bvXt39te//nX09c9//nO0jePVMybTV5V6vMqyZ/4fe/DBB7Njx45lv/nNb7Lu7u6sqqoqu++++7IsS2e/qqhwkmVZ9n//93/ZokWLsurq6qytrS371a9+NfrZW97ylqyjo2NM+x/84AfZq1/96qy6ujq79NJLs127dhW54tLJp69uuumm0bYNDQ3ZO97xjuzgwYMlqLq4nr3V9czXs33T0dGRveUtbznrO62trVl1dXX28pe/PLv99tuLXnep5NtfX/jCF7JXvOIVWW1tbTZ37txs1apV2c9+9rPSFF9E4/VRRIzZVxyvnjGZvqrU41WWZdmHPvSh7JJLLsmqq6uzl7zkJdlb3/rW0WCSZensV1VZlmVTOzYDADBxFXPNCQBQHoQTACApwgkAkBThBABIinACACRFOAEAkiKcAABJEU4AgKQIJwBAUoQTACApwgkAkBThBABIyv8DTqYQVYAnBpcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(pred_pt/true_pt, bins=np.linspace(0,3,100));\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c32a2ee",
   "metadata": {},
   "source": [
    "## Quantized data into the unquantized model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4817027",
   "metadata": {},
   "source": [
    "Before feeding the input data to the model, we need to quantize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63219f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize input data\n",
    "Xs_quantized = [np.round(X * 127).astype(np.int8) for X in Xs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a63aed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_quantized = [np.round(X * 127).astype(np.int8) for X in Xs]\n",
    "Xs_tf = [tf.convert_to_tensor(X_q, dtype=tf.float32) for X_q in Xs_quantized]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ab11cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_quantized = [np.round(y * 127).astype(np.int8) for y in ys]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "744809e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pts = []\n",
    "pred_pts = []\n",
    "\n",
    "for ibatch in range(len(Xs)):\n",
    "    ret = model(Xs_tf[ibatch])\n",
    "\n",
    "    mask_true_particles = ys_quantized[ibatch][..., 0]!=0\n",
    "    \n",
    "    true_pt = ys[ibatch][mask_true_particles, 2]\n",
    "    pred_pt = ret[\"pt\"][mask_true_particles][..., 0].numpy()\n",
    "\n",
    "    true_pts.append(true_pt)\n",
    "    pred_pts.append(pred_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8840038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pt = np.concatenate(true_pts)\n",
    "pred_pt = np.concatenate(pred_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1730822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(pred_pt/true_pt, bins=np.linspace(0,3,100));\n",
    "plt.title(\"After INT8 Quantization\")\n",
    "plt.yscale(\"log\")\n",
    "plt.savefig('Output_after_INT8_quantization.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63be5752",
   "metadata": {},
   "source": [
    "Above one is the output with the quantized dataset but not quantized model. We were wokring with the Dynamic Range Quantization. \n",
    "Dynamic range quantization is a recommended starting point because it provides reduced memory usage and faster computation without you having to provide a representative dataset for calibration. This type of quantization, statically quantizes only the weights from floating point to integer at conversion time, which provides 8-bits of precision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5391ebdd",
   "metadata": {},
   "source": [
    "## Quantized Model with the Quantized Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77582ebf",
   "metadata": {},
   "source": [
    "Let's try with **[quantized model](https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization_of_weights_and_activations)**, Post-training qunatization method, Dynamic range quantization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f439ec",
   "metadata": {},
   "source": [
    "## Dynamic Range Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b108b047",
   "metadata": {},
   "source": [
    "Converting the model using dynamic range quantization(https://www.tensorflow.org/lite/performance/post_training_integer_quant):\n",
    "Let's enable the default optimizations flag to quantize all fixed parameters (such as weights):\n",
    "The model is now a bit smaller with quantized weights, but other variable data is still in float format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3790f748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-07 16:12:09.994015: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2023-11-07 16:12:09.994315: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2023-11-07 16:12:09.995947: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpwcmc0uml\n",
      "2023-11-07 16:12:10.044333: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2023-11-07 16:12:10.044366: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpwcmc0uml\n",
      "2023-11-07 16:12:10.193891: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-11-07 16:12:10.812208: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpwcmc0uml\n",
      "2023-11-07 16:12:11.173575: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 1177887 microseconds.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_tflite_model = converter.convert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc83ead0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the quantized model: 5960536 bytes\n"
     ]
    }
   ],
   "source": [
    "# Measuring the size of the quantized tflite model\n",
    "# Saving the model\n",
    "import os\n",
    "\n",
    "with open('quantized_tflite_model.tflite', 'wb') as f:\n",
    "    f.write(quantized_tflite_model)\n",
    "    \n",
    "model_size = os.path.getsize('quantized_tflite_model.tflite')\n",
    "\n",
    "print(f\"Size of the quantized model: {model_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de3a2f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# Load the quantized TFlite model\n",
    "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
    "interpreter.allocate_tensors()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04e936d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa32d90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'serving_default_input_1:0',\n",
       "  'index': 0,\n",
       "  'shape': array([  1, 111,  17], dtype=int32),\n",
       "  'shape_signature': array([ -1, 111,  17], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d3e1ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data Shape: (10, 147, 17)\n",
      "Expected Input Shape: [111  17]\n"
     ]
    }
   ],
   "source": [
    "# Assuming Xs_quantized is a list of quantized input data\n",
    "input_data = Xs_quantized[ibatch]\n",
    "\n",
    "# Print shapes for debugging\n",
    "print(\"Input Data Shape:\", input_data.shape)\n",
    "print(\"Expected Input Shape:\", input_details[0]['shape'][1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71712e16",
   "metadata": {},
   "source": [
    "Run the inference on the quantized TFLite model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c7783559",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot set tensor: Got value of type INT8 but expected type FLOAT32 for input 0, name: serving_default_input_1:0 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Set input tensor\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_details\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[1;32m      5\u001b[0m interpreter\u001b[38;5;241m.\u001b[39minvoke()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:720\u001b[0m, in \u001b[0;36mInterpreter.set_tensor\u001b[0;34m(self, tensor_index, value)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_tensor\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor_index, value):\n\u001b[1;32m    705\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Sets the value of the input tensor.\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \n\u001b[1;32m    707\u001b[0m \u001b[38;5;124;03m  Note this copies data in `value`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;124;03m    ValueError: If the interpreter could not set the tensor.\u001b[39;00m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 720\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSetTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot set tensor: Got value of type INT8 but expected type FLOAT32 for input 0, name: serving_default_input_1:0 "
     ]
    }
   ],
   "source": [
    "# Set input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run inference\n",
    "interpreter.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e0d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f7dd95a",
   "metadata": {},
   "source": [
    "Let's try with the Full integer quantization \n",
    "## Full integer Quantization\n",
    "Reductions in peak memory usage, and compatibility with integer only hardware devices or accelerators by making sure all model math is integer quantized.\n",
    "\n",
    "For full integer quantization, you need to calibrate or estimate the range, i.e, (min, max) of all floating-point tensors in the model. Unlike constant tensors such as weights and biases, variable tensors such as model input, activations (outputs of intermediate layers) and model output cannot be calibrated unless we run a few inference cycles. As a result, the converter requires a representative dataset to calibrate them. This dataset can be a small subset (around ~100-500 samples) of the training or validation data. Refer to the `representative_dataset()`function below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc71158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply Full Integer quantization\n",
    "def full_integer_quantization(model):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_model = converter.convert()\n",
    "    return tflite_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d69c34e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the dataset\n",
    "ds_builder = tfds.builder(\"clic_edm_qq_pf\", data_dir = '../../../tensorflow_datasets/')\n",
    "dss = ds_builder.as_data_source(\"test\")\n",
    "\n",
    "\n",
    "# Define a function to preprocess the data\n",
    "def preprocess_data(data):\n",
    "    # Implement your preprocessing steps here\n",
    "    return preprocessed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b2ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataset and run inference with quantized model\n",
    "true_pts = []\n",
    "pred_pts = []\n",
    "\n",
    "for data_batch in tfds.as_numpy(dss):\n",
    "    X = preprocess_data(data_batch[\"X\"])\n",
    "    ys = data_batch[\"ygen\"]\n",
    "    \n",
    "    # Run inference with the quantized model\n",
    "    try:\n",
    "        interpreter = tf.lite.Interpreter(model_content=quantized_model)\n",
    "        interpreter.allocate_tensors()\n",
    "        \n",
    "        input_details = interpreter.get_input_details()\n",
    "        output_details = interpreter.get_output_details()\n",
    "        \n",
    "        interpreter.set_tensor(input_details[0]['index'], [X])\n",
    "        interpreter.invoke()\n",
    "        quantized_output = interpreter.get_tensor(output_details[0]['index'])\n",
    "        \n",
    "        mask_true_particles = ys[..., 0] != 0\n",
    "        true_pt = ys[mask_true_particles, 2]\n",
    "        pred_pt = quantized_output[0][mask_true_particles][..., 0]\n",
    "        \n",
    "        true_pts.extend(true_pt)\n",
    "        pred_pts.extend(pred_pt)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41053f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
