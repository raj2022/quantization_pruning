{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1236a133",
   "metadata": {},
   "source": [
    "# Work on the Debugging (11/28/2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34779848",
   "metadata": {},
   "source": [
    "Working on the debugging of the model and working out the task in hand. Also checking if I am doing correct or not.\n",
    "\n",
    "Do the quantization of the model, and the conversion of the data, all in one notebook along with the model inference.\n",
    "\n",
    "<span style=\"color:red;\">\n",
    "  \n",
    "1. Standardize the P_T value before quantizing.\n",
    "2. Three P_T plots:\n",
    "    1. Raw P_T (Number of particles vs P_T (GeV))\n",
    "    2. P_T after standardization (X-axis should be arbitrary units)\n",
    "    3. After int8 quantization. Centers at 0 and range should be between -127, 127)\n",
    "      \n",
    "3. Upon standardization, there could be a few values which could be out of the range of (-127, 127). How to deal with the outliers? One of the methods is to put all of them in the last bin. Are there any other methods available?\n",
    "  \n",
    "4. Fix bin size does not give resolution. We can lose information if we discard those outliers.\n",
    "  \n",
    "5. Print the true and predicted P_T values. Should be in the INT8 range.\n",
    "  \n",
    "6. Put on the distribution if it makes sense.\n",
    "</span>\n",
    "\n",
    "<span style=\"color:green;\">\n",
    "    \n",
    "1. Read about the dataset\n",
    "    \n",
    "2. Understand the outputs what you are plotting\n",
    "</span>\n",
    "\n",
    "\n",
    "**Date(11/28/2023)**\n",
    "1. We need to standradization to whole datasample which is 100 in our case. \n",
    "2. Plot the datasample before and after standradization ??\n",
    "3. Or do we only need to standradize the True P_T?\n",
    "4. What type of output you have right now?\n",
    "\n",
    "Links:-\n",
    "1. https://www.tensorflow.org/lite/performance/post_training_integer_quant\n",
    "2. https://www.tensorflow.org/lite/performance/post_training_quantization\n",
    "3. https://www.tensorflow.org/lite/models/convert/\n",
    "4. https://www.tensorflow.org/lite/performance/post_training_quant\n",
    "5. https://www.tensorflow.org/lite/performance/post_training_float16_quant \n",
    "6. https://www.tensorflow.org/lite/performance/post_training_integer_quant_16x8 \n",
    "7. https://www.tensorflow.org/lite/performance/quantization_spec \n",
    "8. https://arxiv.org/pdf/1712.05877.pdf\n",
    "\n",
    "***DATE(11.29.2023)***\n",
    "\n",
    "` The dataset contains input features and target features consist of different things like pT, eta, phi etc. so all of those need to be standardized and quantized separately. but you can start with just pT`\n",
    "Task:\n",
    "1. Since working only on the `pT`, quntize it after standradization. Further Check the plots before and after quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfd4655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 15:18:56.409803: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-29 15:19:01.455295: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import ROOT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a633e038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path += [\"../../../mlpf/particleflow/mlpf/\"]\n",
    "from tfmodel.model_setup import make_model\n",
    "from tfmodel.utils import parse_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d910a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, _ = parse_config(\"../../../mlpf/particleflow/parameters/clic.yaml\") #positions on the lxplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d5c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(config, tf.float32)\n",
    "model.build((1, None, config[\"dataset\"][\"num_input_features\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cec36a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2609350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"weights-96-5.346523.hdf5\", skip_mismatch=False, by_name=True)\n",
    "## These files hosted at https://huggingface.co/jpata/particleflow/tree/clic_clusters_v1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e690530",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the dataset\n",
    "ds_builder = tfds.builder(\"clic_edm_qq_pf\", data_dir = '../../../mlpf/tensorflow_datasets/') # Tensorflow datsets positions in the lxplus\n",
    "dss = ds_builder.as_data_source(\"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc23eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_from_ds():\n",
    "    for elem in dss:\n",
    "        yield {\"X\": elem[\"X\"], \"ygen\": elem[\"ygen\"], \"ycand\": elem[\"ycand\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45785bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_signature = {k: tf.TensorSpec(shape=(None, v.shape[1])) for (k, v) in dss.dataset_info.features.items()}\n",
    "tf_dataset = tf.data.Dataset.from_generator(yield_from_ds, output_signature=output_signature).take(100).padded_batch(batch_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c57926",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a407194",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(tfds.as_numpy(tf_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2b6ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = [d[\"X\"] for d in data]\n",
    "ys = [d[\"ygen\"] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7335ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pts = []\n",
    "pred_pts = []\n",
    "\n",
    "for ibatch in range(len(Xs)):\n",
    "    ret = model(Xs[ibatch])\n",
    "\n",
    "    mask_true_particles = ys[ibatch][..., 0]!=0\n",
    "    \n",
    "    true_pt = ys[ibatch][mask_true_particles, 2]\n",
    "    pred_pt = ret[\"pt\"][mask_true_particles][..., 0].numpy()\n",
    "\n",
    "    true_pts.append(true_pt)\n",
    "    pred_pts.append(pred_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d51889",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pt = np.concatenate(true_pts)\n",
    "pred_pt = np.concatenate(pred_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85559b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pred_pt/true_pt, bins=np.linspace(0,3,100));\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4b70c",
   "metadata": {},
   "source": [
    "### pT(GeV) Plot before standradization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7379565b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d646f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2f003c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8930c843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
