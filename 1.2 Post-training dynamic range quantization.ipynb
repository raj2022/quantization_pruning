{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db1adeee",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "[TensorFlow Lite](https://www.tensorflow.org/lite/) now supports\n",
    "converting weights to 8 bit precision as part of model conversion from\n",
    "tensorflow graphdefs to TensorFlow Lite's flat buffer format. Dynamic range quantization achieves a 4x reduction in the model size. In addition, TFLite supports on the fly quantization and dequantization of activations to allow for:\n",
    "\n",
    "1.  Using quantized kernels for faster implementation when available.\n",
    "2.  Mixing of floating-point kernels with quantized kernels for different parts\n",
    "    of the graph.\n",
    "\n",
    "The activations are always stored in floating point. For ops that\n",
    "support quantized kernels, the activations are quantized to 8 bits of precision\n",
    "dynamically prior to processing and are de-quantized to float precision after\n",
    "processing. Depending on the model being converted, this can give a speedup over\n",
    "pure floating point computation.\n",
    "\n",
    "In contrast to\n",
    "[quantization aware training](https://github.com/tensorflow/tensorflow/tree/r1.14/tensorflow/contrib/quantize)\n",
    ", the weights are quantized post training and the activations are quantized dynamically \n",
    "at inference in this method.\n",
    "Therefore, the model weights are not retrained to compensate for quantization\n",
    "induced errors. It is important to check the accuracy of the quantized model to\n",
    "ensure that the degradation is acceptable.\n",
    "\n",
    "This tutorial trains an MNIST model from scratch, checks its accuracy in\n",
    "TensorFlow, and then converts the model into a Tensorflow Lite flatbuffer\n",
    "with dynamic range quantization. Finally, it checks the\n",
    "accuracy of the converted model and compare it to the original float model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea1bd72",
   "metadata": {},
   "source": [
    "## Build an MNIST model\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88edd9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pathlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bea95e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c08509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
