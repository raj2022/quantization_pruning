{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d5428ee",
   "metadata": {},
   "source": [
    "# Quantization inference test\n",
    "https://gist.github.com/martinferianc/d6090fffb4c95efed6f1152d5fde079d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d51c327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine types:x86_64\n",
      "Architecture :('64bit', 'ELF')\n"
     ]
    }
   ],
   "source": [
    "# Architecture\n",
    "import platform\n",
    "\n",
    "machine = platform.machine()\n",
    "architecture = platform.architecture()\n",
    "\n",
    "print(f\"Machine types:{machine}\")\n",
    "print(f\"Architecture :{architecture}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abff809a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Quantization Backends: ['qnnpack', 'none', 'x86', 'fbgemm']\n",
      "Default Quantization Backend: x86\n",
      "Current Quantization Backend: x86\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check available quantization backends\n",
    "available_backends = torch.backends.quantized.supported_engines\n",
    "print(f\"Available Quantization Backends: {available_backends}\")\n",
    "\n",
    "# Check the default quantization backend\n",
    "default_backend = torch.backends.quantized.engine\n",
    "print(f\"Default Quantization Backend: {default_backend}\")\n",
    "\n",
    "# Print the current backend in use\n",
    "current_backend = torch.backends.quantized.engine\n",
    "print(f\"Current Quantization Backend: {current_backend}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d5a323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 output (first 5): tensor([[ 0.3440],\n",
      "        [-0.0038],\n",
      "        [ 0.2164],\n",
      "        [ 0.1589],\n",
      "        [-0.0345]])\n",
      "INT8 output (first 5): tensor([[ 0.3355],\n",
      "        [ 0.0040],\n",
      "        [ 0.2201],\n",
      "        [ 0.1595],\n",
      "        [-0.0303]])\n",
      "\n",
      "FP32 model size: 964 bytes\n",
      "INT8 model size: 0 bytes\n",
      "\n",
      "FP32 average inference time: 0.000155 seconds\n",
      "INT8 average inference time: 0.000356 seconds\n",
      "Speedup: 0.43x\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.quantization import quantize_dynamic\n",
    "import time\n",
    "\n",
    "# Define the simple neural network (same as before)\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 20)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create model instance and set to eval mode\n",
    "model = SimpleNet()\n",
    "model.eval()\n",
    "\n",
    "# Prepare sample input data\n",
    "input_data = torch.randn(1000, 10)  # Increased batch size for more noticeable timing difference\n",
    "\n",
    "# Function to measure inference time\n",
    "def measure_inference_time(model, input_data, num_runs=100):\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            _ = model(input_data)\n",
    "    end_time = time.time()\n",
    "    return (end_time - start_time) / num_runs\n",
    "\n",
    "# Measure FP32 inference time\n",
    "fp32_time = measure_inference_time(model, input_data)\n",
    "\n",
    "# Quantize the model to INT8\n",
    "quantized_model = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "# Measure INT8 inference time\n",
    "int8_time = measure_inference_time(quantized_model, input_data)\n",
    "\n",
    "# Compare outputs, memory usage, and inference time\n",
    "with torch.no_grad():\n",
    "    fp32_output = model(input_data)\n",
    "    int8_output = quantized_model(input_data)\n",
    "\n",
    "print(\"FP32 output (first 5):\", fp32_output[:5])\n",
    "print(\"INT8 output (first 5):\", int8_output[:5])\n",
    "\n",
    "print(\"\\nFP32 model size:\", sum(p.numel() for p in model.parameters()) * 4, \"bytes\")\n",
    "print(\"INT8 model size:\", sum(p.numel() for p in quantized_model.parameters()) * 1, \"bytes\")\n",
    "\n",
    "print(f\"\\nFP32 average inference time: {fp32_time:.6f} seconds\")\n",
    "print(f\"INT8 average inference time: {int8_time:.6f} seconds\")\n",
    "print(f\"Speedup: {fp32_time / int8_time:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f9dbc7",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c291f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 01:12:02.332507: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2024-12-19 01:12:08.706554: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f29d40ab950 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-12-19 01:12:08.706601: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2024-12-19 01:12:08.726776: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-12-19 01:12:09.005680: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 30s 15ms/step - loss: 0.4845 - accuracy: 0.8311 - val_loss: 0.0929 - val_accuracy: 0.9742\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 14s 14ms/step - loss: 0.0666 - accuracy: 0.9815 - val_loss: 0.0473 - val_accuracy: 0.9852\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 14s 15ms/step - loss: 0.0470 - accuracy: 0.9869 - val_loss: 0.0344 - val_accuracy: 0.9895\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 14s 14ms/step - loss: 0.0365 - accuracy: 0.9895 - val_loss: 0.0621 - val_accuracy: 0.9817\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 14s 15ms/step - loss: 0.0298 - accuracy: 0.9916 - val_loss: 0.0225 - val_accuracy: 0.9930\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 13s 14ms/step - loss: 0.0267 - accuracy: 0.9923 - val_loss: 0.0363 - val_accuracy: 0.9885\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 14s 15ms/step - loss: 0.0223 - accuracy: 0.9931 - val_loss: 0.0213 - val_accuracy: 0.9940\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 14s 15ms/step - loss: 0.0209 - accuracy: 0.9940 - val_loss: 0.0275 - val_accuracy: 0.9933\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 14s 15ms/step - loss: 0.0167 - accuracy: 0.9952 - val_loss: 0.0386 - val_accuracy: 0.9908\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 14s 15ms/step - loss: 0.0166 - accuracy: 0.9953 - val_loss: 0.0298 - val_accuracy: 0.9914\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0298 - accuracy: 0.9914\n",
      "Test Loss: 0.02984914928674698, Test Accuracy: 0.9914000034332275\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize data to range [0, 1] and add channel dimension\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Define U-Net model\n",
    "\n",
    "# Define U-Net model for classification\n",
    "def unet_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    c1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    c1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(p1)\n",
    "    c2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    # Bottleneck\n",
    "    c3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p2)\n",
    "    c3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c3)\n",
    "\n",
    "    # Decoder\n",
    "    u1 = layers.UpSampling2D((2, 2))(c3)\n",
    "    u1 = layers.concatenate([u1, c2])\n",
    "    c4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u1)\n",
    "    c4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c4)\n",
    "\n",
    "    u2 = layers.UpSampling2D((2, 2))(c4)\n",
    "    u2 = layers.concatenate([u2, c1])\n",
    "    c5 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(u2)\n",
    "    c5 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c5)\n",
    "\n",
    "    # Classification output\n",
    "    g = layers.GlobalAveragePooling2D()(c5)\n",
    "    outputs = layers.Dense(10, activation='softmax')(g)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Prepare model\n",
    "model = unet_model(input_shape=(28, 28, 1))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "054e47df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51473624",
   "metadata": {},
   "source": [
    "# CNN With pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a51ee905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.8289, Test Loss: 0.1054\n",
      "Epoch 2/10, Train Loss: 0.0962, Test Loss: 0.0620\n",
      "Epoch 3/10, Train Loss: 0.0647, Test Loss: 0.0402\n",
      "Epoch 4/10, Train Loss: 0.0502, Test Loss: 0.0501\n",
      "Epoch 5/10, Train Loss: 0.0430, Test Loss: 0.0435\n",
      "Epoch 6/10, Train Loss: 0.0370, Test Loss: 0.0288\n",
      "Epoch 7/10, Train Loss: 0.0307, Test Loss: 0.0266\n",
      "Epoch 8/10, Train Loss: 0.0280, Test Loss: 0.0362\n",
      "Epoch 9/10, Train Loss: 0.0268, Test Loss: 0.0365\n",
      "Epoch 10/10, Train Loss: 0.0217, Test Loss: 0.0375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define U-Net model for classification\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Classification output\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)\n",
    "        pool1 = self.pool1(enc1)\n",
    "\n",
    "        enc2 = self.enc2(pool1)\n",
    "        pool2 = self.pool2(enc2)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(pool2)\n",
    "\n",
    "        # Decoder\n",
    "        up1 = self.up1(bottleneck)\n",
    "        dec1 = self.dec1(torch.cat([up1, enc2], dim=1))\n",
    "\n",
    "        up2 = self.up2(dec1)\n",
    "        dec2 = self.dec2(torch.cat([up2, enc1], dim=1))\n",
    "\n",
    "        # Classification\n",
    "        out = self.classifier(dec2)\n",
    "        return out\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load MNIST data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = UNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "    # Evaluate on test data\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    test_losses.append(test_loss / len(test_loader))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b19af687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and test loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Move model to CPU\n",
    "model = model.to(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04593439",
   "metadata": {},
   "source": [
    "# inference speed calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbe9e88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model inference time: 7.2349 seconds\n",
      "Quantized model inference time: 0.1780 seconds\n",
      "Speedup with quantization: 40.65x\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "# Step 1: Define a simple model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create a random input tensor\n",
    "input_data = torch.rand(1000, 784)  # Batch of 1000, input size 784\n",
    "\n",
    "# Step 2: Initialize the model\n",
    "model = SimpleModel()\n",
    "\n",
    "# Simulate training by setting to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Step 3: Measure inference time for the original model\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(100):  # Run inference 100 times\n",
    "        _ = model(input_data)\n",
    "original_inference_time = time.time() - start_time\n",
    "print(f\"Original model inference time: {original_inference_time:.4f} seconds\")\n",
    "\n",
    "# Step 4: Quantize the model\n",
    "quantized_model = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)  # Quantize Linear layers\n",
    "\n",
    "# Step 5: Measure inference time for the quantized model\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(100):  # Run inference 100 times\n",
    "        _ = quantized_model(input_data)\n",
    "quantized_inference_time = time.time() - start_time\n",
    "print(f\"Quantized model inference time: {quantized_inference_time:.4f} seconds\")\n",
    "\n",
    "# Compare the results\n",
    "speedup = original_inference_time / quantized_inference_time\n",
    "print(f\"Speedup with quantization: {speedup:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf40356",
   "metadata": {},
   "source": [
    "## Inference on CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9e48eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original CNN model inference time: 8.3112 seconds\n",
      "Quantized CNN model inference time: 0.4746 seconds\n",
      "Speedup with quantization: 17.51x\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "# Step 1: Define a CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)  # 1 input channel, 16 output channels\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # 2x2 pooling\n",
    "\n",
    "        # Use a dummy input to calculate the flattened size\n",
    "        dummy_input = torch.zeros(1, 1, 28, 28)  # Example input shape\n",
    "        with torch.no_grad():\n",
    "            flattened_size = self._get_flattened_size(dummy_input)\n",
    "\n",
    "        self.fc1 = nn.Linear(flattened_size, 128)  # Adjust input size dynamically\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def _get_flattened_size(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        return x.numel()  # Total number of elements in the tensor\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create a random input tensor (e.g., batch size 100, 1 channel, 28x28 image)\n",
    "input_data = torch.rand(100, 1, 28, 28)\n",
    "\n",
    "# Step 2: Initialize the model\n",
    "model = CNNModel()\n",
    "\n",
    "# Simulate training by setting to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Step 3: Measure inference time for the original model\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(100):  # Run inference 100 times\n",
    "        _ = model(input_data)\n",
    "original_inference_time = time.time() - start_time\n",
    "print(f\"Original CNN model inference time: {original_inference_time:.4f} seconds\")\n",
    "\n",
    "# Step 4: Quantize the model (Dynamic quantization is typically for Linear layers)\n",
    "quantized_model = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)  # Quantize Linear layers\n",
    "\n",
    "# Step 5: Measure inference time for the quantized model\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(100):  # Run inference 100 times\n",
    "        _ = quantized_model(input_data)\n",
    "quantized_inference_time = time.time() - start_time\n",
    "print(f\"Quantized CNN model inference time: {quantized_inference_time:.4f} seconds\")\n",
    "\n",
    "# Compare the results\n",
    "speedup = original_inference_time / quantized_inference_time\n",
    "print(f\"Speedup with quantization: {speedup:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898c6939",
   "metadata": {},
   "source": [
    "## Setting the backend to FBGEMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "799f3420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Quantization Backends: ['qnnpack', 'none', 'x86', 'fbgemm']\n",
      "Current Quantization Backend: fbgemm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "\n",
    "# Check available backends\n",
    "available_backends = torch.backends.quantized.supported_engines\n",
    "print(f\"Available Quantization Backends: {available_backends}\")\n",
    "\n",
    "# Set the backend to FBGEMM (if available)\n",
    "if \"fbgemm\" in available_backends:\n",
    "    torch.backends.quantized.engine = \"fbgemm\"\n",
    "else:\n",
    "    print(\"FBGEMM backend not available on this system.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Current Quantization Backend: {torch.backends.quantized.engine}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d9f0a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original CNN model inference time: 8.1330 seconds\n",
      "Quantized CNN model inference time: 0.5532 seconds\n",
      "Speedup with quantization: 14.70x\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define a CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        dummy_input = torch.zeros(1, 1, 28, 28)\n",
    "        with torch.no_grad():\n",
    "            flattened_size = self._get_flattened_size(dummy_input)\n",
    "\n",
    "        self.fc1 = nn.Linear(flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def _get_flattened_size(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        return x.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create a random input tensor (batch size 100, 1 channel, 28x28 image)\n",
    "input_data = torch.rand(100, 1, 28, 28)\n",
    "\n",
    "# Step 2: Initialize the model\n",
    "model = CNNModel()\n",
    "\n",
    "# Simulate training by setting to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Step 3: Measure inference time for the original model\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(100):  # Run inference 100 times\n",
    "        _ = model(input_data)\n",
    "original_inference_time = time.time() - start_time\n",
    "print(f\"Original CNN model inference time: {original_inference_time:.4f} seconds\")\n",
    "\n",
    "# Step 4: Quantize the model (Dynamic quantization for Linear layers)\n",
    "quantized_model = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "# Step 5: Measure inference time for the quantized model\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(100):  # Run inference 100 times\n",
    "        _ = quantized_model(input_data)\n",
    "quantized_inference_time = time.time() - start_time\n",
    "print(f\"Quantized CNN model inference time: {quantized_inference_time:.4f} seconds\")\n",
    "\n",
    "# Compare the results\n",
    "speedup = original_inference_time / quantized_inference_time\n",
    "print(f\"Speedup with quantization: {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773f371c",
   "metadata": {},
   "source": [
    "## qnnpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cfc2e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Quantization Backends: ['qnnpack', 'none', 'x86', 'fbgemm']\n",
      "Current Quantization Backend: qnnpack\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "\n",
    "# Check available backends\n",
    "available_backends = torch.backends.quantized.supported_engines\n",
    "print(f\"Available Quantization Backends: {available_backends}\")\n",
    "\n",
    "# Set the backend to FBGEMM (if available)\n",
    "if \"fbgemm\" in available_backends:\n",
    "    torch.backends.quantized.engine = \"qnnpack\"\n",
    "else:\n",
    "    print(\"FBGEMM backend not available on this system.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Current Quantization Backend: {torch.backends.quantized.engine}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d30844d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original CNN model inference time: 10.8331 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W qlinear_dynamic.cpp:247] Warning: Currently, qnnpack incorrectly ignores reduce_range when it is set to true; this may change in a future release. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized CNN model inference time: 8.6092 seconds\n",
      "Speedup with quantization: 1.26x\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define a CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        dummy_input = torch.zeros(1, 1, 28, 28)\n",
    "        with torch.no_grad():\n",
    "            flattened_size = self._get_flattened_size(dummy_input)\n",
    "\n",
    "        self.fc1 = nn.Linear(flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def _get_flattened_size(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        return x.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create a random input tensor (batch size 100, 1 channel, 28x28 image)\n",
    "input_data = torch.rand(100, 1, 28, 28)\n",
    "\n",
    "# Step 2: Initialize the model\n",
    "model = CNNModel()\n",
    "\n",
    "# Simulate training by setting to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Step 3: Measure inference time for the original model\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(100):  # Run inference 100 times\n",
    "        _ = model(input_data)\n",
    "original_inference_time = time.time() - start_time\n",
    "print(f\"Original CNN model inference time: {original_inference_time:.4f} seconds\")\n",
    "\n",
    "# Step 4: Quantize the model (Dynamic quantization for Linear layers)\n",
    "quantized_model = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "# Step 5: Measure inference time for the quantized model\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(100):  # Run inference 100 times\n",
    "        _ = quantized_model(input_data)\n",
    "quantized_inference_time = time.time() - start_time\n",
    "print(f\"Quantized CNN model inference time: {quantized_inference_time:.4f} seconds\")\n",
    "\n",
    "# Compare the results\n",
    "speedup = original_inference_time / quantized_inference_time\n",
    "print(f\"Speedup with quantization: {speedup:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e7ac71",
   "metadata": {},
   "source": [
    "## NONE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c71b0784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Quantization Backends: ['qnnpack', 'none', 'x86', 'fbgemm']\n",
      "Current Quantization Backend: none\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "\n",
    "# Check available backends\n",
    "available_backends = torch.backends.quantized.supported_engines\n",
    "print(f\"Available Quantization Backends: {available_backends}\")\n",
    "\n",
    "# Set the backend to FBGEMM (if available)\n",
    "if \"fbgemm\" in available_backends:\n",
    "    torch.backends.quantized.engine = \"none\"\n",
    "else:\n",
    "    print(\"FBGEMM backend not available on this system.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Current Quantization Backend: {torch.backends.quantized.engine}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa4f613c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original CNN model inference time: 8.9904 seconds\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Didn't find engine for operation quantized::linear_prepack NoQEngine",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/sraj/ipykernel_3632579/1997098938.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Step 4: Quantize the model (Dynamic quantization for Linear layers)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mquantized_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantize_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Step 5: Measure inference time for the quantized model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/ao/quantization/quantize.py\u001b[0m in \u001b[0;36mquantize_dynamic\u001b[0;34m(model, qconfig_spec, dtype, mapping, inplace)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0mpropagate_qconfig_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqconfig_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m     \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/ao/quantization/quantize.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m     _convert(\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_reference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_reference\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         convert_custom_config_dict=convert_custom_config_dict)\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/ao/quantization/quantize.py\u001b[0m in \u001b[0;36m_convert\u001b[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    588\u001b[0m             _convert(mod, mapping, True,  # inplace\n\u001b[1;32m    589\u001b[0m                      is_reference, convert_custom_config_dict)\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mreassign\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswap_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_module_class_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreassign\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/ao/quantization/quantize.py\u001b[0m in \u001b[0;36mswap_module\u001b[0;34m(mod, mapping, custom_module_class_mapping)\u001b[0m\n\u001b[1;32m    621\u001b[0m                 \u001b[0mnew_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_qparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m                 \u001b[0mnew_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m             \u001b[0mswapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/ao/nn/quantized/dynamic/modules/linear.py\u001b[0m in \u001b[0;36mfrom_float\u001b[0;34m(cls, mod)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported dtype specified for dynamic quantized Linear!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mqlinear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mqlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weight_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mqlinear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/ao/nn/quantized/dynamic/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias_, dtype)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;31m# We don't muck around with buffers or attributes or anything here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# to keep the module simple. *everything* is simply a Python attribute.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/ao/nn/quantized/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias_, dtype)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported dtype specified for quantized Linear!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_packed_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearPackedParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_packed_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weight_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/ao/nn/quantized/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mwq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weight_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/ao/nn/quantized/modules/linear.py\u001b[0m in \u001b[0;36mset_weight_bias\u001b[0;34m(self, weight, bias)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_weight_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqint8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_packed_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_prepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_packed_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_prepack_fp16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# We save the function ptr as the `op` attribute on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;31m# OpOverloadPacket to access it here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Didn't find engine for operation quantized::linear_prepack NoQEngine"
     ]
    }
   ],
   "source": [
    "# Step 1: Define a CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        dummy_input = torch.zeros(1, 1, 28, 28)\n",
    "        with torch.no_grad():\n",
    "            flattened_size = self._get_flattened_size(dummy_input)\n",
    "\n",
    "        self.fc1 = nn.Linear(flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def _get_flattened_size(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        return x.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create a random input tensor (batch size 100, 1 channel, 28x28 image)\n",
    "input_data = torch.rand(100, 1, 28, 28)\n",
    "\n",
    "# Step 2: Initialize the model\n",
    "model = CNNModel()\n",
    "\n",
    "# Simulate training by setting to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Step 3: Measure inference time for the original model\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(100):  # Run inference 100 times\n",
    "        _ = model(input_data)\n",
    "original_inference_time = time.time() - start_time\n",
    "print(f\"Original CNN model inference time: {original_inference_time:.4f} seconds\")\n",
    "\n",
    "# Step 4: Quantize the model (Dynamic quantization for Linear layers)\n",
    "quantized_model = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "# Step 5: Measure inference time for the quantized model\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for _ in range(100):  # Run inference 100 times\n",
    "        _ = quantized_model(input_data)\n",
    "quantized_inference_time = time.time() - start_time\n",
    "print(f\"Quantized CNN model inference time: {quantized_inference_time:.4f} seconds\")\n",
    "\n",
    "# Compare the results\n",
    "speedup = original_inference_time / quantized_inference_time\n",
    "print(f\"Speedup with quantization: {speedup:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903a808b",
   "metadata": {},
   "source": [
    "## 8bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "595257a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-30 23:50:42.985038: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-30 23:50:47.511159: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "79/79 [==============================] - 1s 2ms/step - loss: 0.7734 - accuracy: 0.5110\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.7128 - accuracy: 0.5180\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.7069 - accuracy: 0.5144\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.6970 - accuracy: 0.5294\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.6962 - accuracy: 0.5374\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5354\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.6906 - accuracy: 0.5422\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.6863 - accuracy: 0.5440\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.6865 - accuracy: 0.5494\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.6847 - accuracy: 0.5532\n",
      "INFO:tensorflow:Assets written to: /tmp/sraj/tmp120rjol7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/sraj/tmp120rjol7/assets\n",
      "/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:887: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2025-01-30 23:51:39.071369: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-01-30 23:51:39.071478: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-01-30 23:51:39.071797: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/sraj/tmp120rjol7\n",
      "2025-01-30 23:51:39.072855: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-01-30 23:51:39.072876: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/sraj/tmp120rjol7\n",
      "2025-01-30 23:51:39.078187: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2025-01-30 23:51:39.079229: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-01-30 23:51:39.126907: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/sraj/tmp120rjol7\n",
      "2025-01-30 23:51:39.139069: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 67274 microseconds.\n",
      "2025-01-30 23:51:39.159382: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic data\n",
    "x_train = np.random.rand(5000, 50).astype(np.float32)\n",
    "y_train = np.random.randint(0, 2, size=(5000, 1))\n",
    "\n",
    "# Define a bigger and more complex model\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_shape=(50,)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = create_model()\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=64)\n",
    "\n",
    "# Convert the model to TFLite with 8-bit quantization\n",
    "def quantize_model(model):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    \n",
    "    # Representative dataset function\n",
    "    def representative_data_gen():\n",
    "        for _ in range(200):\n",
    "            yield [np.random.rand(1, 50).astype(np.float32)]\n",
    "    \n",
    "    converter.representative_dataset = representative_data_gen\n",
    "    converter.target_spec.supported_types = [tf.int8]\n",
    "    converter.inference_input_type = tf.int8\n",
    "    converter.inference_output_type = tf.int8\n",
    "    tflite_quant_model = converter.convert()\n",
    "    \n",
    "    return tflite_quant_model\n",
    "\n",
    "quantized_model = quantize_model(model)\n",
    "\n",
    "# Save the quantized model\n",
    "with open(\"quantized_model.tflite\", \"wb\") as f:\n",
    "    f.write(quantized_model)\n",
    "\n",
    "print(\"Quantized model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0454bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "79/79 [==============================] - 1s 3ms/step - loss: 0.7904 - accuracy: 0.4954\n",
      "Epoch 2/15\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.7214 - accuracy: 0.5200\n",
      "Epoch 3/15\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.7163 - accuracy: 0.5040\n",
      "Epoch 4/15\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.7014 - accuracy: 0.5274\n",
      "Epoch 5/15\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.6966 - accuracy: 0.5258\n",
      "Epoch 6/15\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.6963 - accuracy: 0.5312\n",
      "Epoch 7/15\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.6944 - accuracy: 0.5358\n",
      "Epoch 8/15\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.6949 - accuracy: 0.5306\n",
      "Epoch 9/15\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.6854 - accuracy: 0.5556\n",
      "Epoch 10/15\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.6900 - accuracy: 0.5478\n",
      "Epoch 11/15\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.6850 - accuracy: 0.5534\n",
      "Epoch 12/15\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.6819 - accuracy: 0.5500\n",
      "Epoch 13/15\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.6792 - accuracy: 0.5608\n",
      "Epoch 14/15\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.6807 - accuracy: 0.5644\n",
      "Epoch 15/15\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.6780 - accuracy: 0.5656\n",
      "INFO:tensorflow:Assets written to: /tmp/sraj/tmp1g8ggn0h/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/sraj/tmp1g8ggn0h/assets\n",
      "/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:887: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2025-01-30 23:57:04.239282: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-01-30 23:57:04.239485: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-01-30 23:57:04.239678: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/sraj/tmp1g8ggn0h\n",
      "2025-01-30 23:57:04.241505: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-01-30 23:57:04.241527: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: /tmp/sraj/tmp1g8ggn0h\n",
      "2025-01-30 23:57:04.247310: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-01-30 23:57:04.306524: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/sraj/tmp1g8ggn0h\n",
      "2025-01-30 23:57:04.325628: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 85950 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model saved successfully!\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "Inference output: [[19]]\n",
      "Quantized model inference time: 0.09 ms\n",
      "Original model inference time: 107.65 ms\n",
      "Inference time reduction: 107.56 ms\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Generate synthetic data\n",
    "x_train = np.random.rand(5000, 50).astype(np.float32)\n",
    "y_train = np.random.randint(0, 2, size=(5000, 1))\n",
    "\n",
    "# Define a more complex model with advanced architecture\n",
    "def create_model():\n",
    "    inputs = tf.keras.Input(shape=(50,))\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = create_model()\n",
    "model.fit(x_train, y_train, epochs=15, batch_size=64)\n",
    "\n",
    "# Convert the model to TFLite with 8-bit quantization\n",
    "def quantize_model(model):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    \n",
    "    # Representative dataset function\n",
    "    def representative_data_gen():\n",
    "        for _ in range(200):\n",
    "            yield [np.random.rand(1, 50).astype(np.float32)]\n",
    "    \n",
    "    converter.representative_dataset = representative_data_gen\n",
    "    converter.target_spec.supported_types = [tf.int8]\n",
    "    converter.inference_input_type = tf.int8\n",
    "    converter.inference_output_type = tf.int8\n",
    "    tflite_quant_model = converter.convert()\n",
    "    \n",
    "    return tflite_quant_model\n",
    "\n",
    "quantized_model = quantize_model(model)\n",
    "\n",
    "# Save the quantized model\n",
    "with open(\"quantized_model.tflite\", \"wb\") as f:\n",
    "    f.write(quantized_model)\n",
    "\n",
    "print(\"Quantized model saved successfully!\")\n",
    "\n",
    "# Load and test inference\n",
    "interpreter = tf.lite.Interpreter(model_path=\"quantized_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Run inference on a random sample and measure time\n",
    "input_data = np.random.rand(1, 50).astype(np.int8)\n",
    "\n",
    "# Measure baseline inference time\n",
    "start_time = time.time()\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "interpreter.invoke()\n",
    "quantized_inference_time = (time.time() - start_time) * 1000  # Convert to milliseconds\n",
    "\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "# Run inference on the original model for comparison\n",
    "start_time = time.time()\n",
    "model.predict(input_data.astype(np.float32))\n",
    "original_inference_time = (time.time() - start_time) * 1000  # Convert to milliseconds\n",
    "\n",
    "print(\"Inference output:\", output_data)\n",
    "print(f\"Quantized model inference time: {quantized_inference_time:.2f} ms\")\n",
    "print(f\"Original model inference time: {original_inference_time:.2f} ms\")\n",
    "print(f\"Inference time reduction: {(original_inference_time - quantized_inference_time):.2f} ms\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
