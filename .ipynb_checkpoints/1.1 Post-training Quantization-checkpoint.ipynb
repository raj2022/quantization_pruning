{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0710476",
   "metadata": {},
   "source": [
    "Source: https://www.tensorflow.org/model_optimization/guide/quantization/post_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a85232",
   "metadata": {},
   "source": [
    "Post-training quantization includes general techniques to reduce CPU and hardware accelerator latency, processing, power, and model size with little degradation in model accuracy. These techniques can be performed on an already-trained float TensorFlow model and applied during TensorFlow Lite conversion. These techniques are enabled as options in the TensorFlow Lite converter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5220e672",
   "metadata": {},
   "source": [
    "## Quantizing Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c5ced9",
   "metadata": {},
   "source": [
    "Weights can be converted to types with reduced precision, such as 16 bit floats or 8 bit integers. We generally recommend 16-bit floats for GPU acceleration and 8-bit integer for CPU execution.\n",
    "\n",
    "For example, here is how to specify 8 bit integer weight quantization:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d8ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.optimizationstimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85a76c3",
   "metadata": {},
   "source": [
    "## Full integer quantization of weights and activations\n",
    "Improve latency, processing, and power usage, and get access to integer-only hardware accelerators by making sure both weights and activations are quantized. This requires a small representative data set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f91e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    for _ in range(num_calibration_steps):\n",
    "        # Get sample input data as a numpy array in a method of your choosing.\n",
    "        yield [input]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5fc7f6",
   "metadata": {},
   "source": [
    "Source: https://www.tensorflow.org/lite/performance/post_training_quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb9fb54",
   "metadata": {},
   "source": [
    "Post-training quantization is a conversion technique that can reduce model size while also improving CPU and hardware accelerator latency, with little degradation in model accuracy. You can quantize an already-trained float TensorFlow model when you convert it to TensorFlow Lite format using the [TensorFlow Lite Converter](https://www.tensorflow.org/lite/models/convert/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6610a832",
   "metadata": {},
   "source": [
    "### Optimization methods\n",
    "There are several post-training quantization options to choose from. Here is a summary table of the choices and the benefits they provide:\n",
    "\n",
    "| Model Technique         | Benefits                            | Hardware                                    |\n",
    "|-------------------------|------------------------------------|---------------------------------------------|\n",
    "| Dynamic range quantization | 4x smaller, 2x-3x speedup       | CPU                                         |\n",
    "| Full integer quantization  | 4x smaller, 3x+ speedup         | CPU, Edge TPU, Microcontrollers             |\n",
    "| Float16 quantization      | 2x smaller, GPU acceleration   | CPU, GPU                                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a455d67a",
   "metadata": {},
   "source": [
    "The following decision tree can help determine which post-training quantization method is best for your use case:\n",
    "![images_1](https://www.tensorflow.org/static/lite/performance/images/optimization.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f312c866",
   "metadata": {},
   "source": [
    "### Dynamic range quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55df3e9b",
   "metadata": {},
   "source": [
    "Dynamic range quantization is a recommended starting point because it provides reduced memory usage and faster computation without you having to provide a representative dataset for calibration. This type of quantization, statically quantizes only the weights from floating point to integer at conversion time, which provides 8-bits of precision:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f742b95",
   "metadata": {},
   "source": [
    "To further reduce latency during inference, \"dynamic-range\" operators dynamically quantize activations based on their range to 8-bits and perform computations with 8-bit weights and activations. This optimization provides latencies close to fully fixed-point inferences. However, the outputs are still stored using floating point so the increased speed of dynamic-range ops is less than a full fixed-point computation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d444e13f",
   "metadata": {},
   "source": [
    "### Full integer quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b023befc",
   "metadata": {},
   "source": [
    "You can get further latency improvements, reductions in peak memory usage, and compatibility with integer only hardware devices or accelerators by making s\n",
    "ure all model math is integer quantized.\n",
    "\n",
    "\n",
    "For full integer quantization, you need to calibrate or estimate the range, i.e, (min, max) of all floating-point tensors in the model. Unlike constant tensors such as weights and biases, variable tensors such as model input, activations (outputs of intermediate layers) and model output cannot be calibrated unless we run a few inference cycles. As a result, the converter requires a representative dataset to calibrate them. This dataset can be a small subset (around ~100-500 samples) of the training or validation data. Refer to the representative_dataset() function below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ade3c8",
   "metadata": {},
   "source": [
    "```python\n",
    "def representative_dataset():\n",
    "  for data in dataset:\n",
    "    yield {\n",
    "      \"image\": data.image,\n",
    "      \"bias\": data.bias,\n",
    "    }\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95d211c",
   "metadata": {},
   "source": [
    "If there are more than one signature in the given TensorFlow model, you can specify the multiple dataset by specifying the signature keys:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186f6018",
   "metadata": {},
   "source": [
    "```python\n",
    "def representative_dataset():\n",
    "  # Feed data set for the \"encode\" signature.\n",
    "  for data in encode_signature_dataset:\n",
    "    yield (\n",
    "      \"encode\", {\n",
    "        \"image\": data.image,\n",
    "        \"bias\": data.bias,\n",
    "      }\n",
    "    )\n",
    "\n",
    "  # Feed data set for the \"decode\" signature.\n",
    "  for data in decode_signature_dataset:\n",
    "    yield (\n",
    "      \"decode\", {\n",
    "        \"image\": data.image,\n",
    "        \"hint\": data.hint,\n",
    "      },\n",
    "    ) \n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb93c6e7",
   "metadata": {},
   "source": [
    "You can generate the representative dataset by providing an input tensor list:\n",
    "\n",
    "```python\n",
    "def representative_dataset():\n",
    "  for data in tf.data.Dataset.from_tensor_slices((images)).batch(1).take(100):\n",
    "    yield [tf.dtypes.cast(data, tf.float32)]\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7288006a",
   "metadata": {},
   "source": [
    "For testing purposes, you can use a dummy dataset as follows:\n",
    "```python\n",
    "def representative_dataset():\n",
    "    for _ in range(100):\n",
    "      data = np.random.rand(1, 244, 244, 3)\n",
    "      yield [data.astype(np.float32)]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e578d80b",
   "metadata": {},
   "source": [
    "#### Integer with float fallback (using default float input/output)\n",
    "\n",
    "In order to fully integer quantize a model, but use float operators when they don't have an integer implementation (to ensure conversion occurs smoothly), use the following steps:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73155bec",
   "metadata": {},
   "source": [
    "```python\n",
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "tflite_quant_model = converter.convert() \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1822401",
   "metadata": {},
   "source": [
    "#### Integer only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2ad57c",
   "metadata": {},
   "source": [
    "Creating integer only models is a common use case for [TensorFlow Lite for Microcontrollers](https://www.tensorflow.org/lite/microcontrollers) and [Coral Edge TPUs](https://coral.ai/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e243fbc",
   "metadata": {},
   "source": [
    "Additionally, to ensure compatibility with integer only devices (such as 8-bit microcontrollers) and accelerators (such as the Coral Edge TPU), you can enforce full integer quantization for all ops including the input and output, by using the following steps:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797b54bc",
   "metadata": {},
   "source": [
    "```python\n",
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "tflite_quant_model = converter.convert()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6975b14c",
   "metadata": {},
   "source": [
    "### Float16 quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47332170",
   "metadata": {},
   "source": [
    "You can reduce the size of a floating point model by quantizing the weights to float16, the IEEE standard for 16-bit floating point numbers. To enable float16 quantization of weights, use the following steps:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b143dac9",
   "metadata": {},
   "source": [
    "```python\n",
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_quant_model = converter.convert()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da87a5f2",
   "metadata": {},
   "source": [
    "The advantages of float16 quantization are as follows:\n",
    "\n",
    "* It reduces model size by up to half (since all weights become half of their original size).\n",
    "* It causes minimal loss in accuracy.\n",
    "* It supports some delegates (e.g. the GPU delegate) which can operate directly on float16 data, resulting in faster execution than float32 computations.\n",
    "\n",
    "The disadvantages of float16 quantization are as follows:\n",
    "\n",
    "* It does not reduce latency as much as a quantization to fixed point math.\n",
    "* By default, a float16 quantized model will \"dequantize\" the weights values to float32 when run on the CPU. (Note that the GPU delegate will not perform this dequantization, since it can operate on float16 data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfd642d",
   "metadata": {},
   "source": [
    "### Integer only: 16-bit activations with 8-bit weights (experimental)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e9538",
   "metadata": {},
   "source": [
    "This is an experimental quantization scheme. It is similar to the \"integer only\" scheme, but activations are quantized based on their range to 16-bits, weights are quantized in 8-bit integer and bias is quantized into 64-bit integer. This is referred to as 16x8 quantization further.\n",
    "\n",
    "The main advantage of this quantization is that it can improve accuracy significantly, but only slightly increase model size.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c255153a",
   "metadata": {},
   "source": [
    "```python\n",
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]\n",
    "tflite_quant_model = converter.convert()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d324b06",
   "metadata": {},
   "source": [
    "If 16x8 quantization is not supported for some operators in the model, then the model still can be quantized, but unsupported operators kept in float. The following option should be added to the target_spec to allow this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c6615e",
   "metadata": {},
   "source": [
    "```python\n",
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n",
    "tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "tflite_quant_model = converter.convert()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e7f6f1",
   "metadata": {},
   "source": [
    "Examples of the use cases where accuracy improvements provided by this quantization scheme include:\n",
    "\n",
    "* super-resolution,\n",
    "* audio signal processing such as noise cancelling and beamforming,\n",
    "* image de-noising,\n",
    "* HDR reconstruction from a single image.\n",
    "\n",
    "The disadvantage of this quantization is:\n",
    "\n",
    "* Currently inference is noticeably slower than 8-bit full integer due to the lack of optimized kernel implementation.\n",
    "* Currently it is incompatible with the existing hardware accelerated TFLite delegates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b018f7",
   "metadata": {},
   "source": [
    "A tutorial for this quantization mode can be found [here](https://www.tensorflow.org/lite/performance/post_training_integer_quant_16x8).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6516d43",
   "metadata": {},
   "source": [
    "### Model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c8fe03",
   "metadata": {},
   "source": [
    "Since weights are quantized post training, there could be an accuracy loss, particularly for smaller networks. Pre-trained fully quantized models are provided for specific networks on [TensorFlow Hub](https://tfhub.dev/s?deployment-format=lite&q=quantized). It is important to check the accuracy of the quantized model to verify that any degradation in accuracy is within acceptable limits. There are tools to evaluate [TensorFlow Lite model accuracy](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/evaluation/tasks).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96608d15",
   "metadata": {},
   "source": [
    "Alternatively, if the accuracy drop is too high, consider using [quantization aware training](https://www.tensorflow.org/model_optimization/guide/quantization/training) . However, doing so requires modifications during model training to add fake quantization nodes, whereas the post-training quantization techniques on this page use an existing pre-trained model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e43da4",
   "metadata": {},
   "source": [
    "### Representation for quantized tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263c35df",
   "metadata": {},
   "source": [
    "8-bit quantization approximates floating point values using the following formula.\n",
    "\n",
    "\\[ real_value = (int8_value - zero_point) \\times scale \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec533409",
   "metadata": {},
   "source": [
    "The representation has two main parts:\n",
    "\n",
    "* Per-axis (aka per-channel) or per-tensor weights represented by int8 two’s complement values in the range [-127, 127] with zero-point equal to 0.\n",
    "\n",
    "* Per-tensor activations/inputs represented by int8 two’s complement values in the range [-128, 127], with a zero-point in range [-128, 127].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81638193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
