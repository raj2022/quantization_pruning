{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb92308e",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec43f0e8",
   "metadata": {},
   "source": [
    "[TensorFlow Lite](https://www.tensorflow.org/lite/) now supports converting activations to 16-bit integer values and weights to 8-bit integer values during model conversion from TensorFlow to TensorFlow Lite's flat buffer format. We refer to this mode as the \"16x8 quantization mode\". This mode can improve accuracy of the quantized model significantly, when activations are sensitive to the quantization, while still achieving almost 3-4x reduction in model size. Moreover, this fully quantized model can be consumed by integer-only hardware accelerators.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384b8d93",
   "metadata": {},
   "source": [
    "Some examples of models that benefit from this mode of the post-training quantization include:\n",
    "\n",
    "* super-resolution,\n",
    "* audio signal processing such as noise cancelling and beamforming,\n",
    "* image de-noising,\n",
    "* HDR reconstruction from a single image\n",
    "\n",
    "In this tutorial, you train an MNIST model from scratch, check its accuracy in TensorFlow, and then convert the model into a Tensorflow Lite flatbuffer using this mode. At the end you check the accuracy of the converted model and compare it to the original float32 model. Note that this example demonstrates the usage of this mode and doesn't show benefits over other available quantization techniques in TensorFlow Lite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc954ba",
   "metadata": {},
   "source": [
    "## Build an MNIST model\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6bd63fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470d4e33",
   "metadata": {},
   "source": [
    "Check that the 16x8 quantization mode is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8: 'EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795d2715",
   "metadata": {},
   "source": [
    "## Train and export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "407c0d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 10s 3ms/step - loss: 0.2772 - accuracy: 0.9218 - val_loss: 0.1344 - val_accuracy: 0.9619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9dbc60bdc0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Define the model architecture\n",
    "model = keras.Sequential([\n",
    "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation=tf.nn.relu),\n",
    "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  epochs=1,\n",
    "  validation_data=(test_images, test_labels)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d424b2f1",
   "metadata": {},
   "source": [
    "For the example, you trained the model for just a single epoch, so it only trains to ~96% accuracy.\n",
    "\n",
    "## Convert to a TensorFlow Lite model\n",
    "Using the TensorFlow Lite Converter, you can now convert the trained model into a TensorFlow Lite model.\n",
    "\n",
    "Now, convert the model using TFliteConverter into default float32 format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fb6c7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 23:04:42.796076: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/sraj/tmpi_2szbt8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 23:04:45.602348: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-10-17 23:04:45.602419: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964aa44e",
   "metadata": {},
   "source": [
    "Write it out to a `.tflite` file:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fca83962",
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_models_dir = pathlib.Path(\"/tmp/mnist_tflite_models/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4729df0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e372f38",
   "metadata": {},
   "source": [
    "To instead quantize the model to 16x8 `quantization` mode, first set the optimizations flag to use default optimizations. Then specify that 16x8 quantization mode is the required supported operation in the target specification:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "722635fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ed1341",
   "metadata": {},
   "source": [
    "As in the case of int8 post-training quantization, it is possible to produce a fully integer quantized model by setting converter options `inference_input(output)_type` to tf.int16.\n",
    "\n",
    "Set the calibration data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5f957e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, _ = tf.keras.datasets.mnist.load_data()\n",
    "images = tf.cast(mnist_train[0], tf.float32) / 255.0\n",
    "mnist_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\n",
    "def representative_data_gen():\n",
    "  for input_value in mnist_ds.take(100):\n",
    "    # Model has only one input so each data point has one element.\n",
    "    yield [input_value]\n",
    "converter.representative_dataset = representative_data_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9fa967",
   "metadata": {},
   "source": [
    "Finally, convert the model as usual. Note, by default the converted model will still use float input and outputs for invocation convenience.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cb7f705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/sraj/tmp6b7tsc2k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/sraj/tmp6b7tsc2k/assets\n",
      "2023-10-17 23:07:51.567779: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-10-17 23:07:51.568371: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24640"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_16x8_model = converter.convert()\n",
    "tflite_model_16x8_file = tflite_models_dir/\"mnist_model_quant_16x8.tflite\"\n",
    "tflite_model_16x8_file.write_bytes(tflite_16x8_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0aca79",
   "metadata": {},
   "source": [
    "Note how the resulting file is approximately `1/3` the size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3086e9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 136K\r\n",
      "-rw-r--r--. 1 sraj zh 25K Oct 17 23:07 mnist_model_quant_16x8.tflite\r\n",
      "-rw-r--r--. 1 sraj zh 24K Oct 14 09:12 mnist_model_quant.tflite\r\n",
      "-rw-r--r--. 1 sraj zh 83K Oct 17 23:05 mnist_model.tflite\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh {tflite_models_dir}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498922c",
   "metadata": {},
   "source": [
    "# Run the TensorFlow Lite models\n",
    "Run the TensorFlow Lite model using the Python TensorFlow Lite Interpreter.\n",
    "\n",
    "### Load the model into the interpreters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d11df3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
    "interpreter.allocate_tensors()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d3523b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter_16x8 = tf.lite.Interpreter(model_path=str(tflite_model_16x8_file))\n",
    "interpreter_16x8.allocate_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baf1f7b",
   "metadata": {},
   "source": [
    "# Test the models on one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f2a8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = np.expand_dims(test_images[0], axis=0).astype(np.float32)\n",
    "\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "interpreter.set_tensor(input_index, test_image)\n",
    "interpreter.invoke()\n",
    "predictions = interpreter.get_tensor(output_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "434aad7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "plt.imshow(test_images[0])\n",
    "template = \"True:{true}, predicted:{predict}\"\n",
    "_ = plt.title(template.format(true= str(test_labels[0]),\n",
    "                              predict=str(np.argmax(predictions[0]))))\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2303744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = np.expand_dims(test_images[0], axis=0).astype(np.float32)\n",
    "\n",
    "input_index = interpreter_16x8.get_input_details()[0][\"index\"]\n",
    "output_index = interpreter_16x8.get_output_details()[0][\"index\"]\n",
    "\n",
    "interpreter_16x8.set_tensor(input_index, test_image)\n",
    "interpreter_16x8.invoke()\n",
    "predictions = interpreter_16x8.get_tensor(output_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2179f42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_images[0])\n",
    "template = \"True:{true}, predicted:{predict}\"\n",
    "_ = plt.title(template.format(true= str(test_labels[0]),\n",
    "                              predict=str(np.argmax(predictions[0]))))\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc409deb",
   "metadata": {},
   "source": [
    "### Evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb34b5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to evaluate the TF Lite model using \"test\" dataset.\n",
    "def evaluate_model(interpreter):\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on every image in the \"test\" dataset.\n",
    "  prediction_digits = []\n",
    "  for test_image in test_images:\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    digit = np.argmax(output()[0])\n",
    "    prediction_digits.append(digit)\n",
    "\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  accurate_count = 0\n",
    "  for index in range(len(prediction_digits)):\n",
    "    if prediction_digits[index] == test_labels[index]:\n",
    "      accurate_count += 1\n",
    "  accuracy = accurate_count * 1.0 / len(prediction_digits)\n",
    "\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89e6c077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9619\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(interpreter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59cc623",
   "metadata": {},
   "source": [
    "Repeat the evaluation on the 16x8 quantized model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "760db01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9618\n"
     ]
    }
   ],
   "source": [
    "# NOTE: This quantization mode is an experimental post-training mode,\n",
    "# it does not have any optimized kernels implementations or\n",
    "# specialized machine learning hardware accelerators. Therefore,\n",
    "# it could be slower than the float interpreter.\n",
    "print(evaluate_model(interpreter_16x8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c1d77d",
   "metadata": {},
   "source": [
    "In this example, you have quantized a model to 16x8 with no difference in the accuracy, but with the 3x reduced size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0179b334",
   "metadata": {},
   "source": [
    "Another a few different quantizations are:\n",
    "1. https://www.tensorflow.org/lite/performance/quantization_spec\n",
    "2. https://www.tensorflow.org/lite/performance/quantization_debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbad257c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
