{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0710476",
   "metadata": {},
   "source": [
    "Source: https://www.tensorflow.org/model_optimization/guide/quantization/post_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a85232",
   "metadata": {},
   "source": [
    "Post-training quantization includes general techniques to reduce CPU and hardware accelerator latency, processing, power, and model size with little degradation in model accuracy. These techniques can be performed on an already-trained float TensorFlow model and applied during TensorFlow Lite conversion. These techniques are enabled as options in the TensorFlow Lite converter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5220e672",
   "metadata": {},
   "source": [
    "## Quantizing Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c5ced9",
   "metadata": {},
   "source": [
    "Weights can be converted to types with reduced precision, such as 16 bit floats or 8 bit integers. We generally recommend 16-bit floats for GPU acceleration and 8-bit integer for CPU execution.\n",
    "\n",
    "For example, here is how to specify 8 bit integer weight quantization:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d8ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.optimizationstimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85a76c3",
   "metadata": {},
   "source": [
    "## Full integer quantization of weights and activations\n",
    "Improve latency, processing, and power usage, and get access to integer-only hardware accelerators by making sure both weights and activations are quantized. This requires a small representative data set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f91e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    for _ in range(num_calibration_steps):\n",
    "        # Get sample input data as a numpy array in a method of your choosing.\n",
    "        yield [input]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5fc7f6",
   "metadata": {},
   "source": [
    "Source: https://www.tensorflow.org/lite/performance/post_training_quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb9fb54",
   "metadata": {},
   "source": [
    "Post-training quantization is a conversion technique that can reduce model size while also improving CPU and hardware accelerator latency, with little degradation in model accuracy. You can quantize an already-trained float TensorFlow model when you convert it to TensorFlow Lite format using the [TensorFlow Lite Converter](https://www.tensorflow.org/lite/models/convert/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6610a832",
   "metadata": {},
   "source": [
    "### Optimization methods\n",
    "There are several post-training quantization options to choose from. Here is a summary table of the choices and the benefits they provide:\n",
    "\n",
    "| Model Technique         | Benefits                            | Hardware                                    |\n",
    "|-------------------------|------------------------------------|---------------------------------------------|\n",
    "| Dynamic range quantization | 4x smaller, 2x-3x speedup       | CPU                                         |\n",
    "| Full integer quantization  | 4x smaller, 3x+ speedup         | CPU, Edge TPU, Microcontrollers             |\n",
    "| Float16 quantization      | 2x smaller, GPU acceleration   | CPU, GPU                                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a455d67a",
   "metadata": {},
   "source": [
    "The following decision tree can help determine which post-training quantization method is best for your use case:\n",
    "![images_1](https://www.tensorflow.org/static/lite/performance/images/optimization.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f312c866",
   "metadata": {},
   "source": [
    "### Dynamic range quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55df3e9b",
   "metadata": {},
   "source": [
    "Dynamic range quantization is a recommended starting point because it provides reduced memory usage and faster computation without you having to provide a representative dataset for calibration. This type of quantization, statically quantizes only the weights from floating point to integer at conversion time, which provides 8-bits of precision:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f742b95",
   "metadata": {},
   "source": [
    "To further reduce latency during inference, \"dynamic-range\" operators dynamically quantize activations based on their range to 8-bits and perform computations with 8-bit weights and activations. This optimization provides latencies close to fully fixed-point inferences. However, the outputs are still stored using floating point so the increased speed of dynamic-range ops is less than a full fixed-point computation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4e2f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
