{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d827d310",
   "metadata": {},
   "source": [
    "Task:\n",
    "1. Try to plot the Jet and MET pT response curve with increased statistics\n",
    " * each event should have multiple jets so 1000 events -> few thousand jets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113ecb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.30/02\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import ROOT\n",
    "\n",
    "import vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d985af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import tensorflow_datasets as tfds\n",
    "import torch_geometric\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a55701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08a0192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "data_dir = \"../../mlpf/tensorflow_datasets/\"\n",
    "dataset = \"clic_edm_ttbar_pf\"\n",
    "\n",
    "# Load dataset\n",
    "builder = tfds.builder(dataset, data_dir=data_dir)\n",
    "ds_train = builder.as_data_source(split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efc1998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ea4423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_FEATURES_TRK = [\n",
    "    \"elemtype\",\n",
    "    \"pt\",\n",
    "    \"eta\",\n",
    "    \"sin_phi\",\n",
    "    \"cos_phi\",\n",
    "    \"p\",\n",
    "    \"chi2\",\n",
    "    \"ndf\",\n",
    "    \"dEdx\",\n",
    "    \"dEdxError\",\n",
    "    \"radiusOfInnermostHit\",\n",
    "    \"tanLambda\",\n",
    "    \"D0\",\n",
    "    \"omega\",\n",
    "    \"Z0\",\n",
    "    \"time\",\n",
    "]\n",
    "X_FEATURES_CL = [\n",
    "    \"elemtype\",\n",
    "    \"et\",\n",
    "    \"eta\",\n",
    "    \"sin_phi\",\n",
    "    \"cos_phi\",\n",
    "    \"energy\",\n",
    "    \"position.x\",\n",
    "    \"position.y\",\n",
    "    \"position.z\",\n",
    "    \"iTheta\",\n",
    "    \"energy_ecal\",\n",
    "    \"energy_hcal\",\n",
    "    \"energy_other\",\n",
    "    \"num_hits\",\n",
    "    \"sigma_x\",\n",
    "    \"sigma_y\",\n",
    "    \"sigma_z\",\n",
    "]\n",
    "Y_FEATURES = [\"cls_id\", \"charge\", \"pt\", \"eta\", \"sin_phi\", \"cos_phi\", \"energy\"]\n",
    "Y_CLASSES = [0, 211, 130, 22, 11, 13]\n",
    "\n",
    "INPUT_DIM = max(len(X_FEATURES_TRK), len(X_FEATURES_CL))\n",
    "NUM_CLASSES = len(Y_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8f1650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JP 2024-02-29: currently torch int8 onnx export does not work with MultiheadAttention because of the following:\n",
    "# - it uses q_scaling_product.mul_scalar which is not supported in ONNX opset 17: the fix is to just remove the q_scaling_product\n",
    "# - somehow, the \"need_weights\" option confuses the ONNX exporter because the multiheaded attention layer then returns a tuple: the fix is to make the MHA always return just the attended values only\n",
    "# I lifted these two modules directly from the pytorch code and made the modifications here.\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as nnF\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class QuantizeableMultiheadAttention(nn.MultiheadAttention):\n",
    "    _FLOAT_MODULE = nn.MultiheadAttention\n",
    "\n",
    "    r\"\"\"Quantizable implementation of the MultiheadAttention.\n",
    "\n",
    "    Note::\n",
    "        Please, refer to :class:`~torch.nn.MultiheadAttention` for more\n",
    "        information\n",
    "\n",
    "    Allows the model to jointly attend to information from different\n",
    "    representation subspaces.\n",
    "    See reference: Attention Is All You Need\n",
    "\n",
    "    The original MHA module is not quantizable.\n",
    "    This reimplements it by explicitly instantiating the linear layers.\n",
    "\n",
    "    .. math::\n",
    "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
    "        \\text{where} head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "\n",
    "    Args:\n",
    "        embed_dim: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n",
    "        bias: add bias as module parameter. Default: True.\n",
    "        add_bias_kv: add bias to the key and value sequences at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        kdim: total number of features in key. Default: None.\n",
    "        vdim: total number of features in value. Default: None.\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
    "\n",
    "    Note that if :attr:`kdim` and :attr:`vdim` are None, they will be set\n",
    "    to :attr:`embed_dim` such that query, key, and value have the same\n",
    "    number of features.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> import torch.ao.nn.quantizable as nnqa\n",
    "        >>> multihead_attn = nnqa.MultiheadAttention(embed_dim, num_heads)\n",
    "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "\n",
    "    Note::\n",
    "        Please, follow the quantization flow to convert the quantizable MHA.\n",
    "    \"\"\"\n",
    "    __constants__ = ['batch_first']\n",
    "\n",
    "    def __init__(self, embed_dim: int, num_heads: int,\n",
    "                 dropout: float = 0., bias: bool = True,\n",
    "                 add_bias_kv: bool = False, add_zero_attn: bool = False,\n",
    "                 kdim: Optional[int] = None, vdim: Optional[int] = None, batch_first: bool = False,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, num_heads, dropout,\n",
    "                         bias, add_bias_kv,\n",
    "                         add_zero_attn, kdim, vdim, batch_first,\n",
    "                         **factory_kwargs)\n",
    "        self.linear_Q = nn.Linear(self.embed_dim, self.embed_dim, bias=bias, **factory_kwargs)\n",
    "        self.linear_K = nn.Linear(self.kdim, self.embed_dim, bias=bias, **factory_kwargs)\n",
    "        self.linear_V = nn.Linear(self.vdim, self.embed_dim, bias=bias, **factory_kwargs)\n",
    "        # for the type: ignore, see https://github.com/pytorch/pytorch/issues/58969\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=bias, **factory_kwargs)  # type: ignore[assignment]\n",
    "\n",
    "        # Functionals\n",
    "        # self.q_scaling_product = torch.ao.nn.quantized.FloatFunctional()\n",
    "        # note: importing torch.ao.nn.quantized at top creates a circular import\n",
    "\n",
    "        # Quant/Dequant\n",
    "        self.quant_attn_output = torch.ao.quantization.QuantStub()\n",
    "        self.quant_attn_output_weights = torch.ao.quantization.QuantStub()\n",
    "        self.dequant_q = torch.ao.quantization.DeQuantStub()\n",
    "        self.dequant_k = torch.ao.quantization.DeQuantStub()\n",
    "        self.dequant_v = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def _get_name(self):\n",
    "        return 'QuantizableMultiheadAttention'\n",
    "\n",
    "    @classmethod\n",
    "    def from_float(cls, other):\n",
    "        assert type(other) == cls._FLOAT_MODULE\n",
    "        assert hasattr(other, 'qconfig'), \"The float module must have 'qconfig'\"\n",
    "        # Setting the dropout to 0.0!\n",
    "        observed = cls(other.embed_dim, other.num_heads, other.dropout,\n",
    "                       (other.in_proj_bias is not None),\n",
    "                       (other.bias_k is not None),\n",
    "                       other.add_zero_attn, other.kdim, other.vdim,\n",
    "                       other.batch_first)\n",
    "        observed.bias_k = other.bias_k\n",
    "        observed.bias_v = other.bias_v\n",
    "        observed.qconfig = other.qconfig\n",
    "\n",
    "        # Set the linear weights\n",
    "        # for the type: ignores, see https://github.com/pytorch/pytorch/issues/58969\n",
    "        observed.out_proj.weight = other.out_proj.weight  # type: ignore[has-type]\n",
    "        observed.out_proj.bias = other.out_proj.bias  # type: ignore[has-type]\n",
    "        if other._qkv_same_embed_dim:\n",
    "            # Use separate params\n",
    "            bias = other.in_proj_bias\n",
    "            _start = 0\n",
    "            _end = _start + other.embed_dim\n",
    "            weight = other.in_proj_weight[_start:_end, :]\n",
    "            if bias is not None:\n",
    "                bias = torch.nn.Parameter(bias[_start:_end], bias.requires_grad)\n",
    "            observed.linear_Q.weight = torch.nn.Parameter(weight,\n",
    "                                                          weight.requires_grad)\n",
    "            observed.linear_Q.bias = bias\n",
    "\n",
    "            bias = other.in_proj_bias\n",
    "            _start = _end\n",
    "            _end = _start + other.embed_dim\n",
    "            weight = other.in_proj_weight[_start:_end, :]\n",
    "            if bias is not None:\n",
    "                bias = torch.nn.Parameter(bias[_start:_end], bias.requires_grad)\n",
    "            observed.linear_K.weight = torch.nn.Parameter(weight,\n",
    "                                                          weight.requires_grad)\n",
    "            observed.linear_K.bias = bias\n",
    "\n",
    "            bias = other.in_proj_bias\n",
    "            _start = _end\n",
    "            weight = other.in_proj_weight[_start:, :]\n",
    "            if bias is not None:\n",
    "                bias = torch.nn.Parameter(bias[_start:], bias.requires_grad)\n",
    "            observed.linear_V.weight = torch.nn.Parameter(weight,\n",
    "                                                          weight.requires_grad)\n",
    "            observed.linear_V.bias = bias\n",
    "        else:\n",
    "            observed.linear_Q.weight = nn.Parameter(other.q_proj_weight)\n",
    "            observed.linear_K.weight = nn.Parameter(other.k_proj_weight)\n",
    "            observed.linear_V.weight = nn.Parameter(other.v_proj_weight)\n",
    "            if other.in_proj_bias is None:\n",
    "                observed.linear_Q.bias = None  # type: ignore[assignment]\n",
    "                observed.linear_K.bias = None  # type: ignore[assignment]\n",
    "                observed.linear_V.bias = None  # type: ignore[assignment]\n",
    "            else:\n",
    "                observed.linear_Q.bias = nn.Parameter(other.in_proj_bias[0:other.embed_dim])\n",
    "                observed.linear_K.bias = nn.Parameter(other.in_proj_bias[other.embed_dim:(other.embed_dim * 2)])\n",
    "                observed.linear_V.bias = nn.Parameter(other.in_proj_bias[(other.embed_dim * 2):])\n",
    "        observed.eval()\n",
    "        # Explicit prepare\n",
    "        observed = torch.ao.quantization.prepare(observed, inplace=True)\n",
    "        return observed\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def dequantize(self):\n",
    "        r\"\"\"Utility to convert the quantized MHA back to float.\n",
    "\n",
    "        The motivation for this is that it is not trivial to conver the weights\n",
    "        from the format that is used in the quantized version back to the\n",
    "        float.\n",
    "        \"\"\"\n",
    "        fp = self._FLOAT_MODULE(self.embed_dim, self.num_heads, self.dropout,\n",
    "                                (self.linear_Q._weight_bias()[1] is not None),\n",
    "                                (self.bias_k is not None),\n",
    "                                self.add_zero_attn, self.kdim, self.vdim, self.batch_first)\n",
    "        assert fp._qkv_same_embed_dim == self._qkv_same_embed_dim\n",
    "        if self.bias_k is not None:\n",
    "            fp.bias_k = nn.Parameter(self.bias_k.dequantize())\n",
    "        if self.bias_v is not None:\n",
    "            fp.bias_v = nn.Parameter(self.bias_v.dequantize())\n",
    "\n",
    "        # Set the linear weights\n",
    "        # Note: Because the linear layers are quantized, mypy does not nkow how\n",
    "        # to deal with them -- might need to ignore the typing checks.\n",
    "        # for the type: ignore[has-type], see https://github.com/pytorch/pytorch/issues/58969\n",
    "        w, b = self.out_proj._weight_bias()  # type: ignore[operator, has-type]\n",
    "        fp.out_proj.weight = nn.Parameter(w.dequantize())\n",
    "        if b is not None:\n",
    "            fp.out_proj.bias = nn.Parameter(b)\n",
    "\n",
    "        wQ, bQ = self.linear_Q._weight_bias()  # type: ignore[operator]\n",
    "        wQ = wQ.dequantize()\n",
    "        wK, bK = self.linear_K._weight_bias()  # type: ignore[operator]\n",
    "        wK = wK.dequantize()\n",
    "        wV, bV = self.linear_V._weight_bias()  # type: ignore[operator]\n",
    "        wV = wV.dequantize()\n",
    "        if fp._qkv_same_embed_dim:\n",
    "            # Use separate params\n",
    "            _start = 0\n",
    "            _end = _start + fp.embed_dim\n",
    "            fp.in_proj_weight[_start:_end, :] = wQ\n",
    "            if fp.in_proj_bias is not None:\n",
    "                assert all(bQ == 0)\n",
    "                fp.in_proj_bias[_start:_end] = bQ\n",
    "\n",
    "            _start = _end\n",
    "            _end = _start + fp.embed_dim\n",
    "            fp.in_proj_weight[_start:_end, :] = wK\n",
    "            if fp.in_proj_bias is not None:\n",
    "                assert all(bK == 0)\n",
    "                fp.in_proj_bias[_start:_end] = bK\n",
    "\n",
    "            _start = _end\n",
    "            fp.in_proj_weight[_start:, :] = wV\n",
    "            if fp.in_proj_bias is not None:\n",
    "                assert all(bV == 0)\n",
    "                fp.in_proj_bias[_start:] = bV\n",
    "        else:\n",
    "            fp.q_proj_weight = nn.Parameter(wQ)\n",
    "            fp.k_proj_weight = nn.Parameter(wK)\n",
    "            fp.v_proj_weight = nn.Parameter(wV)\n",
    "            if fp.in_proj_bias is None:\n",
    "                self.linear_Q.bias = None\n",
    "                self.linear_K.bias = None\n",
    "                self.linear_V.bias = None\n",
    "            else:\n",
    "                fp.in_proj_bias[0:fp.embed_dim] = bQ\n",
    "                fp.in_proj_bias[fp.embed_dim:(fp.embed_dim * 2)] = bK\n",
    "                fp.in_proj_bias[(fp.embed_dim * 2):] = bV\n",
    "\n",
    "        return fp\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_observed(cls, other):\n",
    "        # The whole flow is float -> observed -> quantized\n",
    "        # This class does float -> observed only\n",
    "        # See nn.quantized.MultiheadAttention\n",
    "        raise NotImplementedError(\"It looks like you are trying to prepare an \"\n",
    "                                  \"MHA module. Please, see \"\n",
    "                                  \"the examples on quantizable MHAs.\")\n",
    "\n",
    "    def forward(self,\n",
    "                query: Tensor,\n",
    "                key: Tensor,\n",
    "                value: Tensor,\n",
    "                key_padding_mask: Optional[Tensor] = None,\n",
    "                need_weights: bool = True,\n",
    "                attn_mask: Optional[Tensor] = None,\n",
    "                average_attn_weights: bool = True,\n",
    "                is_causal: bool = False) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        r\"\"\"\n",
    "    Note::\n",
    "        Please, refer to :func:`~torch.nn.MultiheadAttention.forward` for more\n",
    "        information\n",
    "\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. When given a binary mask and a value is True,\n",
    "            the corresponding value on the attention layer will be ignored.\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
    "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
    "\n",
    "    Shape:\n",
    "        - Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n",
    "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
    "          If a BoolTensor is provided, the positions with the\n",
    "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked\n",
    "          positions. If a BoolTensor is provided, positions with ``True``\n",
    "          is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "          is provided, it will be added to the attention weight.\n",
    "        - is_causal: If specified, applies a causal mask as attention mask. Mutually exclusive with providing attn_mask.\n",
    "          Default: ``False``.\n",
    "        - average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across\n",
    "          heads. Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an\n",
    "          effect when ``need_weights=True.``. Default: True (i.e. average weights across heads)\n",
    "\n",
    "        - Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n",
    "        - attn_output_weights: If ``average_attn_weights=True``, returns attention weights averaged\n",
    "          across heads of shape :math:`(N, L, S)`, where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. If ``average_attn_weights=False``, returns attention weights per\n",
    "          head of shape :math:`(N, num_heads, L, S)`.\n",
    "        \"\"\"\n",
    "        return self._forward_impl(query, key, value, key_padding_mask,\n",
    "                                  need_weights, attn_mask, average_attn_weights,\n",
    "                                  is_causal)\n",
    "\n",
    "    def _forward_impl(self,\n",
    "                      query: Tensor,\n",
    "                      key: Tensor,\n",
    "                      value: Tensor,\n",
    "                      key_padding_mask: Optional[Tensor] = None,\n",
    "                      need_weights: bool = True,\n",
    "                      attn_mask: Optional[Tensor] = None,\n",
    "                      average_attn_weights: bool = True,\n",
    "                      is_causal: bool = False) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        # This version will not deal with the static key/value pairs.\n",
    "        # Keeping it here for future changes.\n",
    "        #\n",
    "        # TODO: This method has some duplicate lines with the\n",
    "        # `torch.nn.functional.multi_head_attention`. Will need to refactor.\n",
    "        static_k = None\n",
    "        static_v = None\n",
    "\n",
    "        if attn_mask is not None and is_causal:\n",
    "            raise AssertionError(\"Only allow causal mask or attn_mask\")\n",
    "\n",
    "        if is_causal:\n",
    "            raise AssertionError(\"causal mask not supported by AO MHA module\")\n",
    "\n",
    "        if self.batch_first:\n",
    "            query, key, value = (x.transpose(0, 1) for x in (query, key, value))\n",
    "\n",
    "        tgt_len, bsz, embed_dim_to_check = query.size()\n",
    "        assert self.embed_dim == embed_dim_to_check\n",
    "        # allow MHA to have different sizes for the feature dimension\n",
    "        assert key.size(0) == value.size(0) and key.size(1) == value.size(1)\n",
    "\n",
    "        head_dim = self.embed_dim // self.num_heads\n",
    "        assert head_dim * self.num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        scaling = float(head_dim) ** -0.5\n",
    "\n",
    "        q = self.linear_Q(query)\n",
    "        k = self.linear_K(key)\n",
    "        v = self.linear_V(value)\n",
    "\n",
    "        #JP fix here: disabled this\n",
    "        # q = self.q_scaling_product.mul_scalar(q, scaling)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.uint8:\n",
    "                warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "                attn_mask = attn_mask.to(torch.bool)\n",
    "            assert attn_mask.is_floating_point() or attn_mask.dtype == torch.bool, \\\n",
    "                f'Only float and bool types are supported for attn_mask, not {attn_mask.dtype}'\n",
    "\n",
    "            if attn_mask.dim() == 2:\n",
    "                attn_mask = attn_mask.unsqueeze(0)\n",
    "                if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:\n",
    "                    raise RuntimeError('The size of the 2D attn_mask is not correct.')\n",
    "            elif attn_mask.dim() == 3:\n",
    "                if list(attn_mask.size()) != [bsz * self.num_heads, query.size(0), key.size(0)]:\n",
    "                    raise RuntimeError('The size of the 3D attn_mask is not correct.')\n",
    "            else:\n",
    "                raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "            # attn_mask's dim is 3 now.\n",
    "\n",
    "        # convert ByteTensor key_padding_mask to bool\n",
    "        if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n",
    "            warnings.warn(\"Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "            key_padding_mask = key_padding_mask.to(torch.bool)\n",
    "        if self.bias_k is not None and self.bias_v is not None:\n",
    "            if static_k is None and static_v is None:\n",
    "\n",
    "                # Explicitly assert that bias_k and bias_v are not None\n",
    "                # in a way that TorchScript can understand.\n",
    "                bias_k = self.bias_k\n",
    "                assert bias_k is not None\n",
    "                bias_v = self.bias_v\n",
    "                assert bias_v is not None\n",
    "\n",
    "                k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
    "                v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
    "                if attn_mask is not None:\n",
    "                    attn_mask = nnF.pad(attn_mask, (0, 1))\n",
    "                if key_padding_mask is not None:\n",
    "                    key_padding_mask = nnF.pad(key_padding_mask, (0, 1))\n",
    "            else:\n",
    "                assert static_k is None, \"bias cannot be added to static key.\"\n",
    "                assert static_v is None, \"bias cannot be added to static value.\"\n",
    "        else:\n",
    "            assert self.bias_k is None\n",
    "            assert self.bias_v is None\n",
    "\n",
    "        q = q.contiguous().view(tgt_len, bsz * self.num_heads, head_dim).transpose(0, 1)\n",
    "        if k is not None:\n",
    "            k = k.contiguous().view(-1, bsz * self.num_heads, head_dim).transpose(0, 1)\n",
    "        if v is not None:\n",
    "            v = v.contiguous().view(-1, bsz * self.num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "        if static_k is not None:\n",
    "            assert static_k.size(0) == bsz * self.num_heads\n",
    "            assert static_k.size(2) == head_dim\n",
    "            k = static_k\n",
    "\n",
    "        if static_v is not None:\n",
    "            assert static_v.size(0) == bsz * self.num_heads\n",
    "            assert static_v.size(2) == head_dim\n",
    "            v = static_v\n",
    "\n",
    "        src_len = k.size(1)\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            assert key_padding_mask.size(0) == bsz\n",
    "            assert key_padding_mask.size(1) == src_len\n",
    "\n",
    "        if self.add_zero_attn:\n",
    "            src_len += 1\n",
    "            k_zeros = torch.zeros((k.size(0), 1) + k.size()[2:])\n",
    "            if k.is_quantized:\n",
    "                k_zeros = torch.quantize_per_tensor(k_zeros, k.q_scale(), k.q_zero_point(), k.dtype)\n",
    "            k = torch.cat([k, k_zeros], dim=1)\n",
    "            v_zeros = torch.zeros((v.size(0), 1) + k.size()[2:])\n",
    "            if v.is_quantized:\n",
    "                v_zeros = torch.quantize_per_tensor(v_zeros, v.q_scale(), v.q_zero_point(), v.dtype)\n",
    "            v = torch.cat([v, v_zeros], dim=1)\n",
    "\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = nnF.pad(attn_mask, (0, 1))\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = nnF.pad(key_padding_mask, (0, 1))\n",
    "\n",
    "        # Leaving the quantized zone here\n",
    "        q = self.dequant_q(q)\n",
    "        k = self.dequant_k(k)\n",
    "        v = self.dequant_v(v)\n",
    "        attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "        assert list(attn_output_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                attn_output_weights.masked_fill_(attn_mask, float('-inf'))\n",
    "            else:\n",
    "                attn_output_weights += attn_mask\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            attn_output_weights = attn_output_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_output_weights = attn_output_weights.masked_fill(\n",
    "                key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "                float('-inf'),\n",
    "            )\n",
    "            attn_output_weights = attn_output_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        attn_output_weights = nnF.softmax(\n",
    "            attn_output_weights, dim=-1)\n",
    "        attn_output_weights = nnF.dropout(attn_output_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.bmm(attn_output_weights, v)\n",
    "        assert list(attn_output.size()) == [bsz * self.num_heads, tgt_len, head_dim]\n",
    "        if self.batch_first:\n",
    "            attn_output = attn_output.view(bsz, tgt_len, self.embed_dim)\n",
    "        else:\n",
    "            attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, self.embed_dim)\n",
    "\n",
    "        # Reentering the quantized zone\n",
    "        attn_output = self.quant_attn_output(attn_output)\n",
    "        # for the type: ignore[has-type], see https://github.com/pytorch/pytorch/issues/58969\n",
    "        attn_output = self.out_proj(attn_output)  # type: ignore[has-type]\n",
    "\n",
    "        #JP fix: removed need_weights part from here, return attn_output instead of tuple\n",
    "        return attn_output\n",
    "\n",
    "class QuantizedMultiheadAttention(QuantizeableMultiheadAttention):\n",
    "    _FLOAT_MODULE = torch.ao.nn.quantizable.MultiheadAttention\n",
    "\n",
    "    def _get_name(self):\n",
    "        return \"QuantizedMultiheadAttention\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_float(cls, other):\n",
    "        # The whole flow is float -> observed -> quantized\n",
    "        # This class does observed -> quantized only\n",
    "        raise NotImplementedError(\"It looks like you are trying to convert a \"\n",
    "                                  \"non-observed MHA module. Please, see \"\n",
    "                                  \"the examples on quantizable MHAs.\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_observed(cls, other):\n",
    "        converted = torch.ao.quantization.convert(other, mapping=None,\n",
    "                                                  inplace=False,\n",
    "                                                  remove_qconfig=True,\n",
    "                                                  convert_custom_config_dict=None)\n",
    "        converted.__class__ = cls\n",
    "        # Remove the parameters for the bias_k and bias_v to quantize them\n",
    "        # TODO: This is a potential source of accuracy drop.\n",
    "        #       quantized cat takes the scale and zp of the first\n",
    "        #       element, which might lose the precision in the bias_k\n",
    "        #       and the bias_v (which are cat'ed with k/v being first).\n",
    "        if converted.bias_k is not None:\n",
    "            bias_k = converted._parameters.pop('bias_k')\n",
    "            sc, zp = torch._choose_qparams_per_tensor(bias_k,\n",
    "                                                      reduce_range=False)\n",
    "            bias_k = torch.quantize_per_tensor(bias_k, sc, zp, torch.quint8)\n",
    "            setattr(converted, 'bias_k', bias_k)  # noqa: B010\n",
    "\n",
    "        if converted.bias_v is not None:\n",
    "            bias_v = converted._parameters.pop('bias_v')\n",
    "            sc, zp = torch._choose_qparams_per_tensor(bias_k,  # type: ignore[possibly-undefined]\n",
    "                                                      reduce_range=False)\n",
    "            bias_v = torch.quantize_per_tensor(bias_v, sc, zp, torch.quint8)\n",
    "            setattr(converted, 'bias_v', bias_v)  # noqa: B010\n",
    "\n",
    "        del converted.in_proj_weight\n",
    "        del converted.in_proj_bias\n",
    "\n",
    "        return converted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52bf9733",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss, as described in https://arxiv.org/abs/1708.02002.\n",
    "    It is essentially an enhancement to cross entropy loss and is\n",
    "    useful for classification tasks when there is a large class imbalance.\n",
    "    x is expected to contain raw, unnormalized scores for each class.\n",
    "    y is expected to contain class labels.\n",
    "    Shape:\n",
    "        - x: (batch_size, C) or (batch_size, C, d1, d2, ..., dK), K > 0.\n",
    "        - y: (batch_size,) or (batch_size, d1, d2, ..., dK), K > 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, alpha = None, gamma = 0.0, reduction = \"mean\", ignore_index = -100\n",
    "    ):\n",
    "        \"\"\"Constructor.\n",
    "        Args:\n",
    "            alpha (Tensor, optional): Weights for each class. Defaults to None.\n",
    "            gamma (float, optional): A constant, as described in the paper.\n",
    "                Defaults to 0.\n",
    "            reduction (str, optional): 'mean', 'sum' or 'none'.\n",
    "                Defaults to 'mean'.\n",
    "            ignore_index (int, optional): class label to ignore.\n",
    "                Defaults to -100.\n",
    "        \"\"\"\n",
    "        if reduction not in (\"mean\", \"sum\", \"none\"):\n",
    "            raise ValueError('Reduction must be one of: \"mean\", \"sum\", \"none\".')\n",
    "\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "        self.nll_loss = nn.NLLLoss(weight=alpha, reduction=\"none\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        arg_keys = [\"alpha\", \"gamma\", \"reduction\"]\n",
    "        arg_vals = [self.__dict__[k] for k in arg_keys]\n",
    "        arg_strs = [f\"{k}={v!r}\" for k, v in zip(arg_keys, arg_vals)]\n",
    "        arg_str = \", \".join(arg_strs)\n",
    "        return f\"{type(self).__name__}({arg_str})\"\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        if x.ndim > 2:\n",
    "            # (N, C, d1, d2, ..., dK) --> (N * d1 * ... * dK, C)\n",
    "            c = x.shape[1]\n",
    "            x = x.permute(0, *range(2, x.ndim), 1).reshape(-1, c)\n",
    "            # (N, d1, d2, ..., dK) --> (N * d1 * ... * dK,)\n",
    "            y = y.view(-1)\n",
    "\n",
    "        # compute weighted cross entropy term: -alpha * log(pt)\n",
    "        # (alpha is already part of self.nll_loss)\n",
    "        log_p = F.log_softmax(x, dim=-1)\n",
    "        ce = self.nll_loss(log_p, y)\n",
    "\n",
    "        # get true class column from each row\n",
    "        # this is slow due to indexing\n",
    "        # all_rows = torch.arange(len(x))\n",
    "        # log_pt = log_p[all_rows, y]\n",
    "        log_pt = torch.gather(log_p, 1, y.unsqueeze(axis=-1)).squeeze(axis=-1)\n",
    "\n",
    "        # compute focal term: (1 - pt)^gamma\n",
    "        pt = log_pt.exp()\n",
    "        focal_term = (1 - pt) ** self.gamma\n",
    "\n",
    "        # the full loss: -alpha * ((1 - pt)^gamma) * log(pt)\n",
    "        loss = focal_term * ce\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            loss = loss.sum()\n",
    "\n",
    "        return loss\n",
    "        \n",
    "def mlpf_loss(y, ypred, mask):\n",
    "    loss = {}\n",
    "    loss_obj_id = FocalLoss(gamma=2.0, reduction=\"none\")\n",
    "\n",
    "    msk_true_particle = torch.unsqueeze((y[\"cls_id\"] != 0).to(dtype=torch.float32), axis=-1)\n",
    "    nelem = torch.sum(mask)\n",
    "    npart = torch.sum(y[\"cls_id\"] != 0)\n",
    "    \n",
    "    ypred[\"momentum\"] = ypred[\"momentum\"] * msk_true_particle\n",
    "    y[\"momentum\"] = y[\"momentum\"] * msk_true_particle\n",
    "\n",
    "    ypred[\"cls_id_onehot\"] = ypred[\"cls_id_onehot\"].permute((0, 2, 1))\n",
    "\n",
    "    loss_classification = loss_obj_id(ypred[\"cls_id_onehot\"], y[\"cls_id\"]).reshape(y[\"cls_id\"].shape)\n",
    "    loss_regression = torch.nn.functional.huber_loss(ypred[\"momentum\"], y[\"momentum\"], reduction=\"none\")\n",
    "    \n",
    "    # average over all elements that were not padded\n",
    "    loss[\"Classification\"] = loss_classification.sum() / npart\n",
    "    \n",
    "    mom_normalizer = y[\"momentum\"][y[\"cls_id\"] != 0].std(axis=0)\n",
    "    reg_losses = loss_regression[y[\"cls_id\"] != 0]\n",
    "    # average over all true particles\n",
    "    loss[\"Regression\"] = (reg_losses / mom_normalizer).sum() / npart\n",
    "\n",
    "    px = ypred[\"momentum\"][..., 0:1] * ypred[\"momentum\"][..., 3:4] * msk_true_particle\n",
    "    py = ypred[\"momentum\"][..., 0:1] * ypred[\"momentum\"][..., 2:3] * msk_true_particle\n",
    "    pred_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)\n",
    "\n",
    "    px = y[\"momentum\"][..., 0:1] * y[\"momentum\"][..., 3:4] * msk_true_particle\n",
    "    py = y[\"momentum\"][..., 0:1] * y[\"momentum\"][..., 2:3] * msk_true_particle\n",
    "    true_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)\n",
    "    loss[\"MET\"] = torch.nn.functional.huber_loss(pred_met, true_met).mean()\n",
    "\n",
    "    loss[\"Total\"] = loss[\"Classification\"] + loss[\"Regression\"]\n",
    "    # loss[\"Total\"] += 0.1*loss[\"MET\"]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbf53040",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizeFeaturesStub(torch.ao.quantization.QuantStub):\n",
    "    def __init__(self, num_feats):\n",
    "        super().__init__()\n",
    "        self.num_feats = num_feats\n",
    "        self.quants = torch.nn.ModuleList()\n",
    "        for ifeat in range(self.num_feats):\n",
    "            self.quants.append(torch.ao.quantization.QuantStub())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.quants[ifeat](x[..., ifeat:ifeat+1]) for ifeat in range(self.num_feats)], axis=-1)\n",
    "        \n",
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim=128,\n",
    "        num_heads=2,\n",
    "        width=128,\n",
    "        dropout_mha=0.1,\n",
    "        dropout_ff=0.1,\n",
    "        attention_type=\"efficient\",\n",
    "    ):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "\n",
    "        self.attention_type = attention_type\n",
    "        self.act = nn.ReLU\n",
    "        self.mha = torch.nn.MultiheadAttention(embedding_dim, num_heads, dropout=dropout_mha, batch_first=True)\n",
    "        self.norm0 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.norm1 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.seq = torch.nn.Sequential(\n",
    "            nn.Linear(embedding_dim, width), self.act(), nn.Linear(width, embedding_dim), self.act()\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(dropout_ff)\n",
    "\n",
    "        self.add0 = torch.ao.nn.quantized.FloatFunctional()\n",
    "        self.add1 = torch.ao.nn.quantized.FloatFunctional()\n",
    "        self.mul = torch.ao.nn.quantized.FloatFunctional()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        mha_out = self.mha(x, x, x, need_weights=False)[0]\n",
    "        x = self.add0.add(x, mha_out)\n",
    "        x = self.norm0(x)\n",
    "        x = self.add1.add(x, self.seq(x))\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(x)\n",
    "        # x = self.mul.mul(x, mask.unsqueeze(-1))\n",
    "        return x\n",
    "\n",
    "class RegressionOutput(nn.Module):\n",
    "    def __init__(self, embed_dim, width, act, dropout):\n",
    "        super(RegressionOutput, self).__init__()\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "        self.nn = ffn(embed_dim, 1, width, act, dropout)\n",
    "\n",
    "    def forward(self, elems, x, orig_value):\n",
    "        nn_out = self.nn(x)\n",
    "        nn_out = self.dequant(nn_out)\n",
    "        return orig_value + nn_out\n",
    "\n",
    "def ffn(input_dim, output_dim, width, act, dropout):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, width),\n",
    "        act(),\n",
    "        torch.nn.LayerNorm(width),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(width, output_dim),\n",
    "    )\n",
    "\n",
    "def transform_batch(Xbatch):\n",
    "    Xbatch = Xbatch.clone()\n",
    "    Xbatch[..., 1] = torch.log(Xbatch[..., 1])\n",
    "    Xbatch[..., 5] = torch.log(Xbatch[..., 5])\n",
    "    Xbatch[torch.isnan(Xbatch)] = 0.0\n",
    "    Xbatch[torch.isinf(Xbatch)] = 0.0\n",
    "    return Xbatch\n",
    "    \n",
    "def unpack_target(y):\n",
    "    ret = {}\n",
    "    ret[\"cls_id\"] = y[..., 0].long()\n",
    "\n",
    "    for i, feat in enumerate(Y_FEATURES):\n",
    "        if i >= 2:  # skip the cls and charge as they are defined above\n",
    "            ret[feat] = y[..., i].to(dtype=torch.float32)\n",
    "    ret[\"phi\"] = torch.atan2(ret[\"sin_phi\"], ret[\"cos_phi\"])\n",
    "    \n",
    "    # note ~ momentum = [\"pt\", \"eta\", \"sin_phi\", \"cos_phi\", \"energy\"]\n",
    "    ret[\"momentum\"] = y[..., 2:7].to(dtype=torch.float32)\n",
    "    ret[\"p4\"] = torch.cat(\n",
    "        [ret[\"pt\"].unsqueeze(1), ret[\"eta\"].unsqueeze(1), ret[\"phi\"].unsqueeze(1), ret[\"energy\"].unsqueeze(1)], axis=1\n",
    "    )\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def unpack_predictions(preds):\n",
    "    ret = {}\n",
    "    ret[\"cls_id_onehot\"], ret[\"momentum\"] = preds\n",
    "\n",
    "    ret[\"pt\"] = ret[\"momentum\"][..., 0]\n",
    "    ret[\"eta\"] = ret[\"momentum\"][..., 1]\n",
    "    ret[\"sin_phi\"] = ret[\"momentum\"][..., 2]\n",
    "    ret[\"cos_phi\"] = ret[\"momentum\"][..., 3]\n",
    "    ret[\"energy\"] = ret[\"momentum\"][..., 4]\n",
    "\n",
    "    ret[\"cls_id\"] = torch.argmax(ret[\"cls_id_onehot\"], axis=-1)\n",
    "    ret[\"phi\"] = torch.atan2(ret[\"sin_phi\"], ret[\"cos_phi\"])\n",
    "    ret[\"p4\"] = torch.cat(\n",
    "        [\n",
    "            ret[\"pt\"].unsqueeze(axis=-1),\n",
    "            ret[\"eta\"].unsqueeze(axis=-1),\n",
    "            ret[\"phi\"].unsqueeze(axis=-1),\n",
    "            ret[\"energy\"].unsqueeze(axis=-1),\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "    return ret\n",
    "\n",
    "class MLPF(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=16,\n",
    "        num_classes=6,\n",
    "        num_convs=2,\n",
    "        dropout_ff=0.0,\n",
    "        dropout_conv_reg_mha=0.0,\n",
    "        dropout_conv_reg_ff=0.0,\n",
    "        dropout_conv_id_mha=0.0,\n",
    "        dropout_conv_id_ff=0.0,\n",
    "        num_heads=16,\n",
    "        head_dim=16,\n",
    "        elemtypes=[0,1,2],\n",
    "    ):\n",
    "        super(MLPF, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.act = nn.ReLU\n",
    "        self.elemtypes = elemtypes\n",
    "        self.num_elemtypes = len(self.elemtypes)\n",
    "\n",
    "        embedding_dim = num_heads * head_dim\n",
    "        width = num_heads * head_dim\n",
    "        \n",
    "        self.nn0_id = ffn(self.input_dim, embedding_dim, width, self.act, dropout_ff)\n",
    "        self.nn0_reg = ffn(self.input_dim, embedding_dim, width, self.act, dropout_ff)\n",
    "        \n",
    "        self.conv_id = nn.ModuleList()\n",
    "        self.conv_reg = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_convs):\n",
    "            self.conv_id.append(\n",
    "                SelfAttentionLayer(\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    width=width,\n",
    "                    dropout_mha=dropout_conv_id_mha,\n",
    "                    dropout_ff=dropout_conv_id_ff,\n",
    "                )\n",
    "            )\n",
    "            self.conv_reg.append(\n",
    "                SelfAttentionLayer(\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    width=width,\n",
    "                    dropout_mha=dropout_conv_reg_mha,\n",
    "                    dropout_ff=dropout_conv_reg_ff,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        decoding_dim = self.input_dim + embedding_dim\n",
    "\n",
    "        # DNN that acts on the node level to predict the PID\n",
    "        self.nn_id = ffn(decoding_dim, num_classes, width, self.act, dropout_ff)\n",
    "\n",
    "        # elementwise DNN for node momentum regression\n",
    "        embed_dim = decoding_dim + num_classes\n",
    "        self.nn_pt = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_eta = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_sin_phi = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_cos_phi = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_energy = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        \n",
    "        self.quant = QuantizeFeaturesStub(self.input_dim + len(self.elemtypes))\n",
    "        self.dequant_id = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, X_features, mask):\n",
    "        Xfeat_transformed = transform_batch(X_features)\n",
    "        Xfeat_normed = self.quant(Xfeat_transformed)\n",
    "\n",
    "        embeddings_id, embeddings_reg = [], []\n",
    "        embedding_id = self.nn0_id(Xfeat_normed)\n",
    "        embedding_reg = self.nn0_reg(Xfeat_normed)\n",
    "        for num, conv in enumerate(self.conv_id):\n",
    "            conv_input = embedding_id if num == 0 else embeddings_id[-1]\n",
    "            out_padded = conv(conv_input, mask)\n",
    "            embeddings_id.append(out_padded)\n",
    "        for num, conv in enumerate(self.conv_reg):\n",
    "            conv_input = embedding_reg if num == 0 else embeddings_reg[-1]\n",
    "            out_padded = conv(conv_input, mask)\n",
    "            embeddings_reg.append(out_padded)\n",
    "\n",
    "        final_embedding_id = torch.cat([Xfeat_normed] + [embeddings_id[-1]], axis=-1)\n",
    "        preds_id = self.nn_id(final_embedding_id)\n",
    "\n",
    "        final_embedding_reg = torch.cat([Xfeat_normed] + [embeddings_reg[-1]] + [preds_id], axis=-1)\n",
    "        preds_pt = self.nn_pt(X_features, final_embedding_reg, X_features[..., 1:2])\n",
    "        preds_eta = self.nn_eta(X_features, final_embedding_reg, X_features[..., 2:3])\n",
    "        preds_sin_phi = self.nn_sin_phi(X_features, final_embedding_reg, X_features[..., 3:4])\n",
    "        preds_cos_phi = self.nn_cos_phi(X_features, final_embedding_reg, X_features[..., 4:5])\n",
    "        preds_energy = self.nn_energy(X_features, final_embedding_reg, X_features[..., 5:6])\n",
    "        preds_momentum = torch.cat([preds_pt, preds_eta, preds_sin_phi, preds_cos_phi, preds_energy], axis=-1)\n",
    "        \n",
    "        preds_id = self.dequant_id(preds_id)\n",
    "        return preds_id, preds_momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20fdd196",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.34: : 16016it [50:34,  5.28it/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss=0.44\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "max_events_train = 10000\n",
    "max_events_eval = 10000\n",
    "events_per_batch = 1000\n",
    "nepochs = 1\n",
    "batch_size = 50  # Reduce batch size\n",
    "\n",
    "# Define your model and optimizer\n",
    "model = MLPF(input_dim=INPUT_DIM, num_classes=NUM_CLASSES).to(device=device)\n",
    "optimizer = torch.optim.Adam(lr=1e-4, params=model.parameters())\n",
    "\n",
    "# Define a function to stream data from the dataset\n",
    "def stream_data(dataset, batch_size):\n",
    "    current_index = 0\n",
    "    while current_index < len(dataset):\n",
    "        batch = []\n",
    "        for _ in range(batch_size):\n",
    "            if current_index >= len(dataset):\n",
    "                break\n",
    "            batch.append(dataset[current_index])\n",
    "            current_index += 1\n",
    "        yield batch\n",
    "\n",
    "\n",
    "# Training \n",
    "loss_vals_epochs = []\n",
    "best_loss = float('inf')\n",
    "patience = 3  # Number of epochs to wait for improvement\n",
    "no_improvement = 0\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    loss_vals_steps = []\n",
    "    \n",
    "    # Generate indices for batches\n",
    "    inds_train = range(0, max_events_train, events_per_batch * 10)\n",
    "    \n",
    "    # Create tqdm instance to show progress\n",
    "    data_stream = stream_data(ds_train, batch_size)\n",
    "    data_stream = tqdm.tqdm(data_stream, total=len(inds_train))\n",
    "    \n",
    "    for batch in data_stream:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        X_features = [torch.tensor(elem[\"X\"]).to(torch.float32) for elem in batch]\n",
    "        y_targets = [torch.tensor(elem[\"ygen\"]).to(torch.float32) for elem in batch]\n",
    "\n",
    "        X_features_padded = pad_sequence(X_features, batch_first=True).to(device=device)\n",
    "        y_targets_padded = pad_sequence(y_targets, batch_first=True).to(device=device)\n",
    "        mask = X_features_padded[:, :, 0] != 0\n",
    "\n",
    "        preds = model(X_features_padded, mask)\n",
    "        preds_unpacked = unpack_predictions(preds)\n",
    "        targets_unpacked = unpack_target(y_targets_padded)\n",
    "\n",
    "        loss = mlpf_loss(targets_unpacked, preds_unpacked, mask)\n",
    "        loss[\"Total\"].backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_vals_steps.append(loss[\"Total\"].detach().cpu().item())\n",
    "\n",
    "        # Update tqdm progress bar\n",
    "        data_stream.set_description(f\"Epoch {epoch}, Loss: {loss_vals_steps[-1]:.2f}\")\n",
    "\n",
    "    epoch_loss = np.mean(loss_vals_steps)\n",
    "    loss_vals_epochs.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch}, loss={epoch_loss:.2f}\")\n",
    "\n",
    "    # Check for improvement in validation loss\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        no_improvement = 0\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "\n",
    "    # If no improvement for 'patience' epochs, stop training\n",
    "    if no_improvement >= patience:\n",
    "        print(\"Early stopping: No improvement for\", patience, \"epochs\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdfa6a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_features 50\n",
      "y_features 50\n"
     ]
    }
   ],
   "source": [
    "#  print(ind+events_per_batch)\n",
    "# print(ind)\n",
    "# print(batch_inds)\n",
    "print(f'X_features',len(X_features))\n",
    "print(f'y_features',len(y_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2d1fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the model back on CPU\n",
    "model = model.to(device=\"cpu\")\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "ds_elems = [ds_train[i] for i in range(max_events_train, max_events_train + max_events_eval)]\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32  \n",
    "for i in range(0, len(ds_elems), batch_size):\n",
    "    batch_elems = ds_elems[i:i + batch_size]\n",
    "    # input features\n",
    "    X_features = [torch.tensor(elem[\"X\"]).to(torch.float32) for elem in batch_elems]\n",
    "    X_features_padded = pad_sequence(X_features, batch_first=True)\n",
    "    #  target labels\n",
    "    y_targets = [torch.tensor(elem[\"ygen\"]).to(torch.float32) for elem in batch_elems]\n",
    "    y_targets_padded = pad_sequence(y_targets, batch_first=True)\n",
    "    #  mask for the batch\n",
    "    mask = X_features_padded[:, :, 0] != 0\n",
    "    #  model prediction, loss computation\n",
    "    preds = model(X_features_padded, mask)\n",
    "    preds = preds[0].detach(), preds[1].detach()\n",
    "    # Update mask for the batch\n",
    "    mask = X_features_padded[:, :, 0] != 0\n",
    "    # Unpack predictions and targets for the batch\n",
    "    preds_unpacked = unpack_predictions(preds)\n",
    "    targets_unpacked = unpack_target(y_targets_padded)\n",
    "    # append to a list \n",
    "    all_preds.append(preds_unpacked)\n",
    "    all_targets.append(targets_unpacked)\n",
    "    # Compute loss for the batch\n",
    "    loss = mlpf_loss(targets_unpacked, preds_unpacked, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3b971cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2419cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "msk_true_particles = targets_unpacked[\"cls_id\"]!=0\n",
    "msk_pred_particles = preds_unpacked[\"cls_id\"]!=0\n",
    "\n",
    "\n",
    "pt_target = targets_unpacked[\"pt\"][msk_true_particles].numpy()\n",
    "pt_pred = preds_unpacked[\"pt\"][msk_true_particles].numpy()\n",
    "\n",
    "eta_target = targets_unpacked[\"eta\"][msk_true_particles].numpy()\n",
    "eta_pred = preds_unpacked[\"eta\"][msk_true_particles].numpy()\n",
    "\n",
    "sphi_target = targets_unpacked[\"sin_phi\"][msk_true_particles].numpy()\n",
    "sphi_pred = preds_unpacked[\"sin_phi\"][msk_true_particles].numpy()\n",
    "\n",
    "cphi_target = targets_unpacked[\"cos_phi\"][msk_true_particles].numpy()\n",
    "cphi_pred = preds_unpacked[\"cos_phi\"][msk_true_particles].numpy()\n",
    "\n",
    "energy_target = targets_unpacked[\"energy\"][msk_true_particles].numpy()\n",
    "energy_pred = preds_unpacked[\"energy\"][msk_true_particles].numpy()\n",
    "\n",
    "px = preds_unpacked[\"pt\"] * preds_unpacked[\"cos_phi\"] * msk_true_particles\n",
    "py = preds_unpacked[\"pt\"] * preds_unpacked[\"sin_phi\"] * msk_true_particles\n",
    "pred_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)\n",
    "\n",
    "px = targets_unpacked[\"pt\"] * targets_unpacked[\"cos_phi\"] * msk_true_particles\n",
    "py = targets_unpacked[\"pt\"] * targets_unpacked[\"sin_phi\"] * msk_true_particles\n",
    "true_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "408088f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cls_id': tensor([[4, 1, 1,  ..., 0, 0, 0],\n",
       "         [5, 1, 1,  ..., 0, 0, 0],\n",
       "         [4, 4, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [5, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'pt': tensor([[56.8737,  3.4833,  9.2665,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [43.3078, 27.9431,  7.0786,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [38.7721,  5.3716, 17.8131,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [26.7931, 15.4510, 10.3542,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [27.0307, 15.6282, 10.4800,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [11.9378,  8.4304,  8.0576,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " 'eta': tensor([[-0.5841, -0.4387, -0.3550,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.6704, -0.6761, -0.5333,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-1.0333, -0.1340,  0.3840,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.4791,  0.3042, -0.7401,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.1390,  0.4595, -0.8492,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.4074, -0.9789, -0.3886,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " 'sin_phi': tensor([[ 0.9991, -0.6400, -0.5553,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.1003,  0.2947, -0.0841,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.8163, -0.9429,  0.3380,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [-0.6517, -0.0531, -0.6388,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.7384, -0.6195,  0.2183,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.8212,  0.0037, -0.8509,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " 'cos_phi': tensor([[-0.0424,  0.7684,  0.8316,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.9950, -0.9556,  0.9965,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.5776,  0.3331,  0.9411,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [ 0.7585,  0.9986, -0.7693,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.6743,  0.7850,  0.9759,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.5706, -1.0000, -0.5253,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " 'energy': tensor([[66.8540,  3.8265,  9.8688,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [53.4113, 34.5767,  8.1106,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [61.3816,  5.4198, 19.1434,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [29.9276, 16.1720, 13.3224,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [27.2927, 17.3147, 14.4994,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [12.9431, 12.8123,  8.6748,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " 'phi': tensor([[ 1.6132, -0.6945, -0.5888,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 3.0411,  2.8424, -0.0842,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-2.1866, -1.2312,  0.3448,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         ...,\n",
       "         [-0.7098, -0.0531, -2.4486,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 2.3108, -0.6681,  0.2200,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-2.1780,  3.1379, -2.1239,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " 'momentum': tensor([[[ 5.6874e+01, -5.8408e-01,  9.9910e-01, -4.2351e-02,  6.6854e+01],\n",
       "          [ 3.4833e+00, -4.3873e-01, -6.3997e-01,  7.6840e-01,  3.8265e+00],\n",
       "          [ 9.2665e+00, -3.5497e-01, -5.5534e-01,  8.3162e-01,  9.8688e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.3308e+01, -6.7044e-01,  1.0034e-01, -9.9495e-01,  5.3411e+01],\n",
       "          [ 2.7943e+01, -6.7609e-01,  2.9471e-01, -9.5559e-01,  3.4577e+01],\n",
       "          [ 7.0786e+00, -5.3334e-01, -8.4082e-02,  9.9646e-01,  8.1106e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 3.8772e+01, -1.0333e+00, -8.1630e-01, -5.7762e-01,  6.1382e+01],\n",
       "          [ 5.3716e+00, -1.3395e-01, -9.4289e-01,  3.3311e-01,  5.4198e+00],\n",
       "          [ 1.7813e+01,  3.8404e-01,  3.3805e-01,  9.4113e-01,  1.9143e+01],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 2.6793e+01,  4.7911e-01, -6.5168e-01,  7.5849e-01,  2.9928e+01],\n",
       "          [ 1.5451e+01,  3.0420e-01, -5.3112e-02,  9.9859e-01,  1.6172e+01],\n",
       "          [ 1.0354e+01, -7.4008e-01, -6.3883e-01, -7.6934e-01,  1.3322e+01],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 2.7031e+01,  1.3901e-01,  7.3844e-01, -6.7432e-01,  2.7293e+01],\n",
       "          [ 1.5628e+01,  4.5955e-01, -6.1950e-01,  7.8500e-01,  1.7315e+01],\n",
       "          [ 1.0480e+01, -8.4916e-01,  2.1827e-01,  9.7589e-01,  1.4499e+01],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 1.1938e+01, -4.0742e-01, -8.2123e-01, -5.7059e-01,  1.2943e+01],\n",
       "          [ 8.4304e+00, -9.7892e-01,  3.7202e-03, -9.9999e-01,  1.2812e+01],\n",
       "          [ 8.0576e+00, -3.8860e-01, -8.5092e-01, -5.2530e-01,  8.6748e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]]),\n",
       " 'p4': tensor([[[ 5.6874e+01,  3.4833e+00,  9.2665e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-5.8408e-01, -4.3873e-01, -3.5497e-01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 1.6132e+00, -6.9446e-01, -5.8877e-01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 6.6854e+01,  3.8265e+00,  9.8688e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 4.3308e+01,  2.7943e+01,  7.0786e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-6.7044e-01, -6.7609e-01, -5.3334e-01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 3.0411e+00,  2.8424e+00, -8.4181e-02,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 5.3411e+01,  3.4577e+01,  8.1106e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 3.8772e+01,  5.3716e+00,  1.7813e+01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-1.0333e+00, -1.3395e-01,  3.8404e-01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-2.1866e+00, -1.2312e+00,  3.4484e-01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 6.1382e+01,  5.4198e+00,  1.9143e+01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 2.6793e+01,  1.5451e+01,  1.0354e+01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 4.7911e-01,  3.0420e-01, -7.4008e-01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-7.0980e-01, -5.3137e-02, -2.4486e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 2.9928e+01,  1.6172e+01,  1.3322e+01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 2.7031e+01,  1.5628e+01,  1.0480e+01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 1.3901e-01,  4.5955e-01, -8.4916e-01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 2.3108e+00, -6.6811e-01,  2.2005e-01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 2.7293e+01,  1.7315e+01,  1.4499e+01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 1.1938e+01,  8.4304e+00,  8.0576e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-4.0742e-01, -9.7892e-01, -3.8860e-01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-2.1780e+00,  3.1379e+00, -2.1239e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 1.2943e+01,  1.2812e+01,  8.6748e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets_unpacked)\n",
    "targets_unpacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e193e773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds_unpacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "802a663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "px = targets_unpacked[\"pt\"] * targets_unpacked[\"cos_phi\"] * msk_true_particles\n",
    "py = targets_unpacked[\"pt\"] * targets_unpacked[\"sin_phi\"] * msk_true_particles\n",
    "pz = targets_unpacked[\"pt\"] * np.sinh(targets_unpacked[\"eta\"]) * msk_true_particles\n",
    "# phi = np.arctan2(targets_unpacked[\"sin_phi\"], targets_unpacked[\"cos_phi\"]) * msk_true_particles\n",
    "\n",
    "px_np = px.detach().cpu().numpy()\n",
    "py_np = py.detach().cpu().numpy()\n",
    "pz_np = pz.detach().cpu().numpy()\n",
    "# phi_np = phi.detach().cpu().numpy()\n",
    "\n",
    "# print(\"px_np\", px_np)\n",
    "# print(\"py_np\", py_np)\n",
    "# print(\"pz_np\", pz_np)\n",
    "# print(\"phi_np\", phi_np)\n",
    "\n",
    "true_mom = np.sqrt(np.sum(px_np, axis=1)**2 + np.sum(py_np, axis=1)**2 + np.sum(pz_np, axis=1)**2)\n",
    "\n",
    "E = np.sqrt(px_np**2 + py_np**2 + pz_np**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bc3a2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E Shape (16, 200)\n",
      "px Shape torch.Size([16, 200])\n",
      "py Shape torch.Size([16, 200])\n",
      "pz Shape torch.Size([16, 200])\n"
     ]
    }
   ],
   "source": [
    "# four-vectors\n",
    "# px_py_pz_E = np.column_stack((px_np, py_np, pz_np, E)) \n",
    "print(\"E Shape\", E.shape)\n",
    "print(\"px Shape\", px.shape)\n",
    "print(\"py Shape\", py.shape)\n",
    "print(\"pz Shape\", pz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6404372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastjet as fj\n",
    "import numpy as np\n",
    "import awkward\n",
    "\n",
    "# Four momentum \n",
    "px_np = targets_unpacked[\"pt\"] * targets_unpacked[\"cos_phi\"] * msk_true_particles\n",
    "py_np = targets_unpacked[\"pt\"] * targets_unpacked[\"sin_phi\"] * msk_true_particles\n",
    "pz_np = targets_unpacked[\"pt\"] * np.sinh(targets_unpacked[\"eta\"]) * msk_true_particles\n",
    "E_np = np.sqrt(px_np**2 + py_np**2 + pz_np**2)\n",
    "\n",
    "\n",
    "particles = []\n",
    "for ip in range(E.shape[0]):\n",
    "    for ix in range(E.shape[1]):\n",
    "        px_value = float(px[ip, ix])\n",
    "        py_value = float(py[ip, ix])\n",
    "        pz_value = float(pz[ip, ix])\n",
    "        E_value = float(E[ip, ix])\n",
    "        particle = fj.PseudoJet(px_value, py_value, pz_value, E_value)\n",
    "        particles.append(particle)\n",
    "\n",
    "# print(particles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43801c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--------------------------------------------------------------------------\n",
      "#                         FastJet release 3.4.1\n",
      "#                 M. Cacciari, G.P. Salam and G. Soyez                  \n",
      "#     A software package for jet finding and analysis at colliders      \n",
      "#                           http://fastjet.fr                           \n",
      "#\t                                                                      \n",
      "# Please cite EPJC72(2012)1896 [arXiv:1111.6097] if you use this package\n",
      "# for scientific work and optionally PLB641(2006)57 [hep-ph/0512210].   \n",
      "#                                                                       \n",
      "# FastJet is provided without warranty under the GNU GPL v2 or higher.  \n",
      "# It uses T. Chan's closest pair algorithm, S. Fortune's Voronoi code,\n",
      "# CGAL and 3rd party plugin jet algorithms. See COPYING file for details.\n",
      "#--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "num_threads = 2 \n",
    "\n",
    "def cluster_jets(particles):\n",
    "    jetdef = fj.JetDefinition(fj.antikt_algorithm, 0.4)\n",
    "    jet_ptcut = 20\n",
    "    \n",
    "    cluster = fj.ClusterSequence(particles, jetdef)\n",
    "    jets = cluster.inclusive_jets()\n",
    "    \n",
    "    return jets\n",
    "\n",
    "chunks = [particles[i::num_threads] for i in range(num_threads)]\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    futures = [executor.submit(cluster_jets, chunk) for chunk in chunks]\n",
    "\n",
    "gen_jets = []\n",
    "for future in futures:\n",
    "    gen_jets.extend(future.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e8e812f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jet 1 : 0.0 100000.0 0.0 0.0\n",
      "Jet 2 : 0.09008180599994484 1.688399518593333 3.2425325581975115 0.25203219056129456\n",
      "Jet 3 : 0.17982952874409136 -1.941609349323064 4.889030393616061 0.639602541923523\n",
      "Jet 4 : 0.2837854760326249 -1.8141478483815194 0.4866857889220741 0.8944984823465347\n",
      "Jet 5 : 0.2859382609952973 -1.9548903712942536 3.645599694978839 1.0300532579421997\n",
      "Jet 6 : 0.2861308201805592 1.4929310143035923 5.637340268478351 0.6694888025522232\n",
      "Jet 7 : 0.2941379938467729 -0.5734575981788513 5.126227084403801 0.34384217858314514\n",
      "Jet 8 : 0.34877210409722875 2.7549483849198912 5.248549077315376 2.7524819374084473\n",
      "Jet 9 : 0.7310028699480715 1.6662947650453737 1.0580164066776132 2.007584549486637\n",
      "Jet 10 : 0.4586502391642163 -2.3523484180470198 3.432486624089657 2.432077407836914\n",
      "Jet 11 : 0.5854769714691771 1.2410637209194975 6.108714461801074 1.0999619364738464\n",
      "Jet 12 : 0.5117624579438147 1.7349982561534527 0.0651537824822924 1.4957072734832764\n",
      "Jet 13 : 0.6431122777985504 -2.0522628685423543 2.7779842802087353 2.546139359474182\n",
      "Jet 14 : 0.5467080434729922 -1.2614789087067548 1.1617079925833513 1.042537808418274\n",
      "Jet 15 : 0.5538602225458975 0.3643146834506952 3.421774710186587 0.5910241603851318\n",
      "Jet 16 : 0.642963481848261 2.225352680610504 3.415631600988107 3.0106046199798584\n",
      "Jet 17 : 0.6542428263889553 2.0084812466338695 2.8579387619173424 2.482129618525505\n",
      "Jet 18 : 0.6845608755455563 -2.8273125459487103 5.202695423480123 5.805173933506012\n",
      "Jet 19 : 0.7941553384462687 1.0261901654211973 3.005589481174815 1.2503118515014648\n",
      "Jet 20 : 0.8980049714634354 -1.3140334801141367 4.589288976099246 1.7917308285832405\n",
      "Jet 21 : 1.1631536622935603 -2.248388569375043 3.9627009846398886 5.572278022766113\n",
      "Jet 22 : 1.1368754172705036 -0.8546762347228994 2.6528689850937996 1.5812433958053589\n",
      "Jet 23 : 1.1879619535975148 2.199843217934884 2.4404446695381505 5.425671100616455\n",
      "Jet 24 : 1.2014109013826346 2.6490264325566955 1.6300590649147118 8.536887049674988\n",
      "Jet 25 : 1.2032429834494112 -0.02573017042837325 3.6168033661569794 1.2088762372732162\n",
      "Jet 26 : 1.7030569346605253 0.9843622573887021 0.39945798472743255 2.612409234046936\n",
      "Jet 27 : 1.347488139400367 0.9173230759989304 2.1467673075684335 1.9614060670137405\n",
      "Jet 28 : 1.4290033033610314 0.41740880558210036 1.498874928079723 1.5577488243579865\n",
      "Jet 29 : 1.6617754754189524 1.2246239463597117 3.358689680355339 3.0738119184970856\n",
      "Jet 30 : 2.198834284037222 -1.6080185626164147 5.740650872685211 5.71841698884964\n",
      "Jet 31 : 1.9562461025985014 0.5151484736108396 4.885808425263521 2.2312715500593185\n",
      "Jet 32 : 2.0209630277948363 0.19975051937113525 5.2218101969011945 2.072542980313301\n",
      "Jet 33 : 2.029444889058488 1.0081139528737753 5.195971536294805 3.1570408940315247\n",
      "Jet 34 : 2.3701699594529337 -1.6562255619503885 1.0640498791106376 6.439163863658905\n",
      "Jet 35 : 2.5879238284911423 -0.6867459769419579 1.213835499915877 3.2225465774536133\n",
      "Jet 36 : 2.5989324892370895 -0.2255880277494294 0.6990447140356535 2.6698695570230484\n",
      "Jet 37 : 2.840947751488296 -0.4946494443880636 3.578759778227321 3.198224186897278\n",
      "Jet 38 : 3.3233211085114234 2.25756143467053 5.284469817494564 16.07059681415558\n",
      "Jet 39 : 3.0179329223375713 -0.19989944929589662 1.111748489103069 3.105749763548374\n",
      "Jet 40 : 3.2989199993616833 -1.1958373353088687 3.480946149095736 5.9629701264202595\n",
      "Jet 41 : 4.402593880509506 0.8065409862248705 3.4657375173190976 5.960282981395721\n",
      "Jet 42 : 4.099575293425067 1.0725607345717687 3.9332270833798213 6.707182914018631\n",
      "Jet 43 : 5.114530582842569 1.4722010324439772 4.512213133317557 11.747388899326324\n",
      "Jet 44 : 5.21538484026822 -1.3363596571905536 0.5169214571393717 10.620405375957489\n",
      "Jet 45 : 5.218648902453325 -1.3835632086530747 1.7665400995550065 11.073057174682617\n",
      "Jet 46 : 5.323630573264314 2.8517374196630096 3.3122897108258673 46.25069487094879\n",
      "Jet 47 : 6.215271764247454 1.0669120409353796 0.8625592387931852 10.124699592590332\n",
      "Jet 48 : 6.325393721380861 -0.9264755438295088 2.181446654319459 9.28417731821537\n",
      "Jet 49 : 7.951524594140661 0.6164132105007164 3.8838758109153715 9.597878888249397\n",
      "Jet 50 : 8.449736395532375 -1.5974320091174896 3.8596658670953796 21.750857293605804\n",
      "Jet 51 : 9.177224957643553 1.0186471311261565 5.669455870622299 14.435933619737625\n",
      "Jet 52 : 9.477298541596708 0.5073705974022771 1.8979910536668227 10.795210063457489\n",
      "Jet 53 : 12.649320604506695 1.8184786981995391 3.8060547386461465 40.019073605537415\n",
      "Jet 54 : 14.15847641367517 1.7065458226692007 5.110132549637471 40.34830284118652\n",
      "Jet 55 : 14.387673980517874 -1.1694443245807853 0.04689607530415575 25.477122232317924\n",
      "Jet 56 : 16.86288835543882 -1.4775744645453766 2.6821717916756067 39.0032964348793\n",
      "Jet 57 : 15.381316676084879 -0.17768449941899336 0.19416814782411637 15.742632001638412\n",
      "Jet 58 : 17.15332006249997 1.5793147005206374 2.2136991499349588 43.45008099079132\n",
      "Jet 59 : 20.330874062264495 0.5656218072571918 2.888620040929 23.879256650805473\n",
      "Jet 60 : 24.319082095283687 -0.8924446495608175 5.729433426087134 34.788600385189056\n",
      "Jet 61 : 33.45376450642645 1.1841711686320944 2.605069398715265 60.22123920917511\n",
      "Jet 62 : 42.367516874189036 0.244468237459567 4.345198954743324 44.29645562171936\n",
      "Jet 63 : 32.95423287496457 -0.3046378431405479 5.627739715365307 34.82039063423872\n",
      "Jet 64 : 43.30461856407952 -0.8189287876458684 4.730560575084769 59.303502291440964\n",
      "Jet 65 : 37.7519534974596 0.16659536690989984 2.309340395599968 38.6154802441597\n",
      "Jet 66 : 41.06055862124583 -0.6190319287728727 6.114881610285121 49.59279249608517\n",
      "Jet 67 : 57.53015410728962 -0.7372355693934098 0.3304680004163289 74.70245668292046\n",
      "Jet 68 : 66.88772425633839 0.07966682721519854 2.97838512635176 69.03099033236504\n",
      "Jet 69 : 52.46621772514502 -0.4074544567905387 2.387469211768919 57.33177837729454\n",
      "Jet 70 : 69.29664754741536 0.9204403637281435 4.4384718293256995 101.74903301894665\n",
      "Jet 71 : 72.77305557586824 0.5893860135489599 5.558551932469005 86.74504579603672\n",
      "Jet 72 : 96.35827276373679 -0.9139015490243508 3.9788967693543213 141.06623688340187\n",
      "Jet 73 : 91.55439704163147 0.9226455832860989 1.2656340751845376 135.15591476857662\n",
      "Jet 74 : 85.27654801111349 -0.3469037086237446 4.1466354019914125 92.19717773795128\n",
      "Jet 75 : 84.98820473507699 -0.03384338386100725 1.642804703970144 85.37751220166683\n",
      "Jet 76 : 92.42721140695936 0.3511244872118458 1.0640493086786267 98.7179000377655\n",
      "Jet 77 : 171.37793367931678 -0.22323580630585568 4.90878531370412 180.28879335522652\n",
      "Jet 78 : 137.40305400688956 -0.6357549035031185 3.1646867976683906 168.40995782613754\n",
      "Jet 79 : 132.68666683554727 0.4888888190703436 6.1609675799198955 150.7656301036477\n",
      "Jet 80 : 175.95411447379868 0.3745223673682818 0.5441636984389918 192.13925540447235\n",
      "Jet 81 : 202.14260726254014 -0.5599894101252048 1.6295153664491742 238.24758353829384\n",
      "Jet 82 : 0.0 100000.0 0.0 0.0\n",
      "Jet 83 : 0.054825024615984506 2.386113275018793 2.529766299345116 0.30052733421325684\n",
      "Jet 84 : 0.07519506441340816 -1.9883574840271734 6.011536391275827 0.2797424793243408\n",
      "Jet 85 : 0.08350026785020274 1.2616715327468793 4.887007409761482 0.15925586223602295\n",
      "Jet 86 : 0.09592526171276367 1.5588791476630612 0.8333733853091857 0.23807989060878754\n",
      "Jet 87 : 0.12026943208235558 -1.0917839798229456 3.6776015911360975 0.1993587464094162\n",
      "Jet 88 : 0.12160717270014579 -1.9267874708216013 2.115653898072353 0.42661152780056\n",
      "Jet 89 : 0.1767560277358428 -1.2526253481186809 4.797448460399115 0.33453482389450073\n",
      "Jet 90 : 0.18056353566099076 1.1146863166088636 5.629556336082383 0.3048481047153473\n",
      "Jet 91 : 0.18954143786422428 2.1879563289631143 6.091305834778497 0.855695903301239\n",
      "Jet 92 : 0.2047880733464948 1.3292243467261406 3.707658087906912 0.41395801305770874\n",
      "Jet 93 : 0.23166853131409568 2.603373815088581 0.01769150470325373 1.5734071731567383\n",
      "Jet 94 : 0.25740980220449416 -0.9439844857284592 0.879568262225918 0.38203583657741547\n",
      "Jet 95 : 0.2687495651621798 1.1580001848331563 1.4734855963884508 0.47047577798366547\n",
      "Jet 96 : 0.28358102563729093 -2.076464322183543 2.56751713780114 1.1492024958133698\n",
      "Jet 97 : 0.3118682517120225 -1.449342201688646 2.272321474906935 0.7009294629096985\n",
      "Jet 98 : 0.4028034638479864 1.975385542292494 0.9719893420862653 1.4799244105815887\n",
      "Jet 99 : 0.427217355087487 3.2399678197117394 1.118376123981131 5.4624152183532715\n",
      "Jet 100 : 0.511726211212274 1.829894094617847 4.613896646314355 1.636521726846695\n",
      "Jet 101 : 0.5696586492054171 -2.104778997497412 0.9522596458539286 2.3718199729919434\n",
      "Jet 102 : 0.5768714519144803 2.674237288732951 3.838102668370034 4.202579021453857\n",
      "Jet 103 : 0.9842805124897596 -1.4629498230536635 5.403297903641288 2.2472866773605347\n",
      "Jet 104 : 0.7854060905465863 -1.8317640744136217 3.445172639898729 2.5171358585357666\n",
      "Jet 105 : 0.6717906626273508 -2.2554740756080784 4.6431248847166415 3.239588975906372\n",
      "Jet 106 : 0.6723034396195967 1.5702479456739273 1.653098294353972 1.687175989151001\n",
      "Jet 107 : 0.6941268401223724 0.8841352104011377 5.973544472539009 0.984668955206871\n",
      "Jet 108 : 0.6945317151708065 -2.480224068791428 3.8363321934194543 4.176797389984131\n",
      "Jet 109 : 0.7433352072442234 2.523849492452863 4.2148044752307765 4.66691255569458\n",
      "Jet 110 : 1.007844647559466 -1.183914315005813 1.2521452409238474 1.8072133362293243\n",
      "Jet 111 : 0.8804512020168215 -0.7046657971803543 5.207793761749713 1.110809862613678\n",
      "Jet 112 : 0.9011339631559424 2.2666122737594505 4.590671601395482 4.3931756019592285\n",
      "Jet 113 : 0.9235852745401041 1.782896414716236 0.03484623882316355 2.8239574432373047\n",
      "Jet 114 : 1.141230457476587 1.533845038090774 5.966647617024579 2.773500844836235\n",
      "Jet 115 : 1.1332651654845989 -2.052597720488633 3.923681848786375 4.4866249561309814\n",
      "Jet 116 : 1.1358488447010602 0.7768811566152312 0.4808678219116698 1.5012655854225159\n",
      "Jet 117 : 1.2204469843433654 1.7472683472943382 3.338324570649201 3.611527942121029\n",
      "Jet 118 : 1.3305442247034012 -2.7314963048383722 0.4274544937639831 10.259140014648438\n",
      "Jet 119 : 1.3804616328869008 -1.6447775044277466 0.08942605395162971 3.708549976348877\n",
      "Jet 120 : 2.1444800797510886 -2.7410444619132805 5.078883936468084 16.695640087127686\n",
      "Jet 121 : 2.0301886077904276 0.5816871987777287 2.3049042083574274 2.3851015865802765\n",
      "Jet 122 : 2.6035356365629863 -1.6288542693907266 0.6969088083001288 6.89883291721344\n",
      "Jet 123 : 3.9131891965490566 0.21958040768038636 5.11344805931566 4.087289556860924\n",
      "Jet 124 : 3.8137013794253067 0.23121843747166107 1.4993763229964283 3.9836833104491234\n",
      "Jet 125 : 3.6598787695174138 -0.1733873053044483 0.9255053996475284 3.754912406206131\n",
      "Jet 126 : 3.74132715371385 1.3890433440597016 0.17304059279017214 7.988186597824097\n",
      "Jet 127 : 3.934042493109938 0.6848755171619718 3.5884445232112627 4.954963564872742\n",
      "Jet 128 : 5.946869548836272 1.1754560385745503 2.205588511246199 10.62246359884739\n",
      "Jet 129 : 4.524160338216027 1.2394537974318338 3.3206894997416323 8.510813906788826\n",
      "Jet 130 : 4.624191287908969 -2.2698953713622654 3.1074931566904764 22.624992549419403\n",
      "Jet 131 : 4.85652768080831 1.3617250951663944 4.378084525270897 10.117340952157974\n",
      "Jet 132 : 5.9407531911759675 1.1063793553825654 0.8792335399631408 9.963711231946945\n",
      "Jet 133 : 7.045977406122958 -0.22284570719960028 3.8693825669653052 7.239891603589058\n",
      "Jet 134 : 7.787112652444795 -1.585830267682944 1.8350798206778427 19.838431358337402\n",
      "Jet 135 : 10.8222110574853 -1.6483242248629537 4.093252167261718 29.236476063728333\n",
      "Jet 136 : 9.283183857811116 1.8153826867875116 2.375982891295529 29.300898909568787\n",
      "Jet 137 : 10.199366240725151 1.866872699760025 3.87241926096331 33.816803991794586\n",
      "Jet 138 : 9.73267565382187 0.40483095761000976 3.252699543141949 10.642268761992455\n",
      "Jet 139 : 8.534071863598827 1.6526806881014409 5.085109367781502 23.117914393544197\n",
      "Jet 140 : 12.570130396759065 0.8935468955268556 5.251014564566375 18.064541190862656\n",
      "Jet 141 : 13.360775114966016 -0.9923456034457876 3.1253322656966063 20.521252408623695\n",
      "Jet 142 : 14.48123236311218 -0.6471136976718691 0.5253100848964589 17.736165396869183\n",
      "Jet 143 : 15.031209510997954 -1.0352929417437373 2.424868122643719 23.89596825838089\n",
      "Jet 144 : 16.27194893325761 0.1101269023415928 2.2463337517857025 16.479434557259083\n",
      "Jet 145 : 17.599453253789186 0.99838613886047 4.0550047305763215 27.290366888046265\n",
      "Jet 146 : 19.53101068706806 -1.5660368155402598 2.707440046876794 48.901311814785004\n",
      "Jet 147 : 17.51795135232313 -1.0481533835760861 0.12875016239488013 28.119235634803772\n",
      "Jet 148 : 32.383368064595764 -0.7752423711986168 1.6511487714128257 42.99904191493988\n",
      "Jet 149 : 24.575444916321096 0.7071393767516251 1.6533468221288372 31.20842182636261\n",
      "Jet 150 : 26.38813572291076 -0.3105969389891216 5.581028413232692 27.92176215350628\n",
      "Jet 151 : 28.32288525186962 0.326942078444031 5.947270418878389 29.98228567838669\n",
      "Jet 152 : 28.426722840792202 -0.9030113154964198 5.746342646790488 40.95490036904812\n",
      "Jet 153 : 33.06331020012623 -0.5502452223533817 3.4690151883315714 38.668117083609104\n",
      "Jet 154 : 35.31674577806343 1.2831075573139208 2.7030080187987897 69.01223266124725\n",
      "Jet 155 : 46.77802953993305 -0.37010510629416293 4.2871245039676005 50.98260487988591\n",
      "Jet 156 : 37.778117131995344 -0.6776013128814913 2.877211296594381 47.01843413710594\n",
      "Jet 157 : 42.95086020458949 -0.8508255710673986 4.831842017212927 59.882629469037056\n",
      "Jet 158 : 46.906766803557694 -0.24061539851229288 1.5831402140431412 49.23774801194668\n",
      "Jet 159 : 52.79806624499606 0.4040867780643779 0.7304292320383916 57.75730911642313\n",
      "Jet 160 : 49.165278111072624 -0.22818138957778866 0.2937994216826372 51.068945690989494\n",
      "Jet 161 : 50.17757669133995 0.6910584097573276 2.773113729121261 63.334053695201874\n",
      "Jet 162 : 54.078867873360856 0.8966015024834382 4.507585446273798 77.87136429548264\n",
      "Jet 163 : 58.079518708466765 -0.047241185717644595 2.810462530427222 59.132314175367355\n",
      "Jet 164 : 87.43618119492926 -0.27247464360105 4.897018041073027 92.75066250562668\n",
      "Jet 165 : 65.04012281522895 -0.40062883398637833 2.402190982819824 70.72652670741081\n",
      "Jet 166 : 75.59795711269838 -0.6539075576270665 6.207450348992573 93.27951028943062\n",
      "Jet 167 : 94.96015391244592 0.6662951816246793 5.637606268135354 118.62470008432865\n",
      "Jet 168 : 75.68416487768032 0.28686061560263026 4.2610040902459 79.99628829956055\n",
      "Jet 169 : 88.18118656155578 0.7119295087273768 1.1441365177513596 113.05821104347706\n",
      "Jet 170 : 104.62367134656816 -0.8115900104994492 3.988928968052063 142.97454938292503\n",
      "Jet 171 : 153.57163286570452 0.4048393844106601 0.12758401465169952 170.15975739061832\n"
     ]
    }
   ],
   "source": [
    "for i, jet in enumerate(gen_jets):\n",
    "    print(\"Jet\", i+1, \":\", jet.pt(), jet.eta(), jet.phi(), jet.e())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d04dad18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Jets: 171\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Jets:\", len(gen_jets))\n",
    "\n",
    "\n",
    "gen_jet_pt = [jet.pt() for jet in gen_jets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fc77189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jet pT values in bin 1: [12.649320604506695, 14.15847641367517, 14.387673980517874, 16.86288835543882, 15.381316676084879, 17.15332006249997, 10.8222110574853, 10.199366240725151, 12.570130396759065, 13.360775114966016, 14.48123236311218, 15.031209510997954, 16.27194893325761, 17.599453253789186, 19.53101068706806, 17.51795135232313]\n",
      "Jet pT values in bin 2: [20.330874062264495, 24.319082095283687, 24.575444916321096, 26.38813572291076, 28.32288525186962, 28.426722840792202]\n",
      "Jet pT values in bin 3: [33.45376450642645, 32.95423287496457, 37.7519534974596, 32.383368064595764, 33.06331020012623, 35.31674577806343, 37.778117131995344]\n",
      "Jet pT values in bin 4: [42.367516874189036, 43.30461856407952, 41.06055862124583, 57.53015410728962, 52.46621772514502, 46.77802953993305, 42.95086020458949, 46.906766803557694, 52.79806624499606, 49.165278111072624, 50.17757669133995, 54.078867873360856, 58.079518708466765]\n",
      "Jet pT values in bin 5: [66.88772425633839, 69.29664754741536, 72.77305557586824, 65.04012281522895, 75.59795711269838, 75.68416487768032]\n",
      "Jet pT values in bin 6: [96.35827276373679, 91.55439704163147, 85.27654801111349, 84.98820473507699, 92.42721140695936, 87.43618119492926, 94.96015391244592, 88.18118656155578]\n",
      "Jet pT values in bin 7: [171.37793367931678, 137.40305400688956, 132.68666683554727, 175.95411447379868, 104.62367134656816, 153.57163286570452]\n"
     ]
    }
   ],
   "source": [
    "import ROOT\n",
    "import numpy as np\n",
    "\n",
    "bins = [10, 20, 30, 40, 60, 80, 100, 200]\n",
    "\n",
    "x_vals = []\n",
    "ratio_iqr_median = []\n",
    "\n",
    "# Iterating over bins\n",
    "for i in range(len(bins) - 1):\n",
    "    lim_low = bins[i]\n",
    "    lim_hi = bins[i + 1]\n",
    "    x_vals.append(np.mean([lim_low, lim_hi]))\n",
    "\n",
    "    # Filtering out jet pT values within each bin\n",
    "    jet_values_in_bin = [pt for pt in gen_jet_pt if lim_low < pt <= lim_hi]\n",
    "    print(\"Jet pT values in bin {}: {}\".format(i+1, jet_values_in_bin))  # Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10b51263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ROOT\n",
    "import numpy as np\n",
    "\n",
    "# Define bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100, 200]\n",
    "\n",
    "# Initialize lists to store values\n",
    "x_vals = []\n",
    "ratio_iqr_median = []\n",
    "\n",
    "# Iterate over bins\n",
    "for i in range(len(bins) - 1):\n",
    "    lim_low = bins[i]\n",
    "    lim_hi = bins[i + 1]\n",
    "    x_vals.append(np.mean([lim_low, lim_hi]))\n",
    "\n",
    "    # Filter jet pT values within the bin\n",
    "    jet_values_in_bin = [pt for pt in gen_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Check if there are any jet pT values in the bin\n",
    "    if len(jet_values_in_bin) == 0:\n",
    "        # Append NaN values\n",
    "        ratio_iqr_median.append(np.nan)\n",
    "        continue  # Skip calculation for this bin\n",
    "\n",
    "    #  IQR and median for the jet pT values\n",
    "    jet_iqr = np.percentile(jet_values_in_bin, 75) - np.percentile(jet_values_in_bin, 25)\n",
    "    jet_median = np.median(jet_values_in_bin)\n",
    "    ratio_iqr_median_ratio = jet_iqr / jet_median\n",
    "    ratio_iqr_median.append(ratio_iqr_median_ratio)\n",
    "\n",
    "#  We need to filter out NaN values,ROOT unable to print it\n",
    "x_vals_filtered = [x for x, y in zip(x_vals, ratio_iqr_median) if not np.isnan(y)]\n",
    "ratio_iqr_median_filtered = [y for y in ratio_iqr_median if not np.isnan(y)]\n",
    "\n",
    "# Create a TGraph with filtered values\n",
    "gr_ratio = ROOT.TGraph(len(x_vals_filtered), np.array(x_vals_filtered), np.array(ratio_iqr_median_filtered))\n",
    "# Set the titles to empty strings\n",
    "gr_ratio.SetTitle(\"\")\n",
    "\n",
    "# Create a canvas\n",
    "canvas = ROOT.TCanvas(\"canvas\", \"Ratio IQR/Median vs Jet pT\", 800, 600)\n",
    "\n",
    "# Draw the graph with connecting lines\n",
    "gr_ratio.SetMarkerStyle(20)\n",
    "gr_ratio.SetMarkerColor(ROOT.kBlue)\n",
    "gr_ratio.SetLineColor(ROOT.kBlue)\n",
    "gr_ratio.GetXaxis().SetTitle(\"Jet pT (GeV)\")\n",
    "gr_ratio.GetYaxis().SetTitle(\"Response IQR/Median\")\n",
    "gr_ratio.Draw(\"APL\")\n",
    "\n",
    "# Add legend\n",
    "legend = ROOT.TLegend(0.75, 0.75, 0.9, 0.9)\n",
    "legend.AddEntry(gr_ratio, \"Jet pT\", \"p\")\n",
    "legend.Draw()\n",
    "\n",
    "# Show the canvas\n",
    "canvas.Draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59f5f143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2543052730682299,\n",
       " 0.13562724697669346,\n",
       " 0.10538658809351198,\n",
       " 0.19309252475841274,\n",
       " 0.10419922737446956,\n",
       " 0.06859158337799913,\n",
       " 0.2272403500309068]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_iqr_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0468d700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reconstructed particles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0be43ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_px = preds_unpacked[\"pt\"] * preds_unpacked[\"cos_phi\"] * msk_pred_particles\n",
    "reco_py = preds_unpacked[\"pt\"] * preds_unpacked[\"sin_phi\"] * msk_pred_particles\n",
    "reco_pz = preds_unpacked[\"pt\"] * np.sinh(preds_unpacked[\"eta\"]) * msk_pred_particles\n",
    "reco_phi = np.arctan2(preds_unpacked[\"sin_phi\"], preds_unpacked[\"cos_phi\"]) * msk_pred_particles\n",
    "\n",
    "reco_px_np = reco_px.detach().cpu().numpy()\n",
    "reco_py_np = reco_py.detach().cpu().numpy()\n",
    "reco_pz_np = reco_pz.detach().cpu().numpy()\n",
    "reco_phi_np = reco_phi.detach().cpu().numpy()\n",
    "\n",
    "# print(\"reco_px_np\", reco_px_np)\n",
    "# print(\"reco_py_np\", reco_py_np)\n",
    "# print(\"reco_pz_np\", reco_pz_np)\n",
    "# print(\"reco_phi_np\", reco_phi_np)\n",
    "\n",
    "reco_pred_mom = np.sqrt(np.sum(reco_px_np, axis=1)**2 + np.sum(reco_py_np, axis=1)**2 + np.sum(reco_pz_np, axis=1)**2)\n",
    "\n",
    "reco_E_np = np.sqrt(reco_px_np**2 + reco_py_np**2 + reco_pz_np**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4726630d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E Shape torch.Size([16, 200])\n",
      "px Shape (16, 200)\n",
      "py Shape (16, 200)\n",
      "pz Shape (16, 200)\n"
     ]
    }
   ],
   "source": [
    "# four-vectors\n",
    "# px_py_pz_E = np.column_stack((px_np, py_np, pz_np, E)) \n",
    "print(\"E Shape\", E_np.shape)\n",
    "print(\"px Shape\", reco_px_np.shape)\n",
    "print(\"py Shape\", reco_py_np.shape)\n",
    "print(\"pz Shape\", reco_pz_np.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da2c45e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reco_particles = []   # TODO:Change this to reco_jets\n",
    "for ip in range(E_np.shape[0]):\n",
    "    for ix in range(E_np.shape[1]):\n",
    "        px_value = float(reco_px_np[ip, ix])\n",
    "        py_value = float(reco_py_np[ip, ix])\n",
    "        pz_value = float(reco_pz_np[ip, ix])\n",
    "        E_value = float(E_np[ip, ix])\n",
    "        particle = fj.PseudoJet(px_value, py_value, pz_value, E_value)\n",
    "        reco_particles.append(particle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7dc7e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reco_particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91fa6295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "num_threads = 2 \n",
    "\n",
    "# Function to perform jet clustering\n",
    "def cluster_jets(particles):\n",
    "    jetdef = fj.JetDefinition(fj.antikt_algorithm, 0.4)\n",
    "#     jet_ptcut = 20\n",
    "    \n",
    "    cluster = fj.ClusterSequence(reco_particles, jetdef)\n",
    "    jets = cluster.inclusive_jets()\n",
    "    \n",
    "    return jets\n",
    "\n",
    "# Spliting particles list into chunks to process in parallel\n",
    "chunks = [particles[i::num_threads] for i in range(num_threads)]\n",
    "\n",
    "# jet clustering in parallel\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    futures = [executor.submit(cluster_jets, chunk) for chunk in chunks]\n",
    "\n",
    "reco_jets = []\n",
    "for future in futures:\n",
    "    reco_jets.extend(future.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44dfe7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jet 1 : 0.0 100000.0 0.0 0.0\n",
      "Jet 2 : 0.07882030805177469 1.8167185535706225 2.951369063369511 0.22204189002513885\n",
      "Jet 3 : 0.12113312475164813 0.15249536092622637 5.42331856606485 0.0\n",
      "Jet 4 : 0.14002234582949227 -0.7146855652245196 2.1121936169811706 0.14617332816123962\n",
      "Jet 5 : 0.15016258591546733 -0.02950076899164768 3.9896188892953903 0.0\n",
      "Jet 6 : 0.19252003762763256 1.6374622612016174 1.2372514045277716 0.5467910766601562\n",
      "Jet 7 : 0.31117099161816236 -0.05268089049422681 4.602902737062296 0.0\n",
      "Jet 8 : 0.31129898868580047 -2.009295113023728 4.8492154080068435 0.639602541923523\n",
      "Jet 9 : 0.39814676485266115 -1.7276890484060343 3.393508576883893 1.1012221574783325\n",
      "Jet 10 : 0.4106860121880598 -0.07536259399666187 2.8200299839351524 0.0\n",
      "Jet 11 : 0.7469571355856496 -0.9576980414051838 1.377144389471512 0.9607164263725281\n",
      "Jet 12 : 0.4598942945742946 0.07339406513595004 3.618824661070488 0.0\n",
      "Jet 13 : 0.48274753042076024 0.0821862533037606 2.84338975829884 0.0\n",
      "Jet 14 : 0.8155201256276567 1.4610280368702449 5.844650355251998 1.7769968509674072\n",
      "Jet 15 : 0.5403988512719041 -0.6466500648023377 5.196148426336255 0.6426740884780884\n",
      "Jet 16 : 0.5619107353768218 1.0589653928189589 0.38283997608094816 0.9017731547355652\n",
      "Jet 17 : 0.564687109496793 -2.1192879858838274 0.9554714548098322 2.3718199729919434\n",
      "Jet 18 : 0.5844451704047964 -1.3167383703035531 5.483265195136967 1.1831881999969482\n",
      "Jet 19 : 1.022024050415344 0.982729149861702 6.049604820387193 1.3501981496810913\n",
      "Jet 20 : 1.3966090501073487 2.6301299371137152 4.043000407644211 8.869491577148438\n",
      "Jet 21 : 0.9044953417255407 -2.432153908233426 4.65087546751327 3.239588975906372\n",
      "Jet 22 : 0.9110641020557757 2.2880674038123074 4.592854751131279 4.3931756019592285\n",
      "Jet 23 : 0.921973269163705 -0.16545968753762627 1.195935344864456 0.0\n",
      "Jet 24 : 0.9674690663918077 -0.15935306393382898 0.6145498934174881 0.0\n",
      "Jet 25 : 1.0785978363319262 1.8577032467827288 5.337073871009729 3.439111292362213\n",
      "Jet 26 : 1.7961868486819146 1.817913812519274 3.3653726328075337 5.451708793640137\n",
      "Jet 27 : 1.5275833560967171 -1.842083939619402 2.785336708027699 3.8341593742370605\n",
      "Jet 28 : 1.087056898025616 -1.2695237115723155 3.560858368271561 2.0053343772888184\n",
      "Jet 29 : 1.1028579239462561 -1.2394940011901838 2.40016595604662 1.896774411201477\n",
      "Jet 30 : 1.3251204286709566 1.2632323659230777 2.0360435589230774 1.6206282824277878\n",
      "Jet 31 : 1.152699906000705 -0.23117282986146975 0.7362143276449953 1.168011724948883\n",
      "Jet 32 : 1.3697956977490602 -1.6597766629194362 0.10624068137193857 3.708549976348877\n",
      "Jet 33 : 2.1603914474699555 1.9593505246398046 2.5614079763968216 7.685758829116821\n",
      "Jet 34 : 3.2948562388814184 -1.922740251693444 0.4850660198824315 10.259140014648438\n",
      "Jet 35 : 1.8443693299814696 -2.249027823127073 3.966897340607541 7.237939238548279\n",
      "Jet 36 : 2.2390414686100613 -1.6254137847047831 5.752380627728338 5.71841698884964\n",
      "Jet 37 : 2.1178441011000193 -0.137932481137315 3.2511587551134675 2.1443705558776855\n",
      "Jet 38 : 2.1685942599359103 1.5467721703990809 4.544430737100831 5.2841503620147705\n",
      "Jet 39 : 2.2068534179677 -1.0874225151622456 1.691489510521013 3.9535565450787544\n",
      "Jet 40 : 2.830964707360673 -0.1319629191024621 5.081012365714822 1.0640984773635864\n",
      "Jet 41 : 2.668665739028453 2.440153616166196 5.219321859586117 13.821145057678223\n",
      "Jet 42 : 2.7656297496572955 0.5165280667413397 4.907562555705718 2.9033863693475723\n",
      "Jet 43 : 2.803944097352026 0.47677400115771124 1.7036464298228362 2.4401508569717407\n",
      "Jet 44 : 2.8375045767660896 -0.012916798561549734 4.881581999569891 0.0\n",
      "Jet 45 : 4.851856039598329 1.6206722428257676 0.027339068522645307 12.751264750957489\n",
      "Jet 46 : 4.43499556990813 0.7729007060310221 5.432144214163155 1.7819877862930298\n",
      "Jet 47 : 4.567056966208255 -1.6558670874185963 4.287205071758169 7.864931702613831\n",
      "Jet 48 : 4.705879756341957 -0.03858260861597798 2.686535287966641 0.0\n",
      "Jet 49 : 4.823056769131725 -0.8463630808764008 1.034320750083995 5.585289970040321\n",
      "Jet 50 : 4.8719957468717325 -1.5140033601555538 0.6057529819712298 11.20678485929966\n",
      "Jet 51 : 5.323977394805364 -1.1515995136214918 6.163373507808177 9.004663944244385\n",
      "Jet 52 : 5.474808697090824 -0.19939984871034025 3.764178336380889 5.52441094070673\n",
      "Jet 53 : 5.566592992994697 -2.295776409909803 3.143079832907052 24.127387166023254\n",
      "Jet 54 : 6.814355642856927 -1.5892353092161908 1.8340503265622183 17.15015697479248\n",
      "Jet 55 : 6.129588778448467 -0.6882679121257839 4.422399141801665 7.714693665504456\n",
      "Jet 56 : 7.239607182067749 1.2137214339827191 3.218954999791446 12.696638688445091\n",
      "Jet 57 : 8.302752143670736 -1.7124496395435644 3.864387389371016 21.61172866821289\n",
      "Jet 58 : 8.124533155864714 0.6944424895756033 3.9339100626931485 10.488820314407349\n",
      "Jet 59 : 9.030828081857635 0.8395999504439583 5.166308384179576 12.76693856716156\n",
      "Jet 60 : 9.758965599382435 -0.71789929110692 0.6160044585638865 10.45416385680437\n",
      "Jet 61 : 11.859322219668284 0.07887160845558969 1.7678923003357043 9.34581133723259\n",
      "Jet 62 : 11.014589439420453 1.8473259324276674 3.8554426217182862 36.081839859485626\n",
      "Jet 63 : 13.631005445246325 -0.035891735561866984 0.6140695769452089 10.151417136192322\n",
      "Jet 64 : 14.594562273871688 -1.014222511009667 3.1702145695063892 22.78314083814621\n",
      "Jet 65 : 13.697334065223073 0.5390204346743424 0.5030296262299947 15.426725953817368\n",
      "Jet 66 : 14.011371228589043 0.3458673018376302 5.9889050524902485 11.695712625980377\n",
      "Jet 67 : 14.394917151824698 0.6592597309271581 4.442509197620665 24.209442526102066\n",
      "Jet 68 : 29.81400899539625 0.03790995756631846 1.231381373242584 22.883928410708904\n",
      "Jet 69 : 19.11025008088258 1.759531533203217 2.2891853855652116 55.34202587604523\n",
      "Jet 70 : 16.243463173415748 1.5476234775409108 3.8396752150984765 46.44536513090134\n",
      "Jet 71 : 17.890887849752808 0.9816267100038788 3.4361738004015803 57.52162662148476\n",
      "Jet 72 : 19.378880110978177 1.0086680907714554 5.630261410272576 29.246005043387413\n",
      "Jet 73 : 20.08903660107988 1.0788432614786634 0.9086743827519047 32.15016174316406\n",
      "Jet 74 : 24.586316204972096 0.9135531540752722 2.154049565838411 16.94865530729294\n",
      "Jet 75 : 23.81769133428089 -1.0176689465634203 2.359144555404421 35.09134033322334\n",
      "Jet 76 : 30.58907191284124 -1.4760644939170586 2.7145068462384403 62.02204221487045\n",
      "Jet 77 : 33.74769655108039 1.425444835151134 5.035422337704428 69.8694519251585\n",
      "Jet 78 : 34.53217367352348 -0.33244296783634947 5.555324624575595 36.365240938961506\n",
      "Jet 79 : 54.240035921625285 1.3532065075449942 2.6236221170990874 107.58351990580559\n",
      "Jet 80 : 71.49873939995176 0.8714207751395453 2.6966122258859677 81.28566689789295\n",
      "Jet 81 : 46.94575358940209 -0.908862939652321 5.739007817792718 68.80784483253956\n",
      "Jet 82 : 51.6090362123434 0.1401867688049525 2.295533095111678 51.08373863995075\n",
      "Jet 83 : 74.37475785044116 -0.8974440193909214 4.796455609534343 109.43915224075317\n",
      "Jet 84 : 116.51306682093401 0.9666052390332134 4.369319022470972 172.42099541425705\n",
      "Jet 85 : 107.83944486662512 -0.8250509201105565 0.2729392000317664 144.2230402380228\n",
      "Jet 86 : 86.467512912057 -0.05169341600065555 4.0415096512292 71.74255166947842\n",
      "Jet 87 : 110.47532205876288 0.1979657122130525 2.980609274831416 119.09872831404209\n",
      "Jet 88 : 105.0628574460027 0.8490364244277498 1.4048697257254712 143.30205324292183\n",
      "Jet 89 : 122.0465853362104 -0.9912672618480084 4.090779786567012 178.4875250160694\n",
      "Jet 90 : 133.16227490571785 -0.4680792862222655 2.48239814188659 122.41631047427654\n",
      "Jet 91 : 166.79645000281457 -0.7210819955011398 3.6374558146777343 195.33391128480434\n",
      "Jet 92 : 110.17136213545987 -0.027657394683382457 1.6393866807869064 108.32233448326588\n",
      "Jet 93 : 127.70541924749243 -0.4260124199599147 6.049648179966651 138.69589108228683\n",
      "Jet 94 : 157.5504504899742 0.5546129670747125 5.611870662675388 183.94065177440643\n",
      "Jet 95 : 137.14703672545463 -0.5476376696439711 3.00464850052295 154.7473503127694\n",
      "Jet 96 : 202.15605640488985 -0.2539140013051047 4.962098917158816 232.75325246155262\n",
      "Jet 97 : 266.4555961712544 0.48716470387713356 0.9424527981599682 308.43459591269493\n",
      "Jet 98 : 255.576420258614 0.47484998838068626 6.280535378193346 285.0756834447384\n",
      "Jet 99 : 249.71287403896693 -0.00874366506383484 4.381890666952954 243.61360603570938\n",
      "Jet 100 : 206.2728518913886 0.024484952110219827 0.35378872610760514 531.2046158388257\n",
      "Jet 101 : 253.37856658873815 -0.5714305048645324 1.6295523006843287 301.3708279803395\n",
      "Jet 102 : 0.0 100000.0 0.0 0.0\n",
      "Jet 103 : 0.07882030805177469 1.8167185535706225 2.951369063369511 0.22204189002513885\n",
      "Jet 104 : 0.12113312475164813 0.15249536092622637 5.42331856606485 0.0\n",
      "Jet 105 : 0.14002234582949227 -0.7146855652245196 2.1121936169811706 0.14617332816123962\n",
      "Jet 106 : 0.15016258591546733 -0.02950076899164768 3.9896188892953903 0.0\n",
      "Jet 107 : 0.19252003762763256 1.6374622612016174 1.2372514045277716 0.5467910766601562\n",
      "Jet 108 : 0.31117099161816236 -0.05268089049422681 4.602902737062296 0.0\n",
      "Jet 109 : 0.31129898868580047 -2.009295113023728 4.8492154080068435 0.639602541923523\n",
      "Jet 110 : 0.39814676485266115 -1.7276890484060343 3.393508576883893 1.1012221574783325\n",
      "Jet 111 : 0.4106860121880598 -0.07536259399666187 2.8200299839351524 0.0\n",
      "Jet 112 : 0.7469571355856496 -0.9576980414051838 1.377144389471512 0.9607164263725281\n",
      "Jet 113 : 0.4598942945742946 0.07339406513595004 3.618824661070488 0.0\n",
      "Jet 114 : 0.48274753042076024 0.0821862533037606 2.84338975829884 0.0\n",
      "Jet 115 : 0.8155201256276567 1.4610280368702449 5.844650355251998 1.7769968509674072\n",
      "Jet 116 : 0.5403988512719041 -0.6466500648023377 5.196148426336255 0.6426740884780884\n",
      "Jet 117 : 0.5619107353768218 1.0589653928189589 0.38283997608094816 0.9017731547355652\n",
      "Jet 118 : 0.564687109496793 -2.1192879858838274 0.9554714548098322 2.3718199729919434\n",
      "Jet 119 : 0.5844451704047964 -1.3167383703035531 5.483265195136967 1.1831881999969482\n",
      "Jet 120 : 1.022024050415344 0.982729149861702 6.049604820387193 1.3501981496810913\n",
      "Jet 121 : 1.3966090501073487 2.6301299371137152 4.043000407644211 8.869491577148438\n",
      "Jet 122 : 0.9044953417255407 -2.432153908233426 4.65087546751327 3.239588975906372\n",
      "Jet 123 : 0.9110641020557757 2.2880674038123074 4.592854751131279 4.3931756019592285\n",
      "Jet 124 : 0.921973269163705 -0.16545968753762627 1.195935344864456 0.0\n",
      "Jet 125 : 0.9674690663918077 -0.15935306393382898 0.6145498934174881 0.0\n",
      "Jet 126 : 1.0785978363319262 1.8577032467827288 5.337073871009729 3.439111292362213\n",
      "Jet 127 : 1.7961868486819146 1.817913812519274 3.3653726328075337 5.451708793640137\n",
      "Jet 128 : 1.5275833560967171 -1.842083939619402 2.785336708027699 3.8341593742370605\n",
      "Jet 129 : 1.087056898025616 -1.2695237115723155 3.560858368271561 2.0053343772888184\n",
      "Jet 130 : 1.1028579239462561 -1.2394940011901838 2.40016595604662 1.896774411201477\n",
      "Jet 131 : 1.3251204286709566 1.2632323659230777 2.0360435589230774 1.6206282824277878\n",
      "Jet 132 : 1.152699906000705 -0.23117282986146975 0.7362143276449953 1.168011724948883\n",
      "Jet 133 : 1.3697956977490602 -1.6597766629194362 0.10624068137193857 3.708549976348877\n",
      "Jet 134 : 2.1603914474699555 1.9593505246398046 2.5614079763968216 7.685758829116821\n",
      "Jet 135 : 3.2948562388814184 -1.922740251693444 0.4850660198824315 10.259140014648438\n",
      "Jet 136 : 1.8443693299814696 -2.249027823127073 3.966897340607541 7.237939238548279\n",
      "Jet 137 : 2.2390414686100613 -1.6254137847047831 5.752380627728338 5.71841698884964\n",
      "Jet 138 : 2.1178441011000193 -0.137932481137315 3.2511587551134675 2.1443705558776855\n",
      "Jet 139 : 2.1685942599359103 1.5467721703990809 4.544430737100831 5.2841503620147705\n",
      "Jet 140 : 2.2068534179677 -1.0874225151622456 1.691489510521013 3.9535565450787544\n",
      "Jet 141 : 2.830964707360673 -0.1319629191024621 5.081012365714822 1.0640984773635864\n",
      "Jet 142 : 2.668665739028453 2.440153616166196 5.219321859586117 13.821145057678223\n",
      "Jet 143 : 2.7656297496572955 0.5165280667413397 4.907562555705718 2.9033863693475723\n",
      "Jet 144 : 2.803944097352026 0.47677400115771124 1.7036464298228362 2.4401508569717407\n",
      "Jet 145 : 2.8375045767660896 -0.012916798561549734 4.881581999569891 0.0\n",
      "Jet 146 : 4.851856039598329 1.6206722428257676 0.027339068522645307 12.751264750957489\n",
      "Jet 147 : 4.43499556990813 0.7729007060310221 5.432144214163155 1.7819877862930298\n",
      "Jet 148 : 4.567056966208255 -1.6558670874185963 4.287205071758169 7.864931702613831\n",
      "Jet 149 : 4.705879756341957 -0.03858260861597798 2.686535287966641 0.0\n",
      "Jet 150 : 4.823056769131725 -0.8463630808764008 1.034320750083995 5.585289970040321\n",
      "Jet 151 : 4.8719957468717325 -1.5140033601555538 0.6057529819712298 11.20678485929966\n",
      "Jet 152 : 5.323977394805364 -1.1515995136214918 6.163373507808177 9.004663944244385\n",
      "Jet 153 : 5.474808697090824 -0.19939984871034025 3.764178336380889 5.52441094070673\n",
      "Jet 154 : 5.566592992994697 -2.295776409909803 3.143079832907052 24.127387166023254\n",
      "Jet 155 : 6.814355642856927 -1.5892353092161908 1.8340503265622183 17.15015697479248\n",
      "Jet 156 : 6.129588778448467 -0.6882679121257839 4.422399141801665 7.714693665504456\n",
      "Jet 157 : 7.239607182067749 1.2137214339827191 3.218954999791446 12.696638688445091\n",
      "Jet 158 : 8.302752143670736 -1.7124496395435644 3.864387389371016 21.61172866821289\n",
      "Jet 159 : 8.124533155864714 0.6944424895756033 3.9339100626931485 10.488820314407349\n",
      "Jet 160 : 9.030828081857635 0.8395999504439583 5.166308384179576 12.76693856716156\n",
      "Jet 161 : 9.758965599382435 -0.71789929110692 0.6160044585638865 10.45416385680437\n",
      "Jet 162 : 11.859322219668284 0.07887160845558969 1.7678923003357043 9.34581133723259\n",
      "Jet 163 : 11.014589439420453 1.8473259324276674 3.8554426217182862 36.081839859485626\n",
      "Jet 164 : 13.631005445246325 -0.035891735561866984 0.6140695769452089 10.151417136192322\n",
      "Jet 165 : 14.594562273871688 -1.014222511009667 3.1702145695063892 22.78314083814621\n",
      "Jet 166 : 13.697334065223073 0.5390204346743424 0.5030296262299947 15.426725953817368\n",
      "Jet 167 : 14.011371228589043 0.3458673018376302 5.9889050524902485 11.695712625980377\n",
      "Jet 168 : 14.394917151824698 0.6592597309271581 4.442509197620665 24.209442526102066\n",
      "Jet 169 : 29.81400899539625 0.03790995756631846 1.231381373242584 22.883928410708904\n",
      "Jet 170 : 19.11025008088258 1.759531533203217 2.2891853855652116 55.34202587604523\n",
      "Jet 171 : 16.243463173415748 1.5476234775409108 3.8396752150984765 46.44536513090134\n",
      "Jet 172 : 17.890887849752808 0.9816267100038788 3.4361738004015803 57.52162662148476\n",
      "Jet 173 : 19.378880110978177 1.0086680907714554 5.630261410272576 29.246005043387413\n",
      "Jet 174 : 20.08903660107988 1.0788432614786634 0.9086743827519047 32.15016174316406\n",
      "Jet 175 : 24.586316204972096 0.9135531540752722 2.154049565838411 16.94865530729294\n",
      "Jet 176 : 23.81769133428089 -1.0176689465634203 2.359144555404421 35.09134033322334\n",
      "Jet 177 : 30.58907191284124 -1.4760644939170586 2.7145068462384403 62.02204221487045\n",
      "Jet 178 : 33.74769655108039 1.425444835151134 5.035422337704428 69.8694519251585\n",
      "Jet 179 : 34.53217367352348 -0.33244296783634947 5.555324624575595 36.365240938961506\n",
      "Jet 180 : 54.240035921625285 1.3532065075449942 2.6236221170990874 107.58351990580559\n",
      "Jet 181 : 71.49873939995176 0.8714207751395453 2.6966122258859677 81.28566689789295\n",
      "Jet 182 : 46.94575358940209 -0.908862939652321 5.739007817792718 68.80784483253956\n",
      "Jet 183 : 51.6090362123434 0.1401867688049525 2.295533095111678 51.08373863995075\n",
      "Jet 184 : 74.37475785044116 -0.8974440193909214 4.796455609534343 109.43915224075317\n",
      "Jet 185 : 116.51306682093401 0.9666052390332134 4.369319022470972 172.42099541425705\n",
      "Jet 186 : 107.83944486662512 -0.8250509201105565 0.2729392000317664 144.2230402380228\n",
      "Jet 187 : 86.467512912057 -0.05169341600065555 4.0415096512292 71.74255166947842\n",
      "Jet 188 : 110.47532205876288 0.1979657122130525 2.980609274831416 119.09872831404209\n",
      "Jet 189 : 105.0628574460027 0.8490364244277498 1.4048697257254712 143.30205324292183\n",
      "Jet 190 : 122.0465853362104 -0.9912672618480084 4.090779786567012 178.4875250160694\n",
      "Jet 191 : 133.16227490571785 -0.4680792862222655 2.48239814188659 122.41631047427654\n",
      "Jet 192 : 166.79645000281457 -0.7210819955011398 3.6374558146777343 195.33391128480434\n",
      "Jet 193 : 110.17136213545987 -0.027657394683382457 1.6393866807869064 108.32233448326588\n",
      "Jet 194 : 127.70541924749243 -0.4260124199599147 6.049648179966651 138.69589108228683\n",
      "Jet 195 : 157.5504504899742 0.5546129670747125 5.611870662675388 183.94065177440643\n",
      "Jet 196 : 137.14703672545463 -0.5476376696439711 3.00464850052295 154.7473503127694\n",
      "Jet 197 : 202.15605640488985 -0.2539140013051047 4.962098917158816 232.75325246155262\n",
      "Jet 198 : 266.4555961712544 0.48716470387713356 0.9424527981599682 308.43459591269493\n",
      "Jet 199 : 255.576420258614 0.47484998838068626 6.280535378193346 285.0756834447384\n",
      "Jet 200 : 249.71287403896693 -0.00874366506383484 4.381890666952954 243.61360603570938\n",
      "Jet 201 : 206.2728518913886 0.024484952110219827 0.35378872610760514 531.2046158388257\n",
      "Jet 202 : 253.37856658873815 -0.5714305048645324 1.6295523006843287 301.3708279803395\n"
     ]
    }
   ],
   "source": [
    "for i, jet in enumerate(reco_jets):\n",
    "    print(\"Jet\", i+1, \":\", jet.pt(), jet.eta(), jet.phi(), jet.e())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84b009c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Jets: 202\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Jets:\", len(reco_jets))\n",
    "\n",
    "\n",
    "reco_jet_pt = [jet.pt() for jet in reco_jets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49736597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TCanvas::Constructor>: Deleting canvas with same name: canvas\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ROOT\n",
    "import numpy as np\n",
    "\n",
    "# Define bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100, 200]\n",
    "\n",
    "# Initialize lists to store values\n",
    "x_vals = []\n",
    "ratio_iqr_median = []\n",
    "\n",
    "# Iterate over bins\n",
    "for i in range(len(bins) - 1):\n",
    "    lim_low = bins[i]\n",
    "    lim_hi = bins[i + 1]\n",
    "    x_vals.append(np.mean([lim_low, lim_hi]))\n",
    "\n",
    "    # Filter gen_jet pT values within the bin\n",
    "    gen_jet_values_in_bin = [pt for pt in gen_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Filter reco_jet pT values within the bin\n",
    "    reco_jet_values_in_bin = [pt for pt in reco_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Check if there are any values in both gen_jet_pt and reco_jet_pt\n",
    "    if len(gen_jet_values_in_bin) == 0 or len(reco_jet_values_in_bin) == 0:\n",
    "        # Append NaN values\n",
    "        ratio_iqr_median.append(np.nan)\n",
    "        continue\n",
    "\n",
    "    # Calculate IQR and median for reco_jet_pt / gen_jet_pt values\n",
    "    ratio_values_in_bin = [reco / gen for reco, gen in zip(reco_jet_values_in_bin, gen_jet_values_in_bin)]\n",
    "    ratio_iqr = np.percentile(ratio_values_in_bin, 75) - np.percentile(ratio_values_in_bin, 25)\n",
    "    ratio_median = np.median(ratio_values_in_bin)\n",
    "    ratio_iqr_median_ratio = ratio_iqr / ratio_median\n",
    "    ratio_iqr_median.append(ratio_iqr_median_ratio)\n",
    "\n",
    "# Filter out NaN values\n",
    "x_vals_filtered = [x for x, y in zip(x_vals, ratio_iqr_median) if not np.isnan(y)]\n",
    "ratio_iqr_median_filtered = [y for y in ratio_iqr_median if not np.isnan(y)]\n",
    "\n",
    "# Create a TGraph with filtered values\n",
    "gr_ratio = ROOT.TGraph(len(x_vals_filtered), np.array(x_vals_filtered), np.array(ratio_iqr_median_filtered))\n",
    "\n",
    "# Set the titles to empty strings\n",
    "gr_ratio.SetTitle(\"\")\n",
    "\n",
    "# Create a canvas\n",
    "canvas = ROOT.TCanvas(\"canvas\", \"Ratio IQR/Median vs Jet pT\", 800, 600)\n",
    "\n",
    "# Draw the graph with connecting lines\n",
    "gr_ratio.SetMarkerStyle(20)\n",
    "gr_ratio.SetMarkerColor(ROOT.kBlue)\n",
    "gr_ratio.SetLineColor(ROOT.kBlue)\n",
    "gr_ratio.GetXaxis().SetTitle(\"Jet PT, Gen (GeV)\")\n",
    "gr_ratio.GetYaxis().SetTitle(\"Response IQR/Median\")\n",
    "gr_ratio.Draw(\"APL\")\n",
    "\n",
    "# Get the X and Y axes\n",
    "xaxis = gr_ratio.GetXaxis()\n",
    "yaxis = gr_ratio.GetYaxis()\n",
    "\n",
    "# Set tick length for all four axes\n",
    "xaxis.SetTickLength(0.03)\n",
    "yaxis.SetTickLength(0.03)\n",
    "\n",
    "\n",
    "# Add legend\n",
    "legend = ROOT.TLegend(0.75, 0.75, 0.9, 0.9)\n",
    "legend.AddEntry(gr_ratio, \"Pred FP\", \"p\")\n",
    "legend.Draw()\n",
    "\n",
    "# Show the canvas\n",
    "canvas.Draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b80ef895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin 1: Number of events (Gen Jet): 16, Number of events (Reco Jet): 22\n",
      "Bin 2: Number of events (Gen Jet): 6, Number of events (Reco Jet): 8\n",
      "Bin 3: Number of events (Gen Jet): 7, Number of events (Reco Jet): 6\n",
      "Bin 4: Number of events (Gen Jet): 13, Number of events (Reco Jet): 6\n",
      "Bin 5: Number of events (Gen Jet): 6, Number of events (Reco Jet): 4\n",
      "Bin 6: Number of events (Gen Jet): 8, Number of events (Reco Jet): 2\n",
      "Bin 7: Number of events (Gen Jet): 6, Number of events (Reco Jet): 22\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100, 200]\n",
    "\n",
    "# Iterate over bins\n",
    "for i in range(len(bins) - 1):\n",
    "    lim_low = bins[i]\n",
    "    lim_hi = bins[i + 1]\n",
    "\n",
    "    # Filter gen_jet pT values within the bin\n",
    "    gen_jet_values_in_bin = [pt for pt in gen_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Filter reco_jet pT values within the bin\n",
    "    reco_jet_values_in_bin = [pt for pt in reco_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Calculate the number of events in the bin\n",
    "    num_events_gen_jet = len(gen_jet_values_in_bin)\n",
    "    num_events_reco_jet = len(reco_jet_values_in_bin)\n",
    "\n",
    "    print(f\"Bin {i+1}: Number of events (Gen Jet): {num_events_gen_jet}, Number of events (Reco Jet): {num_events_reco_jet}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "856e8331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TCanvas::Constructor>: Deleting canvas with same name: canvas_0\n",
      "Warning in <TROOT::Append>: Replacing existing TH1: hist_0 (Potential memory leak).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ROOT\n",
    "import numpy as np\n",
    "\n",
    "# Define bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100, 200]\n",
    "\n",
    "# Define the bin index for the first bin (index 0)\n",
    "bin_index = 0\n",
    "\n",
    "# Get the bin limits\n",
    "lim_low = bins[bin_index]\n",
    "lim_hi = bins[bin_index + 1]\n",
    "# Filter gen_jet pT values within the bin\n",
    "gen_jet_values_in_bin = [pt for pt in gen_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "# Filter reco_jet pT values within the bin\n",
    "reco_jet_values_in_bin = [pt for pt in reco_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "# Check if there are any values in both gen_jet_pt and reco_jet_pt\n",
    "if len(gen_jet_values_in_bin) == 0 or len(reco_jet_values_in_bin) == 0:\n",
    "    print(\"No events in the bin.\")\n",
    "else:\n",
    "    # Calculate IQR and median for reco_jet_pt / gen_jet_pt values\n",
    "    ratio_values_in_bin = [reco / gen for reco, gen in zip(reco_jet_values_in_bin, gen_jet_values_in_bin)]\n",
    "    ratio_iqr = np.percentile(ratio_values_in_bin, 75) - np.percentile(ratio_values_in_bin, 25)\n",
    "    ratio_median = np.median(ratio_values_in_bin)\n",
    "    ratio_iqr_median_ratio = ratio_iqr / ratio_median\n",
    "\n",
    "    # Create a canvas for the bin\n",
    "    canvas = ROOT.TCanvas(f\"canvas_{bin_index}\", f\"Response Distribution in Bin {bin_index + 1}\", 800, 600)\n",
    "\n",
    "    # Create histogram for the response distribution in the bin\n",
    "    hist = ROOT.TH1F(f\"hist_{bin_index}\", f\"  Bin {bin_index + 1}\", 20, 0, 2)  # Adjust binning as needed\n",
    "    for val in ratio_values_in_bin:\n",
    "        hist.Fill(val)\n",
    "\n",
    "    # Draw the histogram\n",
    "    hist.SetLineColor(ROOT.kBlue)\n",
    "    hist.Draw()\n",
    "\n",
    "    # Add legend\n",
    "    legend = ROOT.TLegend(0.75, 0.75, 0.9, 0.9)\n",
    "    legend.AddEntry(hist, f\"Bin {bin_index + 1}\", \"l\")\n",
    "    legend.Draw()\n",
    "\n",
    "    # Show the canvas\n",
    "    canvas.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed681ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "203a0d91",
   "metadata": {},
   "source": [
    "# Quantization INT8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d2e0d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/ao/quantization/observer.py:1207: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.qconfig = torch.ao.quantization.get_default_qconfig('onednn')\n",
    "custom_module_config = {\n",
    "        \"float_to_observed_custom_module_class\": {torch.nn.MultiheadAttention: QuantizeableMultiheadAttention},\n",
    "        \"observed_to_quantized_custom_module_class\": {QuantizeableMultiheadAttention: QuantizedMultiheadAttention},\n",
    "}\n",
    "\n",
    "model_prepared = torch.ao.quantization.prepare(model, prepare_custom_config_dict=custom_module_config)\n",
    "\n",
    "#calibrate on data\n",
    "num_events_to_calibrate = 100\n",
    "for ind in range(max_events_train,max_events_train+num_events_to_calibrate):\n",
    "    _X = torch.unsqueeze(torch.tensor(ds_train[ind][\"X\"]).to(torch.float32), 0)\n",
    "    _mask = _X[:, :, 0]!=0\n",
    "    model_prepared(_X, _mask)\n",
    "\n",
    "model_int8 = torch.ao.quantization.convert(model_prepared,convert_custom_config_dict=custom_module_config,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12a705b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizeFeaturesStub(\n",
       "  (quants): ModuleList(\n",
       "    (0): Quantize(scale=tensor([0.0078]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "    (1): Quantize(scale=tensor([0.0396]), zero_point=tensor([138]), dtype=torch.quint8)\n",
       "    (2): Quantize(scale=tensor([0.0342]), zero_point=tensor([129]), dtype=torch.quint8)\n",
       "    (3): Quantize(scale=tensor([0.0078]), zero_point=tensor([127]), dtype=torch.quint8)\n",
       "    (4): Quantize(scale=tensor([0.0078]), zero_point=tensor([128]), dtype=torch.quint8)\n",
       "    (5): Quantize(scale=tensor([0.0339]), zero_point=tensor([117]), dtype=torch.quint8)\n",
       "    (6): Quantize(scale=tensor([49.7997]), zero_point=tensor([62]), dtype=torch.quint8)\n",
       "    (7): Quantize(scale=tensor([22.5108]), zero_point=tensor([128]), dtype=torch.quint8)\n",
       "    (8): Quantize(scale=tensor([30.9407]), zero_point=tensor([130]), dtype=torch.quint8)\n",
       "    (9): Quantize(scale=tensor([0.0122]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "    (10): Quantize(scale=tensor([3.2249]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "    (11): Quantize(scale=tensor([0.3067]), zero_point=tensor([25]), dtype=torch.quint8)\n",
       "    (12): Quantize(scale=tensor([0.4297]), zero_point=tensor([130]), dtype=torch.quint8)\n",
       "    (13): Quantize(scale=tensor([8.0287]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "    (14): Quantize(scale=tensor([9.5126]), zero_point=tensor([151]), dtype=torch.quint8)\n",
       "    (15): Quantize(scale=tensor([4.2339]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "    (16): Quantize(scale=tensor([3.6354]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "    (17-19): 3 x Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_int8.quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "999e620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_quantized = torch.quantize_per_tensor((X_features_padded[:, :, 0]!=0).to(torch.float32), 1, 0, torch.quint8)\n",
    "preds = model_int8(X_features_padded, mask_quantized)\n",
    "preds = preds[0].detach(), preds[1].detach()\n",
    "preds_unpacked_int8 = unpack_predictions(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "272d479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_int8 = mlpf_loss(targets_unpacked, preds_unpacked_int8, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d010742f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Final total loss')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(2), [loss[\"Total\"].detach().numpy(), loss_int8[\"Total\"].detach().numpy()])\n",
    "plt.xticks(range(2), [\"fp32\", \"int8\"])\n",
    "plt.ylabel(\"Final total loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2214072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_pred_int8 = preds_unpacked_int8[\"pt\"][msk_true_particles].numpy()\n",
    "eta_pred_int8 = preds_unpacked_int8[\"eta\"][msk_true_particles].numpy()\n",
    "sphi_pred_int8 = preds_unpacked_int8[\"sin_phi\"][msk_true_particles].numpy()\n",
    "cphi_pred_int8 = preds_unpacked_int8[\"cos_phi\"][msk_true_particles].numpy()\n",
    "energy_pred_int8 = preds_unpacked_int8[\"energy\"][msk_true_particles].numpy()\n",
    "\n",
    "px = preds_unpacked_int8[\"pt\"] * preds_unpacked_int8[\"cos_phi\"] * msk_true_particles\n",
    "py = preds_unpacked_int8[\"pt\"] * preds_unpacked_int8[\"sin_phi\"] * msk_true_particles\n",
    "pred_met_int8 = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07044dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fda66df6dc0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(pred_met/true_met, bins=np.linspace(0,5,61), histtype=\"step\", lw=2, label=\"fp32\");\n",
    "plt.hist(pred_met_int8/true_met, bins=np.linspace(0,5,61), histtype=\"step\", lw=2, label=\"int8\");\n",
    "plt.xlabel(\"reco_met / gen_met\")\n",
    "plt.ylabel(\"number of events / bin\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b3255af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the 3-momentum for the quantized particles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7fe474c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "px_int8 = preds_unpacked_int8[\"pt\"] * preds_unpacked_int8[\"cos_phi\"] * msk_true_particles\n",
    "py_int8 = preds_unpacked_int8[\"pt\"] * preds_unpacked_int8[\"sin_phi\"] * msk_true_particles\n",
    "pz_int8 = preds_unpacked_int8[\"pt\"] * np.sinh(preds_unpacked_int8[\"eta\"]) * msk_true_particles\n",
    "phi_int8 = np.arctan2(preds_unpacked_int8[\"sin_phi\"], preds_unpacked_int8[\"cos_phi\"]) * msk_true_particles\n",
    "\n",
    "px_np_int8 = px_int8.detach().cpu().numpy()\n",
    "py_np_int8 = py_int8.detach().cpu().numpy()\n",
    "pz_np_int8 = pz_int8.detach().cpu().numpy()\n",
    "phi_np_int8 = phi_int8.detach().cpu().numpy()\n",
    "\n",
    "# print(\"px_np\", px_np)\n",
    "# print(\"py_np\", py_np)\n",
    "# print(\"pz_np\", pz_np)\n",
    "# print(\"phi_np\", phi_np)\n",
    "\n",
    "quantized_mom = np.sqrt(np.sum(px_np_int8, axis=1)**2 + np.sum(py_np_int8, axis=1)**2 + np.sum(pz_np_int8, axis=1)**2)\n",
    "int8_E_np = np.sqrt(px_np_int8**2 + py_np_int8**2 + pz_np_int8**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f59b8cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int8_E Shape (16, 200)\n",
      "px Shape (16, 200)\n",
      "py Shape (16, 200)\n",
      "pz Shape (16, 200)\n"
     ]
    }
   ],
   "source": [
    "# four-vectors\n",
    "# px_py_pz_E = np.column_stack((px_np, py_np, pz_np, E)) \n",
    "print(\"int8_E Shape\", int8_E_np.shape)\n",
    "print(\"px Shape\", px_np_int8.shape)\n",
    "print(\"py Shape\", py_np_int8.shape)\n",
    "print(\"pz Shape\", pz_np_int8.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cbb0d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INT8_Jets_particles = []   # TODO:Change this to reco_jets\n",
    "for ip in range(int8_E_np.shape[0]):\n",
    "    for ix in range(int8_E_np.shape[1]):\n",
    "        px_value = float(px_np_int8[ip, ix])\n",
    "        py_value = float(py_np_int8[ip, ix])\n",
    "        pz_value = float(pz_np_int8[ip, ix])\n",
    "        E_value = float(int8_E_np[ip, ix])\n",
    "        particle = fj.PseudoJet(px_value, py_value, pz_value, E_value)\n",
    "        INT8_Jets_particles.append(particle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "983511e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INT8_Jets_particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1602ea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "num_threads = 2 \n",
    "\n",
    "def cluster_jets(particles):\n",
    "    jetdef = fj.JetDefinition(fj.antikt_algorithm, 0.4)\n",
    "#     jet_ptcut = 20\n",
    "    \n",
    "    cluster = fj.ClusterSequence(INT8_Jets_particles, jetdef)\n",
    "    jets = cluster.inclusive_jets()\n",
    "    \n",
    "    return jets\n",
    "\n",
    "chunks = [particles[i::num_threads] for i in range(num_threads)]\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    futures = [executor.submit(cluster_jets, chunk) for chunk in chunks]\n",
    "\n",
    "INT8_reco_jets = []\n",
    "for future in futures:\n",
    "    INT8_reco_jets.extend(future.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "39c90542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jet 1 : 0.0 100000.0 0.0 0.0\n",
      "Jet 2 : 0.042896021468405335 0.28781828540681575 5.077070008271944 0.044685062021017075\n",
      "Jet 3 : 0.05048450439488717 -2.9105019810120414 2.950329933305678 0.46497398614883423\n",
      "Jet 4 : 0.2623754022198784 -4.026813416016047 0.4607782343929308 7.3595967292785645\n",
      "Jet 5 : 0.30600831951709684 -4.362343938649678 5.250558280556705 12.003716468811035\n",
      "Jet 6 : 0.5067151736707779 0.7875412244189958 3.5862781296310144 0.67214435338974\n",
      "Jet 7 : 0.6010212862610597 -3.5409234801126344 0.22132971003957175 10.375956535339355\n",
      "Jet 8 : 0.6487825550245411 -3.947957004875568 1.6607264212054387 16.819273471832275\n",
      "Jet 9 : 0.6661709856550436 -3.158910005574564 1.9913328398502137 7.856617450714111\n",
      "Jet 10 : 0.8367933342235239 2.581819108752742 3.542584139366838 5.563338279724121\n",
      "Jet 11 : 1.1025917268919003 1.4188344006907518 3.379453132508136 2.411531925201416\n",
      "Jet 12 : 1.1628402659739723 -3.3834859336681262 4.48114216899547 17.156118392944336\n",
      "Jet 13 : 1.1899790561526709 -3.2631542284810373 6.085463640796483 15.57093620300293\n",
      "Jet 14 : 1.2174030729120064 -3.5938702834818814 1.3183388131951626 22.15799903869629\n",
      "Jet 15 : 1.4164392717987537 4.705041366541018 6.1996511138931005 78.2667007446289\n",
      "Jet 16 : 1.4708324817722227 0.7332973975900844 4.61586810748538 1.8883703351020813\n",
      "Jet 17 : 1.916148822003898 2.79920540290472 2.1569480035843545 15.801888465881348\n",
      "Jet 18 : 1.9430896343238657 -2.230717532780804 0.9833434441388534 9.146116256713867\n",
      "Jet 19 : 1.9736286594023333 3.2221295925745923 3.0979933329525937 24.79010581970215\n",
      "Jet 20 : 2.2682807791300883 -1.0702547959119784 4.208827259150869 3.696424186229706\n",
      "Jet 21 : 2.305333832721721 -1.2404409289674057 1.7666987887347574 4.318344593048096\n",
      "Jet 22 : 2.4900192928899614 3.4128515228757887 2.3320864687048624 37.830748558044434\n",
      "Jet 23 : 2.4973866218879133 -1.8683804178065104 4.3241090801043685 8.281546592712402\n",
      "Jet 24 : 2.513680151301318 2.192302300479524 3.7207000023678787 11.396380126476288\n",
      "Jet 25 : 2.6407794555221527 -2.57051191248577 5.383510646610109 17.361820220947266\n",
      "Jet 26 : 2.729243214219466 0.9391108170024977 0.30670324016722594 4.027019485831261\n",
      "Jet 27 : 3.188651806023038 -2.7838690979580565 1.8607502522778727 25.89987850189209\n",
      "Jet 28 : 2.9149981865223644 -0.8211253959259233 2.0672740701600025 3.9541842937469482\n",
      "Jet 29 : 3.2691386531111912 1.1156484205159694 4.948394963839672 5.523616790771484\n",
      "Jet 30 : 3.341370582725543 -2.0070678368040804 2.447564308655465 12.65685749053955\n",
      "Jet 31 : 3.5233209531659524 -0.32302306851807117 5.474180176845253 3.7087433338165283\n",
      "Jet 32 : 3.5599732174864163 2.56231423486316 1.7596318532501054 23.220162391662598\n",
      "Jet 33 : 3.5594432195377768 -1.7197532401090458 6.130858101431677 10.255220413208008\n",
      "Jet 34 : 3.80200183292518 1.072169304282596 1.7795285000472787 6.2048211097717285\n",
      "Jet 35 : 4.126240641864167 2.615797863829362 5.450787411463872 28.372966766357422\n",
      "Jet 36 : 4.49444148977964 1.4501348201286655 5.253667599697199 10.108510971069336\n",
      "Jet 37 : 4.996653944396501 -1.8158621544175737 4.7514397075087516 15.777231931686401\n",
      "Jet 38 : 4.728357019467551 3.2991401177370943 5.799525751853987 64.13144874572754\n",
      "Jet 39 : 5.302811740163217 -3.040303760273555 5.430006983972476 55.57622468471527\n",
      "Jet 40 : 6.912091287251132 -2.7817939156443074 5.823646589174096 56.036927700042725\n",
      "Jet 41 : 5.809367317645437 2.0235245720849475 2.816993658220651 22.35781915485859\n",
      "Jet 42 : 7.900171980529625 -2.5611957377888497 4.698027834506109 51.48495864868164\n",
      "Jet 43 : 7.525714532317134 2.7906563706039837 5.9657581368967705 61.54724597930908\n",
      "Jet 44 : 8.626037176837329 2.3793116173785394 5.924832839710763 46.993045806884766\n",
      "Jet 45 : 14.713553117688738 1.6203665141852748 5.99488954545105 38.79111671447754\n",
      "Jet 46 : 7.552752007172549 -0.06968296194018551 4.074102794989107 7.571096420288086\n",
      "Jet 47 : 10.084970402147231 -3.266197031588419 0.8274047813242396 132.37844944000244\n",
      "Jet 48 : 8.51582312015258 -2.5917120748336973 3.9141906426900044 57.1734414100647\n",
      "Jet 49 : 8.54762720623228 0.716919946727014 5.34625017933427 10.83996868133545\n",
      "Jet 50 : 8.91367899996295 1.6767279453503177 4.623519698025664 24.694725275039673\n",
      "Jet 51 : 10.283425324695246 -2.3092178157203755 1.9917088295515286 52.293808937072754\n",
      "Jet 52 : 9.232058392519752 0.03953093447860013 0.49436774266898453 9.292068183422089\n",
      "Jet 53 : 9.650795749723292 0.3465865779410999 1.2574741539006427 10.288460731506348\n",
      "Jet 54 : 20.277636837565495 0.8101137373503537 5.952864690780622 27.75037518143654\n",
      "Jet 55 : 10.286716946733874 0.08448402713909203 6.053359892981841 10.368640661239624\n",
      "Jet 56 : 10.572684841706783 -0.16841793512051753 1.4640216190749054 10.825427293777466\n",
      "Jet 57 : 25.151786326622947 -2.8048339904139628 0.3596878526160879 208.66809034347534\n",
      "Jet 58 : 14.51722245827294 -3.013914990500147 1.50650750327557 148.2211742401123\n",
      "Jet 59 : 18.288428636412505 0.48900706066391536 0.18537596876291546 20.820199951529503\n",
      "Jet 60 : 21.795734981367158 -2.6747789178337196 1.0193045289510756 158.9570164680481\n",
      "Jet 61 : 23.817895795421567 -0.6776085186550909 5.753112628565197 30.05847518146038\n",
      "Jet 62 : 25.391724711409505 -0.7317425468444969 1.401929750542586 33.01580882072449\n",
      "Jet 63 : 12.684827872484215 3.8305146084392074 0.7086885508219669 292.4389762878418\n",
      "Jet 64 : 16.55672919342653 -1.3219933134760125 0.017953028546234505 33.415549993515015\n",
      "Jet 65 : 14.4638069042947 2.7099283870508493 0.7552964373502129 109.16851758956909\n",
      "Jet 66 : 15.728722558403085 -2.072442387647789 5.7147514806034225 63.49968719482422\n",
      "Jet 67 : 15.90174182278087 -2.261717669548425 4.1548224488423555 77.19127631187439\n",
      "Jet 68 : 21.69155706013458 -1.4510998924257144 5.635380045039704 49.040611267089844\n",
      "Jet 69 : 18.39699823987733 -0.7949865677183744 0.2817435591477374 24.69789233803749\n",
      "Jet 70 : 20.13255968451716 -0.3203406646934932 0.25204641089818314 21.493557691574097\n",
      "Jet 71 : 17.59927833267218 3.2127922552620234 0.9664560349570868 219.0189208984375\n",
      "Jet 72 : 23.7707264482665 0.2769314348999508 1.7105439332428618 24.97911310195923\n",
      "Jet 73 : 34.20551716617585 -1.7323196572043509 1.7365053197816698 99.97253847122192\n",
      "Jet 74 : 24.29522559832774 -2.148538536629023 5.26330470557027 105.64615440368652\n",
      "Jet 75 : 24.207217949930932 2.7148601597081123 3.083887699357279 183.64757752418518\n",
      "Jet 76 : 40.90516587518888 -2.1890976775389097 6.14032036339049 185.12443709373474\n",
      "Jet 77 : 23.088545966498135 2.1431598378688803 4.810600227261097 99.85838508605957\n",
      "Jet 78 : 28.37725504154881 2.324639649700366 2.427634719304411 146.5762640624307\n",
      "Jet 79 : 22.240153097058574 2.5002563883267874 1.2797412958853924 136.46600818634033\n",
      "Jet 80 : 21.918482775979935 2.5457091376577177 4.55217474577254 140.63394737243652\n",
      "Jet 81 : 35.749310184062374 1.9653352828396975 0.1780651705454512 130.26742953062057\n",
      "Jet 82 : 27.672172974215705 1.9801735763627188 5.553879771742505 102.25956058502197\n",
      "Jet 83 : 26.398829926668117 1.309122549467527 2.19937778673369 52.545250713825226\n",
      "Jet 84 : 44.08885170063238 -1.6610793475361616 0.34214411182486254 120.6432728767395\n",
      "Jet 85 : 70.34953230416305 1.247112819213975 4.049978529105964 133.7806363105774\n",
      "Jet 86 : 42.33179751952645 2.0817937869960144 1.7725697061054382 172.5635757446289\n",
      "Jet 87 : 31.051925974923194 -1.2143323603316405 2.7744285744627977 57.043797969818115\n",
      "Jet 88 : 58.02071457346972 3.140512087547765 0.42151187868845086 671.9808106422424\n",
      "Jet 89 : 74.11715520111586 -2.1901721111433994 0.514678370800596 335.8001526594162\n",
      "Jet 90 : 40.49206590700664 -2.459585565674423 3.3850506612442834 238.6330542564392\n",
      "Jet 91 : 48.39438494821236 -1.2869306378529661 1.3616198762978513 94.76544988155365\n",
      "Jet 92 : 130.46202287571265 -0.3794429636892167 0.8440402307917519 143.26753736659884\n",
      "Jet 93 : 55.466947990391574 1.0454239382655404 1.3769918437462034 89.19150367379189\n",
      "Jet 94 : 95.9250365139796 -2.351368523862515 1.4593980351416902 508.6838208436966\n",
      "Jet 95 : 45.21475826561299 2.6524270044318414 4.125509508246989 322.38049364089966\n",
      "Jet 96 : 70.99004523565577 2.1011969166864564 1.1717295894813655 294.8598771095276\n",
      "Jet 97 : 58.246357840989866 0.5084527898094648 4.078281055822133 66.70750117301941\n",
      "Jet 98 : 91.5515329713781 2.523576036619783 0.24859314931883753 575.049186706543\n",
      "Jet 99 : 52.70311220665374 -0.4581533425352106 3.456053080222644 58.49049687385559\n",
      "Jet 100 : 84.96641795260331 -2.1394445678400915 3.7434128625582925 366.30638743937016\n",
      "Jet 101 : 56.35405286020733 -2.2438549484621624 2.813792796571015 268.716402053833\n",
      "Jet 102 : 127.26661048265208 0.36685174645219853 0.8264625408306502 139.526719879359\n",
      "Jet 103 : 188.6454375550947 -1.8521343593291588 0.9946168336228198 618.5612887740135\n",
      "Jet 104 : 76.73648352285163 -1.6516685238243627 3.2074002447355667 208.12503588199615\n",
      "Jet 105 : 135.32197874122113 1.606383576427109 1.3160488245342716 352.38024113141\n",
      "Jet 106 : 126.51804909450472 1.4234931367415231 0.249150429445947 279.70590978860855\n",
      "Jet 107 : 230.82392881350393 -1.2081386553569784 0.8111647402702008 425.3538479208946\n",
      "Jet 108 : 85.81219505848104 -0.9984878604338469 3.434031910992161 133.1160652935505\n",
      "Jet 109 : 88.87134882636673 2.1726481684758445 3.320146811434594 395.52654671669006\n",
      "Jet 110 : 133.7025587448014 -0.6225100794640183 4.354651675322445 161.9413719177246\n",
      "Jet 111 : 431.0923296597571 2.279194837459981 0.6986149800736863 2129.86682677269\n",
      "Jet 112 : 349.2249438791375 2.020566314677983 4.205146350955067 1343.0333963632584\n",
      "Jet 113 : 467.93817637735395 1.0306461180463897 0.783955001271642 747.3268901705742\n",
      "Jet 114 : 931.9578889545319 1.6714258274750053 0.7297199115069526 2576.893737182021\n",
      "Jet 115 : 424.9736487899823 -1.597013839718099 3.925014243988704 1097.0097351968288\n",
      "Jet 116 : 500.5258491970993 -0.6861336717591854 3.935347290188496 635.0578958876431\n",
      "Jet 117 : 0.0 100000.0 0.0 0.0\n",
      "Jet 118 : 0.042896021468405335 0.28781828540681575 5.077070008271944 0.044685062021017075\n",
      "Jet 119 : 0.05048450439488717 -2.9105019810120414 2.950329933305678 0.46497398614883423\n",
      "Jet 120 : 0.2623754022198784 -4.026813416016047 0.4607782343929308 7.3595967292785645\n",
      "Jet 121 : 0.30600831951709684 -4.362343938649678 5.250558280556705 12.003716468811035\n",
      "Jet 122 : 0.5067151736707779 0.7875412244189958 3.5862781296310144 0.67214435338974\n",
      "Jet 123 : 0.6010212862610597 -3.5409234801126344 0.22132971003957175 10.375956535339355\n",
      "Jet 124 : 0.6487825550245411 -3.947957004875568 1.6607264212054387 16.819273471832275\n",
      "Jet 125 : 0.6661709856550436 -3.158910005574564 1.9913328398502137 7.856617450714111\n",
      "Jet 126 : 0.8367933342235239 2.581819108752742 3.542584139366838 5.563338279724121\n",
      "Jet 127 : 1.1025917268919003 1.4188344006907518 3.379453132508136 2.411531925201416\n",
      "Jet 128 : 1.1628402659739723 -3.3834859336681262 4.48114216899547 17.156118392944336\n",
      "Jet 129 : 1.1899790561526709 -3.2631542284810373 6.085463640796483 15.57093620300293\n",
      "Jet 130 : 1.2174030729120064 -3.5938702834818814 1.3183388131951626 22.15799903869629\n",
      "Jet 131 : 1.4164392717987537 4.705041366541018 6.1996511138931005 78.2667007446289\n",
      "Jet 132 : 1.4708324817722227 0.7332973975900844 4.61586810748538 1.8883703351020813\n",
      "Jet 133 : 1.916148822003898 2.79920540290472 2.1569480035843545 15.801888465881348\n",
      "Jet 134 : 1.9430896343238657 -2.230717532780804 0.9833434441388534 9.146116256713867\n",
      "Jet 135 : 1.9736286594023333 3.2221295925745923 3.0979933329525937 24.79010581970215\n",
      "Jet 136 : 2.2682807791300883 -1.0702547959119784 4.208827259150869 3.696424186229706\n",
      "Jet 137 : 2.305333832721721 -1.2404409289674057 1.7666987887347574 4.318344593048096\n",
      "Jet 138 : 2.4900192928899614 3.4128515228757887 2.3320864687048624 37.830748558044434\n",
      "Jet 139 : 2.4973866218879133 -1.8683804178065104 4.3241090801043685 8.281546592712402\n",
      "Jet 140 : 2.513680151301318 2.192302300479524 3.7207000023678787 11.396380126476288\n",
      "Jet 141 : 2.6407794555221527 -2.57051191248577 5.383510646610109 17.361820220947266\n",
      "Jet 142 : 2.729243214219466 0.9391108170024977 0.30670324016722594 4.027019485831261\n",
      "Jet 143 : 3.188651806023038 -2.7838690979580565 1.8607502522778727 25.89987850189209\n",
      "Jet 144 : 2.9149981865223644 -0.8211253959259233 2.0672740701600025 3.9541842937469482\n",
      "Jet 145 : 3.2691386531111912 1.1156484205159694 4.948394963839672 5.523616790771484\n",
      "Jet 146 : 3.341370582725543 -2.0070678368040804 2.447564308655465 12.65685749053955\n",
      "Jet 147 : 3.5233209531659524 -0.32302306851807117 5.474180176845253 3.7087433338165283\n",
      "Jet 148 : 3.5599732174864163 2.56231423486316 1.7596318532501054 23.220162391662598\n",
      "Jet 149 : 3.5594432195377768 -1.7197532401090458 6.130858101431677 10.255220413208008\n",
      "Jet 150 : 3.80200183292518 1.072169304282596 1.7795285000472787 6.2048211097717285\n",
      "Jet 151 : 4.126240641864167 2.615797863829362 5.450787411463872 28.372966766357422\n",
      "Jet 152 : 4.49444148977964 1.4501348201286655 5.253667599697199 10.108510971069336\n",
      "Jet 153 : 4.996653944396501 -1.8158621544175737 4.7514397075087516 15.777231931686401\n",
      "Jet 154 : 4.728357019467551 3.2991401177370943 5.799525751853987 64.13144874572754\n",
      "Jet 155 : 5.302811740163217 -3.040303760273555 5.430006983972476 55.57622468471527\n",
      "Jet 156 : 6.912091287251132 -2.7817939156443074 5.823646589174096 56.036927700042725\n",
      "Jet 157 : 5.809367317645437 2.0235245720849475 2.816993658220651 22.35781915485859\n",
      "Jet 158 : 7.900171980529625 -2.5611957377888497 4.698027834506109 51.48495864868164\n",
      "Jet 159 : 7.525714532317134 2.7906563706039837 5.9657581368967705 61.54724597930908\n",
      "Jet 160 : 8.626037176837329 2.3793116173785394 5.924832839710763 46.993045806884766\n",
      "Jet 161 : 14.713553117688738 1.6203665141852748 5.99488954545105 38.79111671447754\n",
      "Jet 162 : 7.552752007172549 -0.06968296194018551 4.074102794989107 7.571096420288086\n",
      "Jet 163 : 10.084970402147231 -3.266197031588419 0.8274047813242396 132.37844944000244\n",
      "Jet 164 : 8.51582312015258 -2.5917120748336973 3.9141906426900044 57.1734414100647\n",
      "Jet 165 : 8.54762720623228 0.716919946727014 5.34625017933427 10.83996868133545\n",
      "Jet 166 : 8.91367899996295 1.6767279453503177 4.623519698025664 24.694725275039673\n",
      "Jet 167 : 10.283425324695246 -2.3092178157203755 1.9917088295515286 52.293808937072754\n",
      "Jet 168 : 9.232058392519752 0.03953093447860013 0.49436774266898453 9.292068183422089\n",
      "Jet 169 : 9.650795749723292 0.3465865779410999 1.2574741539006427 10.288460731506348\n",
      "Jet 170 : 20.277636837565495 0.8101137373503537 5.952864690780622 27.75037518143654\n",
      "Jet 171 : 10.286716946733874 0.08448402713909203 6.053359892981841 10.368640661239624\n",
      "Jet 172 : 10.572684841706783 -0.16841793512051753 1.4640216190749054 10.825427293777466\n",
      "Jet 173 : 25.151786326622947 -2.8048339904139628 0.3596878526160879 208.66809034347534\n",
      "Jet 174 : 14.51722245827294 -3.013914990500147 1.50650750327557 148.2211742401123\n",
      "Jet 175 : 18.288428636412505 0.48900706066391536 0.18537596876291546 20.820199951529503\n",
      "Jet 176 : 21.795734981367158 -2.6747789178337196 1.0193045289510756 158.9570164680481\n",
      "Jet 177 : 23.817895795421567 -0.6776085186550909 5.753112628565197 30.05847518146038\n",
      "Jet 178 : 25.391724711409505 -0.7317425468444969 1.401929750542586 33.01580882072449\n",
      "Jet 179 : 12.684827872484215 3.8305146084392074 0.7086885508219669 292.4389762878418\n",
      "Jet 180 : 16.55672919342653 -1.3219933134760125 0.017953028546234505 33.415549993515015\n",
      "Jet 181 : 14.4638069042947 2.7099283870508493 0.7552964373502129 109.16851758956909\n",
      "Jet 182 : 15.728722558403085 -2.072442387647789 5.7147514806034225 63.49968719482422\n",
      "Jet 183 : 15.90174182278087 -2.261717669548425 4.1548224488423555 77.19127631187439\n",
      "Jet 184 : 21.69155706013458 -1.4510998924257144 5.635380045039704 49.040611267089844\n",
      "Jet 185 : 18.39699823987733 -0.7949865677183744 0.2817435591477374 24.69789233803749\n",
      "Jet 186 : 20.13255968451716 -0.3203406646934932 0.25204641089818314 21.493557691574097\n",
      "Jet 187 : 17.59927833267218 3.2127922552620234 0.9664560349570868 219.0189208984375\n",
      "Jet 188 : 23.7707264482665 0.2769314348999508 1.7105439332428618 24.97911310195923\n",
      "Jet 189 : 34.20551716617585 -1.7323196572043509 1.7365053197816698 99.97253847122192\n",
      "Jet 190 : 24.29522559832774 -2.148538536629023 5.26330470557027 105.64615440368652\n",
      "Jet 191 : 24.207217949930932 2.7148601597081123 3.083887699357279 183.64757752418518\n",
      "Jet 192 : 40.90516587518888 -2.1890976775389097 6.14032036339049 185.12443709373474\n",
      "Jet 193 : 23.088545966498135 2.1431598378688803 4.810600227261097 99.85838508605957\n",
      "Jet 194 : 28.37725504154881 2.324639649700366 2.427634719304411 146.5762640624307\n",
      "Jet 195 : 22.240153097058574 2.5002563883267874 1.2797412958853924 136.46600818634033\n",
      "Jet 196 : 21.918482775979935 2.5457091376577177 4.55217474577254 140.63394737243652\n",
      "Jet 197 : 35.749310184062374 1.9653352828396975 0.1780651705454512 130.26742953062057\n",
      "Jet 198 : 27.672172974215705 1.9801735763627188 5.553879771742505 102.25956058502197\n",
      "Jet 199 : 26.398829926668117 1.309122549467527 2.19937778673369 52.545250713825226\n",
      "Jet 200 : 44.08885170063238 -1.6610793475361616 0.34214411182486254 120.6432728767395\n",
      "Jet 201 : 70.34953230416305 1.247112819213975 4.049978529105964 133.7806363105774\n",
      "Jet 202 : 42.33179751952645 2.0817937869960144 1.7725697061054382 172.5635757446289\n",
      "Jet 203 : 31.051925974923194 -1.2143323603316405 2.7744285744627977 57.043797969818115\n",
      "Jet 204 : 58.02071457346972 3.140512087547765 0.42151187868845086 671.9808106422424\n",
      "Jet 205 : 74.11715520111586 -2.1901721111433994 0.514678370800596 335.8001526594162\n",
      "Jet 206 : 40.49206590700664 -2.459585565674423 3.3850506612442834 238.6330542564392\n",
      "Jet 207 : 48.39438494821236 -1.2869306378529661 1.3616198762978513 94.76544988155365\n",
      "Jet 208 : 130.46202287571265 -0.3794429636892167 0.8440402307917519 143.26753736659884\n",
      "Jet 209 : 55.466947990391574 1.0454239382655404 1.3769918437462034 89.19150367379189\n",
      "Jet 210 : 95.9250365139796 -2.351368523862515 1.4593980351416902 508.6838208436966\n",
      "Jet 211 : 45.21475826561299 2.6524270044318414 4.125509508246989 322.38049364089966\n",
      "Jet 212 : 70.99004523565577 2.1011969166864564 1.1717295894813655 294.8598771095276\n",
      "Jet 213 : 58.246357840989866 0.5084527898094648 4.078281055822133 66.70750117301941\n",
      "Jet 214 : 91.5515329713781 2.523576036619783 0.24859314931883753 575.049186706543\n",
      "Jet 215 : 52.70311220665374 -0.4581533425352106 3.456053080222644 58.49049687385559\n",
      "Jet 216 : 84.96641795260331 -2.1394445678400915 3.7434128625582925 366.30638743937016\n",
      "Jet 217 : 56.35405286020733 -2.2438549484621624 2.813792796571015 268.716402053833\n",
      "Jet 218 : 127.26661048265208 0.36685174645219853 0.8264625408306502 139.526719879359\n",
      "Jet 219 : 188.6454375550947 -1.8521343593291588 0.9946168336228198 618.5612887740135\n",
      "Jet 220 : 76.73648352285163 -1.6516685238243627 3.2074002447355667 208.12503588199615\n",
      "Jet 221 : 135.32197874122113 1.606383576427109 1.3160488245342716 352.38024113141\n",
      "Jet 222 : 126.51804909450472 1.4234931367415231 0.249150429445947 279.70590978860855\n",
      "Jet 223 : 230.82392881350393 -1.2081386553569784 0.8111647402702008 425.3538479208946\n",
      "Jet 224 : 85.81219505848104 -0.9984878604338469 3.434031910992161 133.1160652935505\n",
      "Jet 225 : 88.87134882636673 2.1726481684758445 3.320146811434594 395.52654671669006\n",
      "Jet 226 : 133.7025587448014 -0.6225100794640183 4.354651675322445 161.9413719177246\n",
      "Jet 227 : 431.0923296597571 2.279194837459981 0.6986149800736863 2129.86682677269\n",
      "Jet 228 : 349.2249438791375 2.020566314677983 4.205146350955067 1343.0333963632584\n",
      "Jet 229 : 467.93817637735395 1.0306461180463897 0.783955001271642 747.3268901705742\n",
      "Jet 230 : 931.9578889545319 1.6714258274750053 0.7297199115069526 2576.893737182021\n",
      "Jet 231 : 424.9736487899823 -1.597013839718099 3.925014243988704 1097.0097351968288\n",
      "Jet 232 : 500.5258491970993 -0.6861336717591854 3.935347290188496 635.0578958876431\n"
     ]
    }
   ],
   "source": [
    "for i, jet in enumerate(INT8_reco_jets):\n",
    "    print(\"Jet\", i+1, \":\", jet.pt(), jet.eta(), jet.phi(), jet.e())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "74cd0cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Jets: 232\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Jets:\", len(INT8_reco_jets))\n",
    "\n",
    "\n",
    "INT8_jet_pt = [jet.pt() for jet in INT8_reco_jets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "125547ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ROOT\n",
    "import numpy as np\n",
    "\n",
    "# Define bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100, 200]\n",
    "\n",
    "# Initialize lists to store values\n",
    "x_vals = []\n",
    "reco_ratio_iqr_median = []\n",
    "int8_ratio_iqr_median = []\n",
    "\n",
    "# Iterate over bins\n",
    "for i in range(len(bins) - 1):\n",
    "    lim_low = bins[i]\n",
    "    lim_hi = bins[i + 1]\n",
    "    x_vals.append(np.mean([lim_low, lim_hi]))\n",
    "\n",
    "    # Filter gen_jet_pt values within the bin\n",
    "    gen_jet_values_in_bin = [pt for pt in gen_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Filter reco_jet_pt values within the bin\n",
    "    reco_jet_values_in_bin = [pt for pt in reco_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Filter INT8_jet_pt values within the bin\n",
    "    INT8_jet_values_in_bin = [pt for pt in INT8_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Check if there are any values in both gen_jet_pt and reco_jet_pt\n",
    "    if len(gen_jet_values_in_bin) == 0 or len(reco_jet_values_in_bin) == 0 or len(INT8_jet_values_in_bin) == 0:\n",
    "        # Append NaN values\n",
    "        reco_ratio_iqr_median.append(np.nan)\n",
    "        int8_ratio_iqr_median.append(np.nan)\n",
    "        continue\n",
    "\n",
    "    # Calculate IQR and median for reco_jet_pt / gen_jet_pt values\n",
    "    reco_ratio_values_in_bin = [reco / gen for reco, gen in zip(reco_jet_values_in_bin, gen_jet_values_in_bin)]\n",
    "    reco_ratio_iqr = np.percentile(reco_ratio_values_in_bin, 75) - np.percentile(reco_ratio_values_in_bin, 25)\n",
    "    reco_ratio_median = np.median(reco_ratio_values_in_bin)\n",
    "    reco_ratio_iqr_median_ratio = reco_ratio_iqr / reco_ratio_median\n",
    "    reco_ratio_iqr_median.append(reco_ratio_iqr_median_ratio)\n",
    "\n",
    "    # Calculate IQR and median for INT8_jet_pt / gen_jet_pt values\n",
    "    int8_ratio_values_in_bin = [int8 / gen for int8, gen in zip(INT8_jet_values_in_bin, gen_jet_values_in_bin)]\n",
    "    int8_ratio_iqr = np.percentile(int8_ratio_values_in_bin, 75) - np.percentile(int8_ratio_values_in_bin, 25)\n",
    "    int8_ratio_median = np.median(int8_ratio_values_in_bin)\n",
    "    int8_ratio_iqr_median_ratio = int8_ratio_iqr / int8_ratio_median\n",
    "    int8_ratio_iqr_median.append(int8_ratio_iqr_median_ratio)\n",
    "\n",
    "# Filter out NaN values\n",
    "x_vals_filtered_reco = [x for x, y in zip(x_vals, reco_ratio_iqr_median) if not np.isnan(y)]\n",
    "reco_ratio_iqr_median_filtered = [y for y in reco_ratio_iqr_median if not np.isnan(y)]\n",
    "\n",
    "x_vals_filtered_int8 = [x for x, y in zip(x_vals, int8_ratio_iqr_median) if not np.isnan(y)]\n",
    "int8_ratio_iqr_median_filtered = [y for y in int8_ratio_iqr_median if not np.isnan(y)]\n",
    "\n",
    "# Create TGraphs with filtered values\n",
    "gr_reco_ratio = ROOT.TGraph(len(x_vals_filtered_reco), np.array(x_vals_filtered_reco), np.array(reco_ratio_iqr_median_filtered))\n",
    "gr_int8_ratio = ROOT.TGraph(len(x_vals_filtered_int8), np.array(x_vals_filtered_int8), np.array(int8_ratio_iqr_median_filtered))\n",
    "\n",
    "# Set the titles to empty strings\n",
    "gr_reco_ratio.SetTitle(\"\")\n",
    "gr_int8_ratio.SetTitle(\"\")\n",
    "\n",
    "# Create a canvas\n",
    "canvas = ROOT.TCanvas(\"canvas\", \"Ratio IQR/Median vs Jet pT\", 800, 600)\n",
    "\n",
    "# Draw the first graph (Reco_jet_pt/gen_jet_pt)\n",
    "gr_reco_ratio.SetMarkerStyle(20)\n",
    "gr_reco_ratio.SetMarkerColor(ROOT.kBlue)\n",
    "gr_reco_ratio.SetLineColor(ROOT.kBlue)\n",
    "gr_reco_ratio.GetXaxis().SetTitle(\"Jet P_{T,gen} (GeV)\")\n",
    "gr_reco_ratio.GetYaxis().SetTitle(\"Response IQR / Median\")\n",
    "gr_reco_ratio.Draw(\"APL\")\n",
    "\n",
    "# Draw the second graph (INT8_jet_pt/gen_jet_pt) on the same canvas\n",
    "gr_int8_ratio.SetMarkerStyle(20)\n",
    "gr_int8_ratio.SetMarkerColor(ROOT.kRed)\n",
    "gr_int8_ratio.SetLineColor(ROOT.kRed)\n",
    "gr_int8_ratio.Draw(\"PL same\")\n",
    "\n",
    "# Get the X and Y axes\n",
    "xaxis = gr_reco_ratio.GetXaxis()\n",
    "yaxis = gr_reco_ratio.GetYaxis()\n",
    "\n",
    "# Set tick length for all four axes\n",
    "xaxis.SetTickLength(0.03)\n",
    "yaxis.SetTickLength(0.03)\n",
    "\n",
    "# Set y-axis range to show both plots\n",
    "min_y = min(min(reco_ratio_iqr_median_filtered), min(int8_ratio_iqr_median_filtered))\n",
    "max_y = max(max(reco_ratio_iqr_median_filtered), max(int8_ratio_iqr_median_filtered))\n",
    "yaxis.SetRangeUser(min_y * 0.9, max_y * 1.1)\n",
    "\n",
    "# Add legend\n",
    "legend = ROOT.TLegend(0.75, 0.75, 0.9, 0.9)\n",
    "legend.AddEntry(gr_reco_ratio, \"Pred_FP32\", \"lp\")\n",
    "legend.AddEntry(gr_int8_ratio, \"Pred_INT8\", \"lp\")\n",
    "legend.Draw()\n",
    "\n",
    "# Show the canvas\n",
    "canvas.Draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6726f15b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf9e55e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e07f45f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
