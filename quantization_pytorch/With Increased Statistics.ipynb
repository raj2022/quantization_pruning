{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d827d310",
   "metadata": {},
   "source": [
    "Task:\n",
    "1. Try to plot the Jet and MET pT response curve with increased statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "113ecb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.30/02\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import ROOT\n",
    "\n",
    "import vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d985af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import tensorflow_datasets as tfds\n",
    "import torch_geometric\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a55701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08a0192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "data_dir = \"../../mlpf/tensorflow_datasets/\"\n",
    "dataset = \"clic_edm_ttbar_pf\"\n",
    "\n",
    "# Load dataset\n",
    "builder = tfds.builder(dataset, data_dir=data_dir)\n",
    "ds_train = builder.as_data_source(split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efc1998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ea4423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_FEATURES_TRK = [\n",
    "    \"elemtype\",\n",
    "    \"pt\",\n",
    "    \"eta\",\n",
    "    \"sin_phi\",\n",
    "    \"cos_phi\",\n",
    "    \"p\",\n",
    "    \"chi2\",\n",
    "    \"ndf\",\n",
    "    \"dEdx\",\n",
    "    \"dEdxError\",\n",
    "    \"radiusOfInnermostHit\",\n",
    "    \"tanLambda\",\n",
    "    \"D0\",\n",
    "    \"omega\",\n",
    "    \"Z0\",\n",
    "    \"time\",\n",
    "]\n",
    "X_FEATURES_CL = [\n",
    "    \"elemtype\",\n",
    "    \"et\",\n",
    "    \"eta\",\n",
    "    \"sin_phi\",\n",
    "    \"cos_phi\",\n",
    "    \"energy\",\n",
    "    \"position.x\",\n",
    "    \"position.y\",\n",
    "    \"position.z\",\n",
    "    \"iTheta\",\n",
    "    \"energy_ecal\",\n",
    "    \"energy_hcal\",\n",
    "    \"energy_other\",\n",
    "    \"num_hits\",\n",
    "    \"sigma_x\",\n",
    "    \"sigma_y\",\n",
    "    \"sigma_z\",\n",
    "]\n",
    "Y_FEATURES = [\"cls_id\", \"charge\", \"pt\", \"eta\", \"sin_phi\", \"cos_phi\", \"energy\"]\n",
    "Y_CLASSES = [0, 211, 130, 22, 11, 13]\n",
    "\n",
    "INPUT_DIM = max(len(X_FEATURES_TRK), len(X_FEATURES_CL))\n",
    "NUM_CLASSES = len(Y_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8f1650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JP 2024-02-29: currently torch int8 onnx export does not work with MultiheadAttention because of the following:\n",
    "# - it uses q_scaling_product.mul_scalar which is not supported in ONNX opset 17: the fix is to just remove the q_scaling_product\n",
    "# - somehow, the \"need_weights\" option confuses the ONNX exporter because the multiheaded attention layer then returns a tuple: the fix is to make the MHA always return just the attended values only\n",
    "# I lifted these two modules directly from the pytorch code and made the modifications here.\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as nnF\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class QuantizeableMultiheadAttention(nn.MultiheadAttention):\n",
    "    _FLOAT_MODULE = nn.MultiheadAttention\n",
    "\n",
    "    r\"\"\"Quantizable implementation of the MultiheadAttention.\n",
    "\n",
    "    Note::\n",
    "        Please, refer to :class:`~torch.nn.MultiheadAttention` for more\n",
    "        information\n",
    "\n",
    "    Allows the model to jointly attend to information from different\n",
    "    representation subspaces.\n",
    "    See reference: Attention Is All You Need\n",
    "\n",
    "    The original MHA module is not quantizable.\n",
    "    This reimplements it by explicitly instantiating the linear layers.\n",
    "\n",
    "    .. math::\n",
    "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
    "        \\text{where} head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "\n",
    "    Args:\n",
    "        embed_dim: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n",
    "        bias: add bias as module parameter. Default: True.\n",
    "        add_bias_kv: add bias to the key and value sequences at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        kdim: total number of features in key. Default: None.\n",
    "        vdim: total number of features in value. Default: None.\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
    "\n",
    "    Note that if :attr:`kdim` and :attr:`vdim` are None, they will be set\n",
    "    to :attr:`embed_dim` such that query, key, and value have the same\n",
    "    number of features.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> import torch.ao.nn.quantizable as nnqa\n",
    "        >>> multihead_attn = nnqa.MultiheadAttention(embed_dim, num_heads)\n",
    "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "\n",
    "    Note::\n",
    "        Please, follow the quantization flow to convert the quantizable MHA.\n",
    "    \"\"\"\n",
    "    __constants__ = ['batch_first']\n",
    "\n",
    "    def __init__(self, embed_dim: int, num_heads: int,\n",
    "                 dropout: float = 0., bias: bool = True,\n",
    "                 add_bias_kv: bool = False, add_zero_attn: bool = False,\n",
    "                 kdim: Optional[int] = None, vdim: Optional[int] = None, batch_first: bool = False,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, num_heads, dropout,\n",
    "                         bias, add_bias_kv,\n",
    "                         add_zero_attn, kdim, vdim, batch_first,\n",
    "                         **factory_kwargs)\n",
    "        self.linear_Q = nn.Linear(self.embed_dim, self.embed_dim, bias=bias, **factory_kwargs)\n",
    "        self.linear_K = nn.Linear(self.kdim, self.embed_dim, bias=bias, **factory_kwargs)\n",
    "        self.linear_V = nn.Linear(self.vdim, self.embed_dim, bias=bias, **factory_kwargs)\n",
    "        # for the type: ignore, see https://github.com/pytorch/pytorch/issues/58969\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=bias, **factory_kwargs)  # type: ignore[assignment]\n",
    "\n",
    "        # Functionals\n",
    "        # self.q_scaling_product = torch.ao.nn.quantized.FloatFunctional()\n",
    "        # note: importing torch.ao.nn.quantized at top creates a circular import\n",
    "\n",
    "        # Quant/Dequant\n",
    "        self.quant_attn_output = torch.ao.quantization.QuantStub()\n",
    "        self.quant_attn_output_weights = torch.ao.quantization.QuantStub()\n",
    "        self.dequant_q = torch.ao.quantization.DeQuantStub()\n",
    "        self.dequant_k = torch.ao.quantization.DeQuantStub()\n",
    "        self.dequant_v = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def _get_name(self):\n",
    "        return 'QuantizableMultiheadAttention'\n",
    "\n",
    "    @classmethod\n",
    "    def from_float(cls, other):\n",
    "        assert type(other) == cls._FLOAT_MODULE\n",
    "        assert hasattr(other, 'qconfig'), \"The float module must have 'qconfig'\"\n",
    "        # Setting the dropout to 0.0!\n",
    "        observed = cls(other.embed_dim, other.num_heads, other.dropout,\n",
    "                       (other.in_proj_bias is not None),\n",
    "                       (other.bias_k is not None),\n",
    "                       other.add_zero_attn, other.kdim, other.vdim,\n",
    "                       other.batch_first)\n",
    "        observed.bias_k = other.bias_k\n",
    "        observed.bias_v = other.bias_v\n",
    "        observed.qconfig = other.qconfig\n",
    "\n",
    "        # Set the linear weights\n",
    "        # for the type: ignores, see https://github.com/pytorch/pytorch/issues/58969\n",
    "        observed.out_proj.weight = other.out_proj.weight  # type: ignore[has-type]\n",
    "        observed.out_proj.bias = other.out_proj.bias  # type: ignore[has-type]\n",
    "        if other._qkv_same_embed_dim:\n",
    "            # Use separate params\n",
    "            bias = other.in_proj_bias\n",
    "            _start = 0\n",
    "            _end = _start + other.embed_dim\n",
    "            weight = other.in_proj_weight[_start:_end, :]\n",
    "            if bias is not None:\n",
    "                bias = torch.nn.Parameter(bias[_start:_end], bias.requires_grad)\n",
    "            observed.linear_Q.weight = torch.nn.Parameter(weight,\n",
    "                                                          weight.requires_grad)\n",
    "            observed.linear_Q.bias = bias\n",
    "\n",
    "            bias = other.in_proj_bias\n",
    "            _start = _end\n",
    "            _end = _start + other.embed_dim\n",
    "            weight = other.in_proj_weight[_start:_end, :]\n",
    "            if bias is not None:\n",
    "                bias = torch.nn.Parameter(bias[_start:_end], bias.requires_grad)\n",
    "            observed.linear_K.weight = torch.nn.Parameter(weight,\n",
    "                                                          weight.requires_grad)\n",
    "            observed.linear_K.bias = bias\n",
    "\n",
    "            bias = other.in_proj_bias\n",
    "            _start = _end\n",
    "            weight = other.in_proj_weight[_start:, :]\n",
    "            if bias is not None:\n",
    "                bias = torch.nn.Parameter(bias[_start:], bias.requires_grad)\n",
    "            observed.linear_V.weight = torch.nn.Parameter(weight,\n",
    "                                                          weight.requires_grad)\n",
    "            observed.linear_V.bias = bias\n",
    "        else:\n",
    "            observed.linear_Q.weight = nn.Parameter(other.q_proj_weight)\n",
    "            observed.linear_K.weight = nn.Parameter(other.k_proj_weight)\n",
    "            observed.linear_V.weight = nn.Parameter(other.v_proj_weight)\n",
    "            if other.in_proj_bias is None:\n",
    "                observed.linear_Q.bias = None  # type: ignore[assignment]\n",
    "                observed.linear_K.bias = None  # type: ignore[assignment]\n",
    "                observed.linear_V.bias = None  # type: ignore[assignment]\n",
    "            else:\n",
    "                observed.linear_Q.bias = nn.Parameter(other.in_proj_bias[0:other.embed_dim])\n",
    "                observed.linear_K.bias = nn.Parameter(other.in_proj_bias[other.embed_dim:(other.embed_dim * 2)])\n",
    "                observed.linear_V.bias = nn.Parameter(other.in_proj_bias[(other.embed_dim * 2):])\n",
    "        observed.eval()\n",
    "        # Explicit prepare\n",
    "        observed = torch.ao.quantization.prepare(observed, inplace=True)\n",
    "        return observed\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def dequantize(self):\n",
    "        r\"\"\"Utility to convert the quantized MHA back to float.\n",
    "\n",
    "        The motivation for this is that it is not trivial to conver the weights\n",
    "        from the format that is used in the quantized version back to the\n",
    "        float.\n",
    "        \"\"\"\n",
    "        fp = self._FLOAT_MODULE(self.embed_dim, self.num_heads, self.dropout,\n",
    "                                (self.linear_Q._weight_bias()[1] is not None),\n",
    "                                (self.bias_k is not None),\n",
    "                                self.add_zero_attn, self.kdim, self.vdim, self.batch_first)\n",
    "        assert fp._qkv_same_embed_dim == self._qkv_same_embed_dim\n",
    "        if self.bias_k is not None:\n",
    "            fp.bias_k = nn.Parameter(self.bias_k.dequantize())\n",
    "        if self.bias_v is not None:\n",
    "            fp.bias_v = nn.Parameter(self.bias_v.dequantize())\n",
    "\n",
    "        # Set the linear weights\n",
    "        # Note: Because the linear layers are quantized, mypy does not nkow how\n",
    "        # to deal with them -- might need to ignore the typing checks.\n",
    "        # for the type: ignore[has-type], see https://github.com/pytorch/pytorch/issues/58969\n",
    "        w, b = self.out_proj._weight_bias()  # type: ignore[operator, has-type]\n",
    "        fp.out_proj.weight = nn.Parameter(w.dequantize())\n",
    "        if b is not None:\n",
    "            fp.out_proj.bias = nn.Parameter(b)\n",
    "\n",
    "        wQ, bQ = self.linear_Q._weight_bias()  # type: ignore[operator]\n",
    "        wQ = wQ.dequantize()\n",
    "        wK, bK = self.linear_K._weight_bias()  # type: ignore[operator]\n",
    "        wK = wK.dequantize()\n",
    "        wV, bV = self.linear_V._weight_bias()  # type: ignore[operator]\n",
    "        wV = wV.dequantize()\n",
    "        if fp._qkv_same_embed_dim:\n",
    "            # Use separate params\n",
    "            _start = 0\n",
    "            _end = _start + fp.embed_dim\n",
    "            fp.in_proj_weight[_start:_end, :] = wQ\n",
    "            if fp.in_proj_bias is not None:\n",
    "                assert all(bQ == 0)\n",
    "                fp.in_proj_bias[_start:_end] = bQ\n",
    "\n",
    "            _start = _end\n",
    "            _end = _start + fp.embed_dim\n",
    "            fp.in_proj_weight[_start:_end, :] = wK\n",
    "            if fp.in_proj_bias is not None:\n",
    "                assert all(bK == 0)\n",
    "                fp.in_proj_bias[_start:_end] = bK\n",
    "\n",
    "            _start = _end\n",
    "            fp.in_proj_weight[_start:, :] = wV\n",
    "            if fp.in_proj_bias is not None:\n",
    "                assert all(bV == 0)\n",
    "                fp.in_proj_bias[_start:] = bV\n",
    "        else:\n",
    "            fp.q_proj_weight = nn.Parameter(wQ)\n",
    "            fp.k_proj_weight = nn.Parameter(wK)\n",
    "            fp.v_proj_weight = nn.Parameter(wV)\n",
    "            if fp.in_proj_bias is None:\n",
    "                self.linear_Q.bias = None\n",
    "                self.linear_K.bias = None\n",
    "                self.linear_V.bias = None\n",
    "            else:\n",
    "                fp.in_proj_bias[0:fp.embed_dim] = bQ\n",
    "                fp.in_proj_bias[fp.embed_dim:(fp.embed_dim * 2)] = bK\n",
    "                fp.in_proj_bias[(fp.embed_dim * 2):] = bV\n",
    "\n",
    "        return fp\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_observed(cls, other):\n",
    "        # The whole flow is float -> observed -> quantized\n",
    "        # This class does float -> observed only\n",
    "        # See nn.quantized.MultiheadAttention\n",
    "        raise NotImplementedError(\"It looks like you are trying to prepare an \"\n",
    "                                  \"MHA module. Please, see \"\n",
    "                                  \"the examples on quantizable MHAs.\")\n",
    "\n",
    "    def forward(self,\n",
    "                query: Tensor,\n",
    "                key: Tensor,\n",
    "                value: Tensor,\n",
    "                key_padding_mask: Optional[Tensor] = None,\n",
    "                need_weights: bool = True,\n",
    "                attn_mask: Optional[Tensor] = None,\n",
    "                average_attn_weights: bool = True,\n",
    "                is_causal: bool = False) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        r\"\"\"\n",
    "    Note::\n",
    "        Please, refer to :func:`~torch.nn.MultiheadAttention.forward` for more\n",
    "        information\n",
    "\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. When given a binary mask and a value is True,\n",
    "            the corresponding value on the attention layer will be ignored.\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
    "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
    "\n",
    "    Shape:\n",
    "        - Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n",
    "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
    "          If a BoolTensor is provided, the positions with the\n",
    "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked\n",
    "          positions. If a BoolTensor is provided, positions with ``True``\n",
    "          is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "          is provided, it will be added to the attention weight.\n",
    "        - is_causal: If specified, applies a causal mask as attention mask. Mutually exclusive with providing attn_mask.\n",
    "          Default: ``False``.\n",
    "        - average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across\n",
    "          heads. Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an\n",
    "          effect when ``need_weights=True.``. Default: True (i.e. average weights across heads)\n",
    "\n",
    "        - Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n",
    "        - attn_output_weights: If ``average_attn_weights=True``, returns attention weights averaged\n",
    "          across heads of shape :math:`(N, L, S)`, where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. If ``average_attn_weights=False``, returns attention weights per\n",
    "          head of shape :math:`(N, num_heads, L, S)`.\n",
    "        \"\"\"\n",
    "        return self._forward_impl(query, key, value, key_padding_mask,\n",
    "                                  need_weights, attn_mask, average_attn_weights,\n",
    "                                  is_causal)\n",
    "\n",
    "    def _forward_impl(self,\n",
    "                      query: Tensor,\n",
    "                      key: Tensor,\n",
    "                      value: Tensor,\n",
    "                      key_padding_mask: Optional[Tensor] = None,\n",
    "                      need_weights: bool = True,\n",
    "                      attn_mask: Optional[Tensor] = None,\n",
    "                      average_attn_weights: bool = True,\n",
    "                      is_causal: bool = False) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        # This version will not deal with the static key/value pairs.\n",
    "        # Keeping it here for future changes.\n",
    "        #\n",
    "        # TODO: This method has some duplicate lines with the\n",
    "        # `torch.nn.functional.multi_head_attention`. Will need to refactor.\n",
    "        static_k = None\n",
    "        static_v = None\n",
    "\n",
    "        if attn_mask is not None and is_causal:\n",
    "            raise AssertionError(\"Only allow causal mask or attn_mask\")\n",
    "\n",
    "        if is_causal:\n",
    "            raise AssertionError(\"causal mask not supported by AO MHA module\")\n",
    "\n",
    "        if self.batch_first:\n",
    "            query, key, value = (x.transpose(0, 1) for x in (query, key, value))\n",
    "\n",
    "        tgt_len, bsz, embed_dim_to_check = query.size()\n",
    "        assert self.embed_dim == embed_dim_to_check\n",
    "        # allow MHA to have different sizes for the feature dimension\n",
    "        assert key.size(0) == value.size(0) and key.size(1) == value.size(1)\n",
    "\n",
    "        head_dim = self.embed_dim // self.num_heads\n",
    "        assert head_dim * self.num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        scaling = float(head_dim) ** -0.5\n",
    "\n",
    "        q = self.linear_Q(query)\n",
    "        k = self.linear_K(key)\n",
    "        v = self.linear_V(value)\n",
    "\n",
    "        #JP fix here: disabled this\n",
    "        # q = self.q_scaling_product.mul_scalar(q, scaling)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.uint8:\n",
    "                warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "                attn_mask = attn_mask.to(torch.bool)\n",
    "            assert attn_mask.is_floating_point() or attn_mask.dtype == torch.bool, \\\n",
    "                f'Only float and bool types are supported for attn_mask, not {attn_mask.dtype}'\n",
    "\n",
    "            if attn_mask.dim() == 2:\n",
    "                attn_mask = attn_mask.unsqueeze(0)\n",
    "                if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:\n",
    "                    raise RuntimeError('The size of the 2D attn_mask is not correct.')\n",
    "            elif attn_mask.dim() == 3:\n",
    "                if list(attn_mask.size()) != [bsz * self.num_heads, query.size(0), key.size(0)]:\n",
    "                    raise RuntimeError('The size of the 3D attn_mask is not correct.')\n",
    "            else:\n",
    "                raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "            # attn_mask's dim is 3 now.\n",
    "\n",
    "        # convert ByteTensor key_padding_mask to bool\n",
    "        if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n",
    "            warnings.warn(\"Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "            key_padding_mask = key_padding_mask.to(torch.bool)\n",
    "        if self.bias_k is not None and self.bias_v is not None:\n",
    "            if static_k is None and static_v is None:\n",
    "\n",
    "                # Explicitly assert that bias_k and bias_v are not None\n",
    "                # in a way that TorchScript can understand.\n",
    "                bias_k = self.bias_k\n",
    "                assert bias_k is not None\n",
    "                bias_v = self.bias_v\n",
    "                assert bias_v is not None\n",
    "\n",
    "                k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
    "                v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
    "                if attn_mask is not None:\n",
    "                    attn_mask = nnF.pad(attn_mask, (0, 1))\n",
    "                if key_padding_mask is not None:\n",
    "                    key_padding_mask = nnF.pad(key_padding_mask, (0, 1))\n",
    "            else:\n",
    "                assert static_k is None, \"bias cannot be added to static key.\"\n",
    "                assert static_v is None, \"bias cannot be added to static value.\"\n",
    "        else:\n",
    "            assert self.bias_k is None\n",
    "            assert self.bias_v is None\n",
    "\n",
    "        q = q.contiguous().view(tgt_len, bsz * self.num_heads, head_dim).transpose(0, 1)\n",
    "        if k is not None:\n",
    "            k = k.contiguous().view(-1, bsz * self.num_heads, head_dim).transpose(0, 1)\n",
    "        if v is not None:\n",
    "            v = v.contiguous().view(-1, bsz * self.num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "        if static_k is not None:\n",
    "            assert static_k.size(0) == bsz * self.num_heads\n",
    "            assert static_k.size(2) == head_dim\n",
    "            k = static_k\n",
    "\n",
    "        if static_v is not None:\n",
    "            assert static_v.size(0) == bsz * self.num_heads\n",
    "            assert static_v.size(2) == head_dim\n",
    "            v = static_v\n",
    "\n",
    "        src_len = k.size(1)\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            assert key_padding_mask.size(0) == bsz\n",
    "            assert key_padding_mask.size(1) == src_len\n",
    "\n",
    "        if self.add_zero_attn:\n",
    "            src_len += 1\n",
    "            k_zeros = torch.zeros((k.size(0), 1) + k.size()[2:])\n",
    "            if k.is_quantized:\n",
    "                k_zeros = torch.quantize_per_tensor(k_zeros, k.q_scale(), k.q_zero_point(), k.dtype)\n",
    "            k = torch.cat([k, k_zeros], dim=1)\n",
    "            v_zeros = torch.zeros((v.size(0), 1) + k.size()[2:])\n",
    "            if v.is_quantized:\n",
    "                v_zeros = torch.quantize_per_tensor(v_zeros, v.q_scale(), v.q_zero_point(), v.dtype)\n",
    "            v = torch.cat([v, v_zeros], dim=1)\n",
    "\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = nnF.pad(attn_mask, (0, 1))\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = nnF.pad(key_padding_mask, (0, 1))\n",
    "\n",
    "        # Leaving the quantized zone here\n",
    "        q = self.dequant_q(q)\n",
    "        k = self.dequant_k(k)\n",
    "        v = self.dequant_v(v)\n",
    "        attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "        assert list(attn_output_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                attn_output_weights.masked_fill_(attn_mask, float('-inf'))\n",
    "            else:\n",
    "                attn_output_weights += attn_mask\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            attn_output_weights = attn_output_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_output_weights = attn_output_weights.masked_fill(\n",
    "                key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "                float('-inf'),\n",
    "            )\n",
    "            attn_output_weights = attn_output_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        attn_output_weights = nnF.softmax(\n",
    "            attn_output_weights, dim=-1)\n",
    "        attn_output_weights = nnF.dropout(attn_output_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.bmm(attn_output_weights, v)\n",
    "        assert list(attn_output.size()) == [bsz * self.num_heads, tgt_len, head_dim]\n",
    "        if self.batch_first:\n",
    "            attn_output = attn_output.view(bsz, tgt_len, self.embed_dim)\n",
    "        else:\n",
    "            attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, self.embed_dim)\n",
    "\n",
    "        # Reentering the quantized zone\n",
    "        attn_output = self.quant_attn_output(attn_output)\n",
    "        # for the type: ignore[has-type], see https://github.com/pytorch/pytorch/issues/58969\n",
    "        attn_output = self.out_proj(attn_output)  # type: ignore[has-type]\n",
    "\n",
    "        #JP fix: removed need_weights part from here, return attn_output instead of tuple\n",
    "        return attn_output\n",
    "\n",
    "class QuantizedMultiheadAttention(QuantizeableMultiheadAttention):\n",
    "    _FLOAT_MODULE = torch.ao.nn.quantizable.MultiheadAttention\n",
    "\n",
    "    def _get_name(self):\n",
    "        return \"QuantizedMultiheadAttention\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_float(cls, other):\n",
    "        # The whole flow is float -> observed -> quantized\n",
    "        # This class does observed -> quantized only\n",
    "        raise NotImplementedError(\"It looks like you are trying to convert a \"\n",
    "                                  \"non-observed MHA module. Please, see \"\n",
    "                                  \"the examples on quantizable MHAs.\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_observed(cls, other):\n",
    "        converted = torch.ao.quantization.convert(other, mapping=None,\n",
    "                                                  inplace=False,\n",
    "                                                  remove_qconfig=True,\n",
    "                                                  convert_custom_config_dict=None)\n",
    "        converted.__class__ = cls\n",
    "        # Remove the parameters for the bias_k and bias_v to quantize them\n",
    "        # TODO: This is a potential source of accuracy drop.\n",
    "        #       quantized cat takes the scale and zp of the first\n",
    "        #       element, which might lose the precision in the bias_k\n",
    "        #       and the bias_v (which are cat'ed with k/v being first).\n",
    "        if converted.bias_k is not None:\n",
    "            bias_k = converted._parameters.pop('bias_k')\n",
    "            sc, zp = torch._choose_qparams_per_tensor(bias_k,\n",
    "                                                      reduce_range=False)\n",
    "            bias_k = torch.quantize_per_tensor(bias_k, sc, zp, torch.quint8)\n",
    "            setattr(converted, 'bias_k', bias_k)  # noqa: B010\n",
    "\n",
    "        if converted.bias_v is not None:\n",
    "            bias_v = converted._parameters.pop('bias_v')\n",
    "            sc, zp = torch._choose_qparams_per_tensor(bias_k,  # type: ignore[possibly-undefined]\n",
    "                                                      reduce_range=False)\n",
    "            bias_v = torch.quantize_per_tensor(bias_v, sc, zp, torch.quint8)\n",
    "            setattr(converted, 'bias_v', bias_v)  # noqa: B010\n",
    "\n",
    "        del converted.in_proj_weight\n",
    "        del converted.in_proj_bias\n",
    "\n",
    "        return converted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52bf9733",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss, as described in https://arxiv.org/abs/1708.02002.\n",
    "    It is essentially an enhancement to cross entropy loss and is\n",
    "    useful for classification tasks when there is a large class imbalance.\n",
    "    x is expected to contain raw, unnormalized scores for each class.\n",
    "    y is expected to contain class labels.\n",
    "    Shape:\n",
    "        - x: (batch_size, C) or (batch_size, C, d1, d2, ..., dK), K > 0.\n",
    "        - y: (batch_size,) or (batch_size, d1, d2, ..., dK), K > 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, alpha = None, gamma = 0.0, reduction = \"mean\", ignore_index = -100\n",
    "    ):\n",
    "        \"\"\"Constructor.\n",
    "        Args:\n",
    "            alpha (Tensor, optional): Weights for each class. Defaults to None.\n",
    "            gamma (float, optional): A constant, as described in the paper.\n",
    "                Defaults to 0.\n",
    "            reduction (str, optional): 'mean', 'sum' or 'none'.\n",
    "                Defaults to 'mean'.\n",
    "            ignore_index (int, optional): class label to ignore.\n",
    "                Defaults to -100.\n",
    "        \"\"\"\n",
    "        if reduction not in (\"mean\", \"sum\", \"none\"):\n",
    "            raise ValueError('Reduction must be one of: \"mean\", \"sum\", \"none\".')\n",
    "\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "        self.nll_loss = nn.NLLLoss(weight=alpha, reduction=\"none\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        arg_keys = [\"alpha\", \"gamma\", \"reduction\"]\n",
    "        arg_vals = [self.__dict__[k] for k in arg_keys]\n",
    "        arg_strs = [f\"{k}={v!r}\" for k, v in zip(arg_keys, arg_vals)]\n",
    "        arg_str = \", \".join(arg_strs)\n",
    "        return f\"{type(self).__name__}({arg_str})\"\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        if x.ndim > 2:\n",
    "            # (N, C, d1, d2, ..., dK) --> (N * d1 * ... * dK, C)\n",
    "            c = x.shape[1]\n",
    "            x = x.permute(0, *range(2, x.ndim), 1).reshape(-1, c)\n",
    "            # (N, d1, d2, ..., dK) --> (N * d1 * ... * dK,)\n",
    "            y = y.view(-1)\n",
    "\n",
    "        # compute weighted cross entropy term: -alpha * log(pt)\n",
    "        # (alpha is already part of self.nll_loss)\n",
    "        log_p = F.log_softmax(x, dim=-1)\n",
    "        ce = self.nll_loss(log_p, y)\n",
    "\n",
    "        # get true class column from each row\n",
    "        # this is slow due to indexing\n",
    "        # all_rows = torch.arange(len(x))\n",
    "        # log_pt = log_p[all_rows, y]\n",
    "        log_pt = torch.gather(log_p, 1, y.unsqueeze(axis=-1)).squeeze(axis=-1)\n",
    "\n",
    "        # compute focal term: (1 - pt)^gamma\n",
    "        pt = log_pt.exp()\n",
    "        focal_term = (1 - pt) ** self.gamma\n",
    "\n",
    "        # the full loss: -alpha * ((1 - pt)^gamma) * log(pt)\n",
    "        loss = focal_term * ce\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            loss = loss.sum()\n",
    "\n",
    "        return loss\n",
    "        \n",
    "def mlpf_loss(y, ypred, mask):\n",
    "    loss = {}\n",
    "    loss_obj_id = FocalLoss(gamma=2.0, reduction=\"none\")\n",
    "\n",
    "    msk_true_particle = torch.unsqueeze((y[\"cls_id\"] != 0).to(dtype=torch.float32), axis=-1)\n",
    "    nelem = torch.sum(mask)\n",
    "    npart = torch.sum(y[\"cls_id\"] != 0)\n",
    "    \n",
    "    ypred[\"momentum\"] = ypred[\"momentum\"] * msk_true_particle\n",
    "    y[\"momentum\"] = y[\"momentum\"] * msk_true_particle\n",
    "\n",
    "    ypred[\"cls_id_onehot\"] = ypred[\"cls_id_onehot\"].permute((0, 2, 1))\n",
    "\n",
    "    loss_classification = loss_obj_id(ypred[\"cls_id_onehot\"], y[\"cls_id\"]).reshape(y[\"cls_id\"].shape)\n",
    "    loss_regression = torch.nn.functional.huber_loss(ypred[\"momentum\"], y[\"momentum\"], reduction=\"none\")\n",
    "    \n",
    "    # average over all elements that were not padded\n",
    "    loss[\"Classification\"] = loss_classification.sum() / npart\n",
    "    \n",
    "    mom_normalizer = y[\"momentum\"][y[\"cls_id\"] != 0].std(axis=0)\n",
    "    reg_losses = loss_regression[y[\"cls_id\"] != 0]\n",
    "    # average over all true particles\n",
    "    loss[\"Regression\"] = (reg_losses / mom_normalizer).sum() / npart\n",
    "\n",
    "    px = ypred[\"momentum\"][..., 0:1] * ypred[\"momentum\"][..., 3:4] * msk_true_particle\n",
    "    py = ypred[\"momentum\"][..., 0:1] * ypred[\"momentum\"][..., 2:3] * msk_true_particle\n",
    "    pred_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)\n",
    "\n",
    "    px = y[\"momentum\"][..., 0:1] * y[\"momentum\"][..., 3:4] * msk_true_particle\n",
    "    py = y[\"momentum\"][..., 0:1] * y[\"momentum\"][..., 2:3] * msk_true_particle\n",
    "    true_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)\n",
    "    loss[\"MET\"] = torch.nn.functional.huber_loss(pred_met, true_met).mean()\n",
    "\n",
    "    loss[\"Total\"] = loss[\"Classification\"] + loss[\"Regression\"]\n",
    "    # loss[\"Total\"] += 0.1*loss[\"MET\"]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbf53040",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizeFeaturesStub(torch.ao.quantization.QuantStub):\n",
    "    def __init__(self, num_feats):\n",
    "        super().__init__()\n",
    "        self.num_feats = num_feats\n",
    "        self.quants = torch.nn.ModuleList()\n",
    "        for ifeat in range(self.num_feats):\n",
    "            self.quants.append(torch.ao.quantization.QuantStub())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.quants[ifeat](x[..., ifeat:ifeat+1]) for ifeat in range(self.num_feats)], axis=-1)\n",
    "        \n",
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim=128,\n",
    "        num_heads=2,\n",
    "        width=128,\n",
    "        dropout_mha=0.1,\n",
    "        dropout_ff=0.1,\n",
    "        attention_type=\"efficient\",\n",
    "    ):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "\n",
    "        self.attention_type = attention_type\n",
    "        self.act = nn.ReLU\n",
    "        self.mha = torch.nn.MultiheadAttention(embedding_dim, num_heads, dropout=dropout_mha, batch_first=True)\n",
    "        self.norm0 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.norm1 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.seq = torch.nn.Sequential(\n",
    "            nn.Linear(embedding_dim, width), self.act(), nn.Linear(width, embedding_dim), self.act()\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(dropout_ff)\n",
    "\n",
    "        self.add0 = torch.ao.nn.quantized.FloatFunctional()\n",
    "        self.add1 = torch.ao.nn.quantized.FloatFunctional()\n",
    "        self.mul = torch.ao.nn.quantized.FloatFunctional()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        mha_out = self.mha(x, x, x, need_weights=False)[0]\n",
    "        x = self.add0.add(x, mha_out)\n",
    "        x = self.norm0(x)\n",
    "        x = self.add1.add(x, self.seq(x))\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(x)\n",
    "        # x = self.mul.mul(x, mask.unsqueeze(-1))\n",
    "        return x\n",
    "\n",
    "class RegressionOutput(nn.Module):\n",
    "    def __init__(self, embed_dim, width, act, dropout):\n",
    "        super(RegressionOutput, self).__init__()\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "        self.nn = ffn(embed_dim, 1, width, act, dropout)\n",
    "\n",
    "    def forward(self, elems, x, orig_value):\n",
    "        nn_out = self.nn(x)\n",
    "        nn_out = self.dequant(nn_out)\n",
    "        return orig_value + nn_out\n",
    "\n",
    "def ffn(input_dim, output_dim, width, act, dropout):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, width),\n",
    "        act(),\n",
    "        torch.nn.LayerNorm(width),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(width, output_dim),\n",
    "    )\n",
    "\n",
    "def transform_batch(Xbatch):\n",
    "    Xbatch = Xbatch.clone()\n",
    "    Xbatch[..., 1] = torch.log(Xbatch[..., 1])\n",
    "    Xbatch[..., 5] = torch.log(Xbatch[..., 5])\n",
    "    Xbatch[torch.isnan(Xbatch)] = 0.0\n",
    "    Xbatch[torch.isinf(Xbatch)] = 0.0\n",
    "    return Xbatch\n",
    "    \n",
    "def unpack_target(y):\n",
    "    ret = {}\n",
    "    ret[\"cls_id\"] = y[..., 0].long()\n",
    "\n",
    "    for i, feat in enumerate(Y_FEATURES):\n",
    "        if i >= 2:  # skip the cls and charge as they are defined above\n",
    "            ret[feat] = y[..., i].to(dtype=torch.float32)\n",
    "    ret[\"phi\"] = torch.atan2(ret[\"sin_phi\"], ret[\"cos_phi\"])\n",
    "    \n",
    "    # note ~ momentum = [\"pt\", \"eta\", \"sin_phi\", \"cos_phi\", \"energy\"]\n",
    "    ret[\"momentum\"] = y[..., 2:7].to(dtype=torch.float32)\n",
    "    ret[\"p4\"] = torch.cat(\n",
    "        [ret[\"pt\"].unsqueeze(1), ret[\"eta\"].unsqueeze(1), ret[\"phi\"].unsqueeze(1), ret[\"energy\"].unsqueeze(1)], axis=1\n",
    "    )\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def unpack_predictions(preds):\n",
    "    ret = {}\n",
    "    ret[\"cls_id_onehot\"], ret[\"momentum\"] = preds\n",
    "\n",
    "    ret[\"pt\"] = ret[\"momentum\"][..., 0]\n",
    "    ret[\"eta\"] = ret[\"momentum\"][..., 1]\n",
    "    ret[\"sin_phi\"] = ret[\"momentum\"][..., 2]\n",
    "    ret[\"cos_phi\"] = ret[\"momentum\"][..., 3]\n",
    "    ret[\"energy\"] = ret[\"momentum\"][..., 4]\n",
    "\n",
    "    ret[\"cls_id\"] = torch.argmax(ret[\"cls_id_onehot\"], axis=-1)\n",
    "    ret[\"phi\"] = torch.atan2(ret[\"sin_phi\"], ret[\"cos_phi\"])\n",
    "    ret[\"p4\"] = torch.cat(\n",
    "        [\n",
    "            ret[\"pt\"].unsqueeze(axis=-1),\n",
    "            ret[\"eta\"].unsqueeze(axis=-1),\n",
    "            ret[\"phi\"].unsqueeze(axis=-1),\n",
    "            ret[\"energy\"].unsqueeze(axis=-1),\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "    return ret\n",
    "\n",
    "class MLPF(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=16,\n",
    "        num_classes=6,\n",
    "        num_convs=2,\n",
    "        dropout_ff=0.0,\n",
    "        dropout_conv_reg_mha=0.0,\n",
    "        dropout_conv_reg_ff=0.0,\n",
    "        dropout_conv_id_mha=0.0,\n",
    "        dropout_conv_id_ff=0.0,\n",
    "        num_heads=16,\n",
    "        head_dim=16,\n",
    "        elemtypes=[0,1,2],\n",
    "    ):\n",
    "        super(MLPF, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.act = nn.ReLU\n",
    "        self.elemtypes = elemtypes\n",
    "        self.num_elemtypes = len(self.elemtypes)\n",
    "\n",
    "        embedding_dim = num_heads * head_dim\n",
    "        width = num_heads * head_dim\n",
    "        \n",
    "        self.nn0_id = ffn(self.input_dim, embedding_dim, width, self.act, dropout_ff)\n",
    "        self.nn0_reg = ffn(self.input_dim, embedding_dim, width, self.act, dropout_ff)\n",
    "        \n",
    "        self.conv_id = nn.ModuleList()\n",
    "        self.conv_reg = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_convs):\n",
    "            self.conv_id.append(\n",
    "                SelfAttentionLayer(\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    width=width,\n",
    "                    dropout_mha=dropout_conv_id_mha,\n",
    "                    dropout_ff=dropout_conv_id_ff,\n",
    "                )\n",
    "            )\n",
    "            self.conv_reg.append(\n",
    "                SelfAttentionLayer(\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    width=width,\n",
    "                    dropout_mha=dropout_conv_reg_mha,\n",
    "                    dropout_ff=dropout_conv_reg_ff,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        decoding_dim = self.input_dim + embedding_dim\n",
    "\n",
    "        # DNN that acts on the node level to predict the PID\n",
    "        self.nn_id = ffn(decoding_dim, num_classes, width, self.act, dropout_ff)\n",
    "\n",
    "        # elementwise DNN for node momentum regression\n",
    "        embed_dim = decoding_dim + num_classes\n",
    "        self.nn_pt = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_eta = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_sin_phi = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_cos_phi = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_energy = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        \n",
    "        self.quant = QuantizeFeaturesStub(self.input_dim + len(self.elemtypes))\n",
    "        self.dequant_id = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, X_features, mask):\n",
    "        Xfeat_transformed = transform_batch(X_features)\n",
    "        Xfeat_normed = self.quant(Xfeat_transformed)\n",
    "\n",
    "        embeddings_id, embeddings_reg = [], []\n",
    "        embedding_id = self.nn0_id(Xfeat_normed)\n",
    "        embedding_reg = self.nn0_reg(Xfeat_normed)\n",
    "        for num, conv in enumerate(self.conv_id):\n",
    "            conv_input = embedding_id if num == 0 else embeddings_id[-1]\n",
    "            out_padded = conv(conv_input, mask)\n",
    "            embeddings_id.append(out_padded)\n",
    "        for num, conv in enumerate(self.conv_reg):\n",
    "            conv_input = embedding_reg if num == 0 else embeddings_reg[-1]\n",
    "            out_padded = conv(conv_input, mask)\n",
    "            embeddings_reg.append(out_padded)\n",
    "\n",
    "        final_embedding_id = torch.cat([Xfeat_normed] + [embeddings_id[-1]], axis=-1)\n",
    "        preds_id = self.nn_id(final_embedding_id)\n",
    "\n",
    "        final_embedding_reg = torch.cat([Xfeat_normed] + [embeddings_reg[-1]] + [preds_id], axis=-1)\n",
    "        preds_pt = self.nn_pt(X_features, final_embedding_reg, X_features[..., 1:2])\n",
    "        preds_eta = self.nn_eta(X_features, final_embedding_reg, X_features[..., 2:3])\n",
    "        preds_sin_phi = self.nn_sin_phi(X_features, final_embedding_reg, X_features[..., 3:4])\n",
    "        preds_cos_phi = self.nn_cos_phi(X_features, final_embedding_reg, X_features[..., 4:5])\n",
    "        preds_energy = self.nn_energy(X_features, final_embedding_reg, X_features[..., 5:6])\n",
    "        preds_momentum = torch.cat([preds_pt, preds_eta, preds_sin_phi, preds_cos_phi, preds_energy], axis=-1)\n",
    "        \n",
    "        preds_id = self.dequant_id(preds_id)\n",
    "        return preds_id, preds_momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20fdd196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss=5.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss=2.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, loss=2.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, loss=2.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, loss=2.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, loss=1.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, loss=1.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, loss=1.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss=1.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, loss=1.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_events_train = 1000\n",
    "max_events_eval = 1000\n",
    "events_per_batch = 50\n",
    "nepochs = 10\n",
    "\n",
    "model = MLPF(input_dim=INPUT_DIM, num_classes=NUM_CLASSES).to(device=device)\n",
    "optimizer = torch.optim.Adam(lr=1e-4, params=model.parameters())\n",
    "\n",
    "# Training loop\n",
    "loss_vals_epochs = []\n",
    "for epoch in range(nepochs):\n",
    "    loss_vals_steps = []\n",
    "    \n",
    "    # Generate indices for 10 batches\n",
    "    inds_train = range(0, max_events_train, events_per_batch * 10)\n",
    "    \n",
    "    for ind in tqdm.tqdm(inds_train):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Load the data for 10 batches\n",
    "        ds_elems = []\n",
    "        for i in range(10):\n",
    "            batch_inds = range(ind + i * events_per_batch, ind + (i + 1) * events_per_batch)\n",
    "            ds_elems.extend([ds_train[i] for i in batch_inds])\n",
    "            \n",
    "        X_features = [torch.tensor(elem[\"X\"]).to(torch.float32) for elem in ds_elems]\n",
    "        y_targets = [torch.tensor(elem[\"ygen\"]).to(torch.float32) for elem in ds_elems]\n",
    "\n",
    "        # Batch the data into [batch_size, num_elems, num_features]\n",
    "        X_features_padded = pad_sequence(X_features, batch_first=True).to(device=device)\n",
    "        y_targets_padded = pad_sequence(y_targets, batch_first=True).to(device=device)\n",
    "        mask = X_features_padded[:, :, 0] != 0\n",
    "\n",
    "        # Run the model\n",
    "        preds = model(X_features_padded, mask)\n",
    "        preds_unpacked = unpack_predictions(preds)\n",
    "        targets_unpacked = unpack_target(y_targets_padded)\n",
    "\n",
    "        # Compute loss, update model weights\n",
    "        loss = mlpf_loss(targets_unpacked, preds_unpacked, mask)\n",
    "        loss[\"Total\"].backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_vals_steps.append(loss[\"Total\"].detach().cpu().item())\n",
    "\n",
    "    loss_vals_epochs.append(np.mean(loss_vals_steps))\n",
    "    print(\"Epoch {}, loss={:.2f}\".format(epoch, loss_vals_epochs[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a60717be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Clear CUDA cache\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdfa6a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "## print(ind+events_per_batch)\n",
    "print(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2d1fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the model back on CPU\n",
    "model = model.to(device=\"cpu\")\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "ds_elems = [ds_train[i] for i in range(max_events_train, max_events_train + max_events_eval)]\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32  \n",
    "for i in range(0, len(ds_elems), batch_size):\n",
    "    batch_elems = ds_elems[i:i + batch_size]\n",
    "    # input features\n",
    "    X_features = [torch.tensor(elem[\"X\"]).to(torch.float32) for elem in batch_elems]\n",
    "    X_features_padded = pad_sequence(X_features, batch_first=True)\n",
    "    #  target labels\n",
    "    y_targets = [torch.tensor(elem[\"ygen\"]).to(torch.float32) for elem in batch_elems]\n",
    "    y_targets_padded = pad_sequence(y_targets, batch_first=True)\n",
    "    #  mask for the batch\n",
    "    mask = X_features_padded[:, :, 0] != 0\n",
    "    #  model prediction, loss computation\n",
    "    preds = model(X_features_padded, mask)\n",
    "    preds = preds[0].detach(), preds[1].detach()\n",
    "    # Update mask for the batch\n",
    "    mask = X_features_padded[:, :, 0] != 0\n",
    "    # Unpack predictions and targets for the batch\n",
    "    preds_unpacked = unpack_predictions(preds)\n",
    "    targets_unpacked = unpack_target(y_targets_padded)\n",
    "    # append to a list \n",
    "    all_preds.append(preds_unpacked)\n",
    "    all_targets.append(targets_unpacked)\n",
    "    # Compute loss for the batch\n",
    "    loss = mlpf_loss(targets_unpacked, preds_unpacked, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2419cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "msk_true_particles = targets_unpacked[\"cls_id\"]!=0\n",
    "msk_pred_particles = preds_unpacked[\"cls_id\"]!=0\n",
    "\n",
    "\n",
    "pt_target = targets_unpacked[\"pt\"][msk_true_particles].numpy()\n",
    "pt_pred = preds_unpacked[\"pt\"][msk_true_particles].numpy()\n",
    "\n",
    "eta_target = targets_unpacked[\"eta\"][msk_true_particles].numpy()\n",
    "eta_pred = preds_unpacked[\"eta\"][msk_true_particles].numpy()\n",
    "\n",
    "sphi_target = targets_unpacked[\"sin_phi\"][msk_true_particles].numpy()\n",
    "sphi_pred = preds_unpacked[\"sin_phi\"][msk_true_particles].numpy()\n",
    "\n",
    "cphi_target = targets_unpacked[\"cos_phi\"][msk_true_particles].numpy()\n",
    "cphi_pred = preds_unpacked[\"cos_phi\"][msk_true_particles].numpy()\n",
    "\n",
    "energy_target = targets_unpacked[\"energy\"][msk_true_particles].numpy()\n",
    "energy_pred = preds_unpacked[\"energy\"][msk_true_particles].numpy()\n",
    "\n",
    "px = preds_unpacked[\"pt\"] * preds_unpacked[\"cos_phi\"] * msk_true_particles\n",
    "py = preds_unpacked[\"pt\"] * preds_unpacked[\"sin_phi\"] * msk_true_particles\n",
    "pred_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)\n",
    "\n",
    "px = targets_unpacked[\"pt\"] * targets_unpacked[\"cos_phi\"] * msk_true_particles\n",
    "py = targets_unpacked[\"pt\"] * targets_unpacked[\"sin_phi\"] * msk_true_particles\n",
    "true_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "802a663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "px = targets_unpacked[\"pt\"] * targets_unpacked[\"cos_phi\"] * msk_true_particles\n",
    "py = targets_unpacked[\"pt\"] * targets_unpacked[\"sin_phi\"] * msk_true_particles\n",
    "pz = targets_unpacked[\"pt\"] * np.sinh(targets_unpacked[\"eta\"]) * msk_true_particles\n",
    "# phi = np.arctan2(targets_unpacked[\"sin_phi\"], targets_unpacked[\"cos_phi\"]) * msk_true_particles\n",
    "\n",
    "px_np = px.detach().cpu().numpy()\n",
    "py_np = py.detach().cpu().numpy()\n",
    "pz_np = pz.detach().cpu().numpy()\n",
    "# phi_np = phi.detach().cpu().numpy()\n",
    "\n",
    "# print(\"px_np\", px_np)\n",
    "# print(\"py_np\", py_np)\n",
    "# print(\"pz_np\", pz_np)\n",
    "# print(\"phi_np\", phi_np)\n",
    "\n",
    "true_mom = np.sqrt(np.sum(px_np, axis=1)**2 + np.sum(py_np, axis=1)**2 + np.sum(pz_np, axis=1)**2)\n",
    "\n",
    "E = np.sqrt(px_np**2 + py_np**2 + pz_np**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bc3a2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E Shape (8, 140)\n",
      "px Shape torch.Size([8, 140])\n",
      "py Shape torch.Size([8, 140])\n",
      "pz Shape torch.Size([8, 140])\n"
     ]
    }
   ],
   "source": [
    "# four-vectors\n",
    "# px_py_pz_E = np.column_stack((px_np, py_np, pz_np, E)) \n",
    "print(\"E Shape\", E.shape)\n",
    "print(\"px Shape\", px.shape)\n",
    "print(\"py Shape\", py.shape)\n",
    "print(\"pz Shape\", pz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6404372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastjet as fj\n",
    "import numpy as np\n",
    "import awkward\n",
    "\n",
    "# Four momentum \n",
    "px_np = targets_unpacked[\"pt\"] * targets_unpacked[\"cos_phi\"] * msk_true_particles\n",
    "py_np = targets_unpacked[\"pt\"] * targets_unpacked[\"sin_phi\"] * msk_true_particles\n",
    "pz_np = targets_unpacked[\"pt\"] * np.sinh(targets_unpacked[\"eta\"]) * msk_true_particles\n",
    "E_np = np.sqrt(px_np**2 + py_np**2 + pz_np**2)\n",
    "\n",
    "\n",
    "particles = []\n",
    "for ip in range(E.shape[0]):\n",
    "    for ix in range(E.shape[1]):\n",
    "        px_value = float(px[ip, ix])\n",
    "        py_value = float(py[ip, ix])\n",
    "        pz_value = float(pz[ip, ix])\n",
    "        E_value = float(E[ip, ix])\n",
    "        particle = fj.PseudoJet(px_value, py_value, pz_value, E_value)\n",
    "        particles.append(particle)\n",
    "\n",
    "# print(particles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43801c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--------------------------------------------------------------------------\n",
      "#                         FastJet release 3.4.1\n",
      "#                 M. Cacciari, G.P. Salam and G. Soyez                  \n",
      "#     A software package for jet finding and analysis at colliders      \n",
      "#                           http://fastjet.fr                           \n",
      "#\t                                                                      \n",
      "# Please cite EPJC72(2012)1896 [arXiv:1111.6097] if you use this package\n",
      "# for scientific work and optionally PLB641(2006)57 [hep-ph/0512210].   \n",
      "#                                                                       \n",
      "# FastJet is provided without warranty under the GNU GPL v2 or higher.  \n",
      "# It uses T. Chan's closest pair algorithm, S. Fortune's Voronoi code,\n",
      "# CGAL and 3rd party plugin jet algorithms. See COPYING file for details.\n",
      "#--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "num_threads = 2 \n",
    "\n",
    "def cluster_jets(particles):\n",
    "    jetdef = fj.JetDefinition(fj.antikt_algorithm, 0.4)\n",
    "    jet_ptcut = 20\n",
    "    \n",
    "    cluster = fj.ClusterSequence(particles, jetdef)\n",
    "    jets = cluster.inclusive_jets()\n",
    "    \n",
    "    return jets\n",
    "\n",
    "chunks = [particles[i::num_threads] for i in range(num_threads)]\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    futures = [executor.submit(cluster_jets, chunk) for chunk in chunks]\n",
    "\n",
    "gen_jets = []\n",
    "for future in futures:\n",
    "    gen_jets.extend(future.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e8e812f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jet 1 : 0.0 100000.0 0.0 0.0\n",
      "Jet 2 : 0.011041980130607957 2.692439330189458 1.0399473608569432 0.08190593868494034\n",
      "Jet 3 : 0.07000993972384678 -2.1822228049772345 5.5512280422800915 0.314301997423172\n",
      "Jet 4 : 0.07377876188429909 1.7342021062630282 4.105313850505195 0.21546894311904907\n",
      "Jet 5 : 0.08616098389878034 -0.4969775617853109 1.933582687454558 0.09702210873365402\n",
      "Jet 6 : 0.13705206097790096 -0.9638529369981373 0.8436832699525574 0.20579731464385986\n",
      "Jet 7 : 0.23505169687217836 -2.8119811891305586 1.4066414972168668 1.9630275964736938\n",
      "Jet 8 : 0.2896120340012331 0.4355251572111674 3.143479649168185 0.3175160884857178\n",
      "Jet 9 : 0.3007328075834574 1.4483230699820087 0.3141434981941669 0.675285816192627\n",
      "Jet 10 : 0.30249243944012394 2.060476321989617 1.8119019465971327 1.206506371498108\n",
      "Jet 11 : 0.3093734888358455 1.7058964547859963 5.216290774796743 0.8802661597728729\n",
      "Jet 12 : 0.41367409731209925 2.2353236253313953 4.333303625129647 1.9561688899993896\n",
      "Jet 13 : 0.5337753765830313 1.5358996607253925 1.8469224669874693 1.299874871969223\n",
      "Jet 14 : 0.4645943287369452 0.21086883605797693 3.6196137190851676 0.47496190667152405\n",
      "Jet 15 : 0.6839321574740315 -2.332349340545643 0.5536612308179103 3.5571341514587402\n",
      "Jet 16 : 0.9051225662112234 1.6470171750359979 2.508788576433663 2.4366328716278076\n",
      "Jet 17 : 0.9576567019339343 -1.5385791123941837 0.9694055016528826 2.334261327981949\n",
      "Jet 18 : 1.0867206937647351 0.9365251545912655 5.688290573260018 1.603403925895691\n",
      "Jet 19 : 1.6326257797608748 -1.362703934897254 2.714073786980889 3.411008656024933\n",
      "Jet 20 : 1.4187239864012904 -1.636565063273639 0.16587763380095447 3.791244387626648\n",
      "Jet 21 : 1.235950180192325 -0.3249036402007201 2.5950879679559984 1.3017610311508179\n",
      "Jet 22 : 1.2694760957146978 1.2031240245464168 4.024756707163883 2.304581642150879\n",
      "Jet 23 : 2.1742043019668396 -0.8187841802969698 5.619096225366787 2.9692549109458923\n",
      "Jet 24 : 1.7155691654281247 -1.946153211911147 1.339219871158209 6.129333078861237\n",
      "Jet 25 : 1.8241764609776259 -0.4583588111335749 4.304587439405304 2.0191781520843506\n",
      "Jet 26 : 1.857141397952936 -2.3855661909812333 1.809599636243022 10.174580574035645\n",
      "Jet 27 : 2.922761510476325 -1.4016272097887261 3.352603121186881 6.312683254480362\n",
      "Jet 28 : 2.7302744796205514 0.9707910844578465 3.4801924946643714 4.121097087860107\n",
      "Jet 29 : 6.54679024944158 -0.40351434051706 4.7963473321696535 7.205405384302139\n",
      "Jet 30 : 7.719352285562896 0.05698861185109327 4.894088054182397 7.887947112321854\n",
      "Jet 31 : 5.5301784570685815 -0.7325505520837556 0.42549866773000883 7.085145473480225\n",
      "Jet 32 : 5.729240138476597 -1.3174195632240813 4.749469526610582 11.469781517982483\n",
      "Jet 33 : 5.7668504465005395 -0.5582410449427405 6.107274528069102 6.719196826219559\n",
      "Jet 34 : 12.087777590032037 1.051906178751006 1.3410234988254255 19.65199565887451\n",
      "Jet 35 : 7.548290524147785 -1.0200527366658574 6.115104672570833 11.891637727618217\n",
      "Jet 36 : 8.39979075804532 1.9584276814604697 0.024682996496266884 30.363843590021133\n",
      "Jet 37 : 12.269351423200156 -0.8563226378833156 3.9926875844175087 17.159504413604736\n",
      "Jet 38 : 11.87510766293239 2.128368139455019 3.5981473277384337 50.60110729932785\n",
      "Jet 39 : 14.329460702136545 0.32204021573875086 6.02786203237452 15.255635976791382\n",
      "Jet 40 : 21.239466438112693 0.17438363640847507 2.7504009081608167 21.850238144397736\n",
      "Jet 41 : 16.830003183120517 1.0071800274119338 5.177850171023595 26.13241171836853\n",
      "Jet 42 : 17.291040315689976 0.8125800487906992 2.4976641846824146 23.321376979351044\n",
      "Jet 43 : 22.852879731954356 -0.5045824712357526 0.7820248477215972 26.113952815532684\n",
      "Jet 44 : 21.32926998300937 0.7150711372116415 0.34453450090574805 27.157668501138687\n",
      "Jet 45 : 23.256133492226308 -0.8146893997269093 3.1049835558592913 31.453083515167236\n",
      "Jet 46 : 27.278551592958216 0.14775571881810012 2.2230757712899765 27.78925895690918\n",
      "Jet 47 : 27.591408120905644 0.5677414862397706 4.807275406084946 32.39928048849106\n",
      "Jet 48 : 28.726685986013713 -0.7793173183589521 3.5204932287551287 37.972486078739166\n",
      "Jet 49 : 30.452088279966482 -0.27612098988511274 3.781071257954041 31.79224480688572\n",
      "Jet 50 : 31.79067090021313 -0.07334230417652646 3.106351266565624 32.17128063738346\n",
      "Jet 51 : 36.90568441793363 1.3011460644389963 2.7118964220655375 73.05135083198547\n",
      "Jet 52 : 36.973613300931945 1.2595849129683507 4.44966620634401 70.55467984080315\n",
      "Jet 53 : 37.37702147354704 -0.23297209968124585 0.4000542489810243 38.79082429409027\n",
      "Jet 54 : 51.101288311755205 -0.8939020397313365 5.201169998340195 73.4778827726841\n",
      "Jet 55 : 45.8419596125513 -0.7798230547673821 2.4305376340367992 60.517376869916916\n",
      "Jet 56 : 90.18069310864924 0.42019237839756735 4.20148432792806 100.30191014707088\n",
      "Jet 57 : 74.37252797834975 0.5821490800715299 5.391030110351049 87.95068609714508\n",
      "Jet 58 : 91.64924488537982 -0.46261280738767396 1.2908201758219755 102.49059066176414\n",
      "Jet 59 : 0.0 100000.0 0.0 0.0\n",
      "Jet 60 : 0.09486658695556198 -2.750686206245949 5.582323639959978 0.745521068572998\n",
      "Jet 61 : 0.13144335424892206 2.81889719604635 3.947060906327256 1.105309247970581\n",
      "Jet 62 : 0.1622659750645094 -2.4613094932004693 6.1031071602277365 0.9578132033348083\n",
      "Jet 63 : 0.2035906301547303 1.8824394114111795 1.3688629822679954 0.6842405200004578\n",
      "Jet 64 : 0.23601775779454068 1.634131799301319 3.247691694350155 0.6278223395347595\n",
      "Jet 65 : 0.23818160263270435 -0.10436056020292435 2.746366986572861 0.23947980999946594\n",
      "Jet 66 : 0.3136916217991751 2.127288582945217 4.518517553297337 1.3349525928497314\n",
      "Jet 67 : 0.36987394607037116 -1.3628948635446547 4.2913259972154325 0.7699679732322693\n",
      "Jet 68 : 0.3752500442718573 -1.927942370814699 1.176727647257572 1.3172770738601685\n",
      "Jet 69 : 0.4133892697955252 1.8475064538512136 2.3995771981915817 1.3438479900360107\n",
      "Jet 70 : 0.4284976233567352 1.661567919271363 5.169695927786489 1.16924250125885\n",
      "Jet 71 : 0.4555363589745508 -0.0072500531273114845 5.116001068264638 0.45554834604263306\n",
      "Jet 72 : 0.5673889816085228 -1.6290434954284554 1.9134961216374293 1.5044891834259033\n",
      "Jet 73 : 0.5253305581719443 -0.8021245573206112 1.6074265226529159 0.7035882472991943\n",
      "Jet 74 : 0.5472135082671796 -1.15331131475269 5.735499648178061 0.9533165693283081\n",
      "Jet 75 : 0.5902699593234395 0.9822889667440492 3.2574394952885397 0.9002605974674225\n",
      "Jet 76 : 0.8053212235944286 -1.84829446773064 5.06162886273506 2.6241856068372726\n",
      "Jet 77 : 0.627527022928188 -1.374838785830699 0.3312710310109637 1.3208661079406738\n",
      "Jet 78 : 0.6891588466132603 0.17663857007214584 4.692979496734722 0.7003343552350998\n",
      "Jet 79 : 0.7144010247892659 -1.744845715554413 0.4892215081779288 2.107372283935547\n",
      "Jet 80 : 0.7180439163271767 -1.6849962685200537 2.420938946774562 2.002579927444458\n",
      "Jet 81 : 0.8043599597504228 1.3028690512993388 3.751344779378203 1.5892504453659058\n",
      "Jet 82 : 0.8358245400527367 -0.40275449563962235 5.7033887462521635 0.9045358896255493\n",
      "Jet 83 : 0.9967247688378011 0.3405534382703862 2.6935989731274974 1.06216811388731\n",
      "Jet 84 : 0.8538606998885827 0.7217315662703727 6.140392790612078 1.0860697031021118\n",
      "Jet 85 : 1.4290273722620679 -2.0700273280237846 0.1872487298099448 5.7586119174957275\n",
      "Jet 86 : 0.9443611177827071 1.0986418795417154 0.7198379656601892 1.573972463607788\n",
      "Jet 87 : 1.1552863216978841 -2.505551122925532 1.475806012443007 7.125181794166565\n",
      "Jet 88 : 1.1684249003099534 0.8224383961070053 3.90510612727838 1.590643584728241\n",
      "Jet 89 : 1.1959478152127065 1.622119185518937 0.4490795294689736 3.1461141109466553\n",
      "Jet 90 : 2.006715021248622 -0.20102079597794095 4.307193526610894 2.0794129967689514\n",
      "Jet 91 : 1.709361727116995 -1.3179650784449592 3.295815117486666 3.426189363002777\n",
      "Jet 92 : 1.8109463666625116 -0.6229692948451154 4.499171040968368 2.178617164492607\n",
      "Jet 93 : 1.9037730397472699 1.888162808270375 3.65448145844091 6.434805989265442\n",
      "Jet 94 : 1.954359087995064 1.191952949443856 0.297084512551984 3.520965099334717\n",
      "Jet 95 : 4.304331107602383 -0.7569275382371066 6.163977695695993 5.62505629658699\n",
      "Jet 96 : 5.471964860949537 -0.7993135855563152 4.11859770809041 7.366955935955048\n",
      "Jet 97 : 5.990793945075063 -0.4308041474130722 1.4044783551266706 6.612989097833633\n",
      "Jet 98 : 5.70644925904125 -0.7804286405696176 0.4197143414602057 7.553574293851852\n",
      "Jet 99 : 7.048384564284228 -1.2596889772932298 4.757449905821112 13.454724371433258\n",
      "Jet 100 : 6.674256883997412 -0.44874985241370646 1.918791187561641 7.405705004930496\n",
      "Jet 101 : 8.24174042057763 1.5090698765733142 2.837956464613818 19.562530755996704\n",
      "Jet 102 : 8.551526438047922 1.2297032638138015 4.485454624167464 15.932824313640594\n",
      "Jet 103 : 8.79222361389927 -1.0304279691536102 1.9467393762333878 13.900761216878891\n",
      "Jet 104 : 9.044073140319304 0.9719675045162013 1.3284765802502205 13.713461101055145\n",
      "Jet 105 : 11.653980523739603 -0.8247333778903686 2.352457445036695 15.858471393585205\n",
      "Jet 106 : 13.876965913879083 1.0420389804013754 2.7380786633883982 22.220248758792877\n",
      "Jet 107 : 14.371536171612853 -0.2876566346035179 3.794051282886089 15.091209888458252\n",
      "Jet 108 : 20.005675860046825 -1.3177542318466269 6.119920014099182 40.08760619908571\n",
      "Jet 109 : 25.992656498970913 -0.5045085741583503 0.9529382822426465 29.6765998005867\n",
      "Jet 110 : 28.138642774971647 0.5940585698339643 5.453657606055398 33.2947171330452\n",
      "Jet 111 : 32.987373705322035 -0.5203917043083124 5.164419251491948 37.709830820560455\n",
      "Jet 112 : 33.05637570270966 0.6455237163375281 0.3543000862390138 40.380343198776245\n",
      "Jet 113 : 35.63721218419644 -0.14873161686552128 0.36638157029608226 36.41951867938042\n",
      "Jet 114 : 36.63110495500662 0.32472034725039756 5.9780215738616285 38.78859454393387\n",
      "Jet 115 : 67.14520084290115 -0.7741301411540218 3.3050401429160514 89.44782177358866\n",
      "Jet 116 : 56.94802324890666 0.7693853523812112 4.9139505520184095 75.57121843099594\n",
      "Jet 117 : 46.6570344502225 -0.0835084359409299 3.198825464866114 47.14141993224621\n",
      "Jet 118 : 73.87816747851754 0.05000836412194313 2.28918176285924 75.36429799720645\n",
      "Jet 119 : 75.28610459653778 0.5200341837871725 4.348717025739615 86.7261546254158\n",
      "Jet 120 : 62.842045590793184 -0.9676663432862919 5.20549136125554 95.05595213919878\n"
     ]
    }
   ],
   "source": [
    "for i, jet in enumerate(gen_jets):\n",
    "    print(\"Jet\", i+1, \":\", jet.pt(), jet.eta(), jet.phi(), jet.e())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d04dad18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Jets: 120\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Jets:\", len(gen_jets))\n",
    "\n",
    "\n",
    "gen_jet_pt = [jet.pt() for jet in gen_jets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fc77189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jet pT values in bin 1: [12.087777590032037, 12.269351423200156, 11.87510766293239, 14.329460702136545, 16.830003183120517, 17.291040315689976, 11.653980523739603, 13.876965913879083, 14.371536171612853]\n",
      "Jet pT values in bin 2: [21.239466438112693, 22.852879731954356, 21.32926998300937, 23.256133492226308, 27.278551592958216, 27.591408120905644, 28.726685986013713, 20.005675860046825, 25.992656498970913, 28.138642774971647]\n",
      "Jet pT values in bin 3: [30.452088279966482, 31.79067090021313, 36.90568441793363, 36.973613300931945, 37.37702147354704, 32.987373705322035, 33.05637570270966, 35.63721218419644, 36.63110495500662]\n",
      "Jet pT values in bin 4: [51.101288311755205, 45.8419596125513, 56.94802324890666, 46.6570344502225]\n",
      "Jet pT values in bin 5: [74.37252797834975, 67.14520084290115, 73.87816747851754, 75.28610459653778, 62.842045590793184]\n",
      "Jet pT values in bin 6: [90.18069310864924, 91.64924488537982]\n",
      "Jet pT values in bin 7: []\n"
     ]
    }
   ],
   "source": [
    "import ROOT\n",
    "import numpy as np\n",
    "\n",
    "#  bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100, 200]\n",
    "\n",
    "#  lists to store values\n",
    "x_vals = []\n",
    "ratio_iqr_median = []\n",
    "\n",
    "# Iterating over bins\n",
    "for i in range(len(bins) - 1):\n",
    "    lim_low = bins[i]\n",
    "    lim_hi = bins[i + 1]\n",
    "    x_vals.append(np.mean([lim_low, lim_hi]))\n",
    "\n",
    "    # Filtering out jet pT values within each bin\n",
    "    jet_values_in_bin = [pt for pt in gen_jet_pt if lim_low < pt <= lim_hi]\n",
    "    print(\"Jet pT values in bin {}: {}\".format(i+1, jet_values_in_bin))  # Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10b51263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxwAAAI8CAIAAAD0vjrdAAAABmJLR0QAAAAAAAD5Q7t/AAAgAElEQVR4nO3dT47bxt8n4OKLbI2OZ20HuchA1GWcxD5BshG1ym8OkDjwZURdJAjet7cDd19gOIuKaZqiqD9NiSzyeRAEbYmtrhLb4sffKlZlVVUFAABe5r/GbgAAwBwIVQAAAxCqAAAGIFQBAAxAqAIAGIBQBQAwAKEKAGAAQhUAwACEKgCAAQhVAAADEKoAAAYgVAEADECoAgAYgFAFADAAoQoAYABCFQDAAIQqAIABCFUAAAMQqgAABiBUAQAMQKgCABiAUAUAMAChCgBgAEIVAMAAhCoAgAEIVQAAAxCqAAAGIFQBAAxAqAIAGIBQBQAwAKEKAGAAQhUAwACEKgCAAQhVAAADEKoAAAYgVAEADECoAgAYgFAFADAAoQoAYABCFQDAAIQqAIABCFUAAAMQqgAABiBUAQAMQKgCABiAUAUAMAChCgBgAEIVAMAAhCoAgAEIVQAAAxCqAAAGIFQBAAxAqAIAGIBQBQAwAKEKAGAAQhUAwAC+G7sBd5Vl2dhNAIAFqapq7Cbcz7JCVVjY2QXgmCzLXBFubWm1DMN/AAADEKoAAAYgVAEADECoAgAYgFAFADAAoQoAYABCFQDAAMYMVUVR5Hme53lRFC85ODtwzgsCAAzoRKgqiuIwskQv/MF5nm+32/1+v9/vt9ttnuc9B2dZtt1u49fb7TbLsrIsX9gAAIAB9a2oXpZljDKr1WrYn1qW5X6/32w2saRUFMV2uy3LsjNaxQd3u139bJZl6/U6roQb01XzWQCA++tbpD/P8/1+f4tV/GOhq/nKWZatVqvO+tPhUzGExW9vfn3Oz7UpAQDBFeEulvYmnxj+G7xGdeyVV6vVfr8/dmRPFaoOW2VZGhMEAMbSN/xXFMV6vb7RDz5/tO4wKtXzq2rNOV7HKl4AALfTV6nK87y/SnSdzsRz5k8pyzLmp91uFx+J9a3NZlNVVVVVm81mv9/3vNqxefcnXdxPAGBJTkxUj5GlM1JcPUp6dUqLc7zCt9PSW1PUi6Kom91pUYO7AMDd9IWqcMs5VS39A3ZlWcaByPqGwdphRIvZ69i9hAAAt9AXqvI8v93kpPNfOSaqS2dKSVQAwD1duaJ6HGW7+qce3usXl63qPDjWqDp/XJxi1apdmaUOANzfieG/EMLhli9x0tJLSkHxvsK6EhZfqv5BsTQVR/rqhHTYjLhxzWq1iguy1y/Sk88AAG7kxKpcx+56e/myBXHRzvqPzfnmzfG+ejbVobrlrUYezruqLW0VMgCOcUW4g6W9yX29jbknxp0sy+qw0vz6hZqVqhe+TpyZfnIPwUWdXQCOcUW4g6W9yeduUxNH4mIGitWjFN+mpZ1dAI5xRbiDpb3J525TU68RFb4UlswHBwConVhRXZACADjHWRPV62lVIYTNZhPv/kuxoLe0OiQAx7gi3MHS3uQTw3+73a4eAYzb7W23W2sWAAC0XBwhk97+ZWmRGYBjXBHuYGlv8sJ6u7CzC8Axrgh3sLQ3uWP4L8uyelp6dty9WwoAKauvqoMs9HgLcf70RcZu8rR0bFOzWq3qAb56QhUAMAVnrnfN/XWEqnrdhHpvPgBgIuLubfWK3APqfM24qW74cr8aPU5vqDwz/bXKRQ39AkBLT/VLYeykjlB15hBpovkj0WYDABPXEaqaa1DFdT7rB1t/BACGUn4Ra0Kd89mbw3Px65MFpOZaSM0fYVbW8Krj4ujpZrNpPR5nr/d842Ql2uykleX/G7sJAB3uf0Wo5yQdXliPTVdqHXnpdbyqqlgEWa1W1ZfLd8tut+t/hfq7LuvtlwZf8V3p6ltRPUbgw6QcHzeHnR6Pj9Vvv1WvX1d5Hl6/rn79tXp8NPAK0KEsyzj3PGpGn+122ywmveSW/CzL6v18m6+zXq9d0IdyOlTBpR4fq59+Cp8/h6enLITs6Sl7fg7v3gW5CkjRfn/bz646UcWiUVmWsezx5afv68txfCp+HYtP1XkThes4Fb8lvk49k6cZ6XiJvlBVLwHaejzWrgzEcsyff4Yffgh//fX1joePH7Mffwx//DFiowAuc5+Kez0ctNvtWhfWOjANtVhoK4EVRVGXrJRRBtEXquJZXK/XRVE0p7Ztt1uLgtLj48dvEtWXB7OPH0dpDsDF7lZx32634dtlt5tiMamuM71E5x1mdZaa7CLvaekLVSGE3W63Wq222+36i/1+v9lsRFqOeX6unp6OPRWenowAAgm4c8X92ODPgINCx2KTKsmATiz+WS+q3lxm/bYtInEPD9n333fnqoeH8P33NooCEvDxY6xRtR7MHh6q338f7KfU19btdttZrahrVM1lEW5hkGIYl62oLlFxjp9/Ds/P1ceP33wevX9fvXo1VosALnCy4n6Lfx+OFWvyPJeohnI6VDXf7qqqsixbrVaG/+jx4UN49y6EUIXw7+fO+/fV33+HT59GbRbAeV5Sca/XRziceN5js9n0H3y7ooYL+oBOhKq4ZU2cRFWvpR6XzXAaOObNm+zTp+rt2/DwUD0/hxDCq1fh06fw9q2xPyANV1fcm8sfnExCzQOMBc3A6bv/drtdURT1yS6KYrPZNJfNgENv32ZVlT09ZbtdqKrs//yfTKICEvLhQ/jnn/DLL1/vrYkV9w8fTnxjTzbqmZ187JJa7ydz4qee4dhE9VgxMV19EKcX/zw8l2685Hx5LksB6XnzJvv0KTw8hIeHKoTq4aE6s+Le3GWv9VRncopp5lipIt50f2njO8W1G1rqC7or+yBOLKkAV8iycN4avwDT9eZN9vvv/1bcn56y//znsop7KyfVs2jCt9WK+pjD7WKaY0SXNr5Tq0pSFEWdtAw+DuL08N9hdo5vvRMAwBJcWnGvd0der9dZluV5nmVZcy+a1vHN7WLi8fFb6oG51gW3Lm7leX5R3trv983XrxPVse2cuVj/fsvxzK1Wq/hFfeIPd9hOwsn+8nLH3mPvPTApN70iHIspcXe/84/vvNq21kbvb0l9cHVkBOFYk2r1dKsz+t22tMtuVp0ap2mWB+sTkGiZKt7M2OPku8FJx8b+jAkCk5Jlp6+AL9Tc4e2c+eZlWTZ31+2pQjWP7C9W1Rfx2Nn6G+NTt76a3+FNnpSF9XZhZ/f+epKTUAVMykKuCK1QdWcLeZNrJqozmP7YVFXhVKEQABLWsfjnmcVA61QBANQ6QpU9gLiC0T0AFq5v+G+1WvXcFHC3JjIbRgABmLGOUBUzU9yLpl4zw2AfPZSpAKYpz/Pdbmclqvu4bEmF1Wp1hzswb2dptyHczfmhSvwCJsIV4Q6W9iZf0NtWukqxdrW0s3sfl+YkuQqYAleEO1jam3xxb8dd8eKFlnZ270OoAlLkinAHS3uTO+7+6xTXYK1vDGytkc9iSUgAEJ0IVYdZaqi9slmseA+gKAbAzHSHKlmKc8hGAFDrCFX1rsObzeacDSBZJokKAJo6ZpBl563PmOLUs6XNmLupl4QqgQwYnSvCHSztTe6oVK1Wq/u3g7S8MBWZVgXA/CwrQi4tMt/OyyORUAWMyxXhDpb2JndsU2NfGvrJQwBwqHvvv6IoiqKI6cp9f9yCzZUBmJmOUBVCiJWqmK7KssyyLMuyeaSrrNfYrUuAMhUAdOoOVbU6XcUNrut0le7gYNVr7NYBAKk6EapqcRywTh7r9TrdXMXVhi1TGQEEYE6WNS1/abchDG7wsT+DicBYXBHuYGlv8tFtaqLmRPX4SPx6u90u6m0iCEAA0KsjQpZluV6vm49UVXU4iTvFULW0yDyg2yUqWQ0YhSvCHSztTe6YUxUT1W63a85PDyFsNpvdbhcfX9R7BABwUvfwX9xKOYSQ5/lut1uv15vNZh5LKnAFxSQAOOn03X91urp1UwAA0nV08c/7NoPpunWZysIKAMzDuetUAQDQQ6iij9lUAHCmo+tUtb4+XD/dECFDiSOA0hsASetYQOLMfYVTXFVhaQtmvNA9g45QBdyZK8IdLO1N7qhUxbWpAAA437Ii5NIi80vcv3SkWAXckyvCHSztTe6eU9VUbwJoEtVyyDfAEpw53QXOdDRCHu4AGO12uxiw9vt9cvFzaZH5aqOEKkkOYGaWdtntrlQVRbHdbkMIq9UqRqhYr9rv93XSSnTqVf+/SxZ17o8ZK9y4BxCApHVEyJioVqvV4TIKIYRYozr27MQtLTJfZ8RkI1QBzMnSLrtHl1Q49i7UlZ4U36alnd0rjBtrhCqAOVnaZbe9onqsP202m86j47Nx4C/FShUTZx9AANLVPafq2I1+eZ4vKnIujUIRAFztmr3/1KgAAFraoSrWqPpjk1A1S8pUAPASF09UP+eAyVrajLmLTCdUTaclALzE0i67HcN/cR56lmWHFamyLGOiSnSRKo6RYwDghbojZHPxzxBCXPwzhLDf70MIm82mKIp7tnIoS4vMZ5paoppaewC4ztIuu329jet8Nh9JdM3P2tLO7pkmGGIm2CQALrW0y+5ZvY0bKt++MTe3tLN7jmnGl2m2CoCLLO2yO+neFkURC2N5np8ccDzn4KWd3XNMM75Ms1UAXGRpl93uvf/O+c5bT6tqDT72jzzG6fNxBlj8rt1ud1hdW9rZPWnK2WXKbQPgHEu77B5dUuGkm75NZVmu1+t6RnycON+Zk8KX+NV89tiiD0s7uydNObhMuW0AnGNpl92J9vYwFWVZdqxYdfhUDGFCVb+Jp5aJNw+Ak5Z22b1mm5r7iGN5zT+2bkVsPjWPefQ02VwZgLR0b6g8BefnpMPyVVxkix7qQAAwrClWqjrH+M7MWCfXfM+udX1/pkeiAoDBTbFSdfVYXn3D4LEp7SHNLQsBgOmbYqWqU/9K7rFAtd/vN5tNVVWmWPVIqExlWhUACZlipSo6fz+cuP5C6lvoAABJO/dex3qx8pu2phYH8lpLKhzbyPnYqlSdRy58+C+hMlWUXIMBqC3tsnu6t82Vzauq6lkvakCt4lMrYzWXBo1fhxA2m03rRQ4T2NLO7qEUM0qKbQYgLO+ye2L4LxaBNptNWZYxWsX1ovI8v2muyvN8s9lst9v6trvDu/laDThcRuHWG+kkRzoBgNvpi5DNzWGaa5QfW6/8FoYddlxaZG5JNFQl2mwAlnbZ7bv771igiRWg+8wKz/PcrXyDSDeauAcQgCQks6QCAMCU9YWqYxWpWDpSQEpIumUqAEhF30T1PM9Xq1W8Cy8+EmdTha5b7ZisGSSqOAKYei8AmLfTM8jqIFXr2QRm4pY2Yy6aRxyZRy8AFmVpl92F9XZhZzfMKIvMpiMAy7G0y+5lE9XLsrQVDADAoROhqiiKLMtikCqKYr1er9frLMusq5mEOVV3LKwAwMT11eXqHWDiMXFx891uVxRFa2O+VCytDjmnUBVm1x2A2VvaZff0kgrx7YjFqnp19XCvxT+5mggCAPd0YvivXkyhubp6/H+ioSrrNXbr6GMEEIAp6wtVeZ7HTZRDCNvttjNgJafqNXbrBqNMBQB3diJUhcbue3WBKk60SjRUAQDcwokZZPXKn6vVKhao6unqKYaqhcyYm3eZat69A5iThVx2awvr7QLO7uwzx+w7CDAbS7jsNl22+CcAAJ1OhKqyLPM8d6NcKpZQxXEPIADT9F3Pc/Wc9Pq+PwAAOvWFqubin0zfEspUADBZ5y7+CdNhBBCACTp38U8mTpkKAMZ14l7HpHekOTTjezuXFqqW1l+AFM34stvpxET1EMJ+v++8129Rb9PESRgAMLq+UBWZVsUExWlVoiQA07Gsutws65CLzRaL7ThAKmZ52e1x7orqZVnOZmbVnAgWADARp0NVXFF9vV6v1+u4lnpcvwrGZWEFACblxJyqOEV9tVrleZ7neaxXbbfb8GVpUEakTAUA09E32FkUxXa73e12cWGF1uMpjpLObHBXqPIOAEzZzC67J/X1Ni7+2XlAlmWHYWv6Tu4DndC5lyci7wPAZC0tVJ1eUmFmFnV2AYC7ObFNTehaTj0+nlyZak6UZwBgak7U5ZoT1eMjcZb6ZrNJcaL6bOqQQlWTdwNgmmZz2T3T6d7GaenNRxJNVGEuZ1eGaPGGAEzTPC6757ugt2VZpj7kN4+zK0O0eEMApmkel93znTVRvbWceurRKmkCBABM04kIWZbler1uPbharRLdsib1yCxRHeOdAZig1C+7lzqxTU1MVJvNZrfbVVW12+1Wq9V+v1esAgBo6gtVcTb6brcriqJeRqEsy81ms9/vEy1WpUsxBgCmrC9Uxdh0WJRK9NY/ZszmygCM7vTin4fUqO5PmQoAJu704p+Hq1JlWZboXPV0Z8wJVSd5iwCmJt3L7nX6eluWZVEU+/0+hLBareKDrT+GEPI8T2VAMNGzKy6cyRsFMCmJXnav1tfbw7XUj0nlLUv07MoKZ/JGAUxKopfdqy2stwmeXUHhfN4rgElJ8bL7EifWqTqU4lQqFsI9gACM6ESoKooiy7IYpOLX6/W6foRbU3oBgFScmKgeV1SPx2RZFr6sBbrf71Ms6GWn6hiT6pREdQVvGsB0GP77Kt7TF9+OWJra7Xb1vX6JFquqXmO3jpcyAgjAWE4M/9VLJzRXV4//TzRUpULFBQDSciJUxVWpQgjNtRWObV8DALBYp4f/8jyP+Wmz2YTGRCuh6naUqQAgOSdmkNXrf9b70sS53od71yRh4jPm9vtqtcqCUPVi3kCAKZj4ZXdwF/e2LMt0a1TTPLuPj9Wff4aPH8PTU/j++/D0FP7nf8KbN6ZbX0+oApiCaV52b+eCxT9NpbqFx8fqp5/C58/h6SkLIXt6ykII796Fx8cF/RYCwAycDlV5nmdZFpf9DCFkWZbiwN9k/fln+OGH8NdfzbpU9uOP4Y8/xmrRHFhYAYD7O1GXq2dQlWUZF/zM83y/39dTrNIywTrk69dVrE61PDx0P86ZjAACjG6Cl92bOn33X1xCvR71K8tys9nUSy3wEs/P1dPTsafC09OCfhEBIHV9oerYJKqkV1SflIeH7Pvvjz0Vvv9epep6RgABuLPvxm7A0v38c3h+rj5+/Ob6//599erVWC0CAK7RV6k6th1NvSjobZq0LB8+hH/+Cb/88nWk7/376u+/w4cPIzYKALhYX6WqKIq4fnq9A2B8ZL/fx9XVebk3b7JPn6o//ggPD9Xzc3h4CK9ehU+fwtu3xq5eKo4ALmmKJABjOj0tv15UvZbocuph8rchlGWV57LUkIQqgBFN/LI7uAt6m/Ra6tHSzi5CFcCIlnbZXVhvF3Z2EaoARrS0y+7RiepxbarWMF9ZlmVZFkWRJXu3etZr7NYxMAsrAHA3HREyTk5vPlIvpN568OatG9rSIjNBsQpgPEu77Hb0tt6aJs6gqgNW/UhIdj2FpZ1dglAFMJ6lXXa7l1Ro3t+32+3W63W6d/yxcBZWAOA++hb/jGJRKtHSFADAfXSHKhEKAOAipytVkDr3AAJwB0IVAMAAuieqH26ifPiIIUIAgNrRJRVOSvEmyaXd20mTewAB7mxpl92OStVms7l/OwAAkrasCLm0yEyTShXAnS3tsjvmRPW4veDhDoM98jw/nN11uIWfdUo55B5AAG6qe6L6HTQ3E9zv93Gr5v5vKcuytf8gAMBEjFOpivFos9lUVVVV1Wazibmq5/iiKFrbPNdPhRB2u13VoFIFANzZOIOd8QbD5o/Osmy1Wh3LVc0bEne7XXM1h6Iottvtmb1Y2uAuLaZVAdzT0i67o82pWq1WrT/2DO3F+tNutzt8qs5h5wwgsnCmVQFwO2POqRrw1ZqlrJ6KFwDAjYxQqepMPFdnrFjfak3P6nm1w1sFz3Rd8wCAhRihUjVsjepwilX/TYKLGtzlUBwB9FsAwOCmsqHy1QN2hxEtPmIEEAC4p9FC1a1Dj/2eAYB7GidUHd7rF5etuvR1yrI8XD9djYp+7gEE4BbGCVUxBtXFpPhFnY06o1KnPM9Xq9V2u62DVFEU1+UzAICXGGdJhTzPN5vNdrut76o7XIPqzIJTTGDNxdY3m40V1QGAOxt5qdOYnF4+/ymu/Bm3Z+45bGlLu9LDPYAAt7a0y+7Ceruws0sPoQrg1pZ22Z3KkgoAAEkTqgAABiBUsVAWVgBgWEIVAMAAhCoAgAEIVSyXEUAABiRUAQAMQKgCABiAUMWiGQEEYCjj7P03oqz3ErqohV8BgAEtLlSJTQDALRj+Y+mMAAIwCKEKAGAAQhUAwACEKgCAAQhVYFoVAAMQqgAABiBUAQAMQKiCEIwAAvBiQhUAwACEKgCAAQhV8C8jgAC8hFAFADAAoQoAYABCFXxlBBCAqwlVAAADEKoAAAYgVME3jAACcJ3vxm7AvWW9F8yqqu7WEgBgThYXqsQmAOAWDP8BAAxAqII206oAuIJQBdzVfm8IHpgnoQq4h8fH6rffqtevqzwPr19Xv/5aPT5KV8CsCFXQwQjgsB4fq59+Cp8/h6enLITs6Sl7fg7v3gW5CpgToQq4uT//DD/8EP7662tQ/fgx+/HH8McfIzYKYGDZopYYyLJl9ZeXyLLgl2Uor19XT08dpb+Hh+7HgXlY2mVXpQq6GQEcyvNz9fR07Knw9LSgD1xg3pYVIZcWmXkhxapBZFkIoQqhM6JWVSW6wmwt7bK7uBXVgTtoFvmqKvz6a3h+rj5+/CY/vX9fvXr19cglffAC82T4D44yAniRLPv6X1V9/S+E8OFD+Oef8MsvX3PT+/fV33+HDx++HlZ/L0CihCrgej1BqunNm+zTp/DwEB4eqhCqh4fq1avw6VN4+/ZrhmqlK4DkLGuwc2mDu7ycaVWHWkN7lyrLKs9PhybDgjADS7vsLqy3Czu7vJxQFb0wSA3yo50ISM7SLrsmqkOfOBq1pM+Er0YMUk31j5augIkTqoCvJhKkOklXwMQtLlRlvTNgF1WlhGjKQaqTdAVM0+JCldjEpWY5AphckOokXQGTsrhQBYs1jyDVSboCpkCogjmbcZDq1EpXS+gyMB1CFZyW1gjg0oJUp9hxhSvgnoQqmAkB4pBhQeCehCpImKLUmaQr4A6EKjjLdEYABamXkK6A2xGqIAGC1OCkK2BwQhVMlCB1H9IVMBShCs51hxFAQWpE0hXwQkIVjEyQmhqLXQHXEapgBIJUEix2BVzkv8ZuAKRnv7/m6pplX/+rqq//MXH1mapPH0AnoQrO9fhY/fprlWVVnofXr6tff60eH09kIkFqTqQroJ9QBWd5fKx++il8/hxCyELInp6y5+fw7l04zFWC1OxJV0CnrFrSh32WLau/DOi336rPn8Nff31z/fzll+rhIfz+e2aO1MKZdwWdlnbZXVhvF3Z2GdDr19XTU2dFogoh82tFJF1B09Iuu4u7+y/rrdQv6txzvufn6unp6LOfP1chGAEiBItdwbItK0IuLTIzoGOVqoeHYxUsCEG6YtmWdtldXKUKrvPzz+H5ufr48Zv89P599erVWC0iDZYSheVw9x+c5cOH8M8/4Zdfvl4S37+v/v47fPgwYqNIiRsGYfaEKjjLmzfZp0/h4SE8PFQhVA8P1atX4dOn8PatyyMXsBwDzNiyBjuXNrjLjZRllecuhgzDpCtmbGmX3YX1dmFnF0iIdMX8LO2ya6I6wCRYjgFSJ1QBTIt0BYkSqgAmSrqCtAhVAFNnsStIglAFkIwYpxSuYJrSWKeqKIo8z/M8L4rizG/J87wsyxu2CWAkFruCaUqgUpXn+X6/j1/v9/uyLE+mpbIs628BmCuTrmBSpl6pivFos9lUVVVV1Wazibmq5/iiKNbr9R3bCDAytSuYgqmvypVlWQih2cgsy1ar1bFclTU+S3a7XZ7nrWcn3l+AQahdMQVLu+xOvVIVQlitVq0/9gztxYLWbre7fbsApkvtCu4vgVDVqjYBcL7r0tV+v6DqAgxl0qGqc4zvhRkru9ZLfijA6FrpqtPjY/Xbb9Xr11Weh9evq19/rR4fpSs416RD1S1qVNW1Bm8JwCiOFa4eH6uffgqfP4enpyyE7Okpe34O794FuQrONOlQ1cnqUwAvdzgs+Oef4Ycfwl9/fc1ZHz9mP/4Y/vhjxGZCShJYp0qKAridbxe7ao8LfvyYPTxUv/9+50ZBkqZeqTq81y8uWzVWewBm6fn56Bjf83N4ejICCKdNPVTFfWnqyVXxi3qzmrIssyw7f+8aADo9PGTff3/sqfD9927WgdOmPvyX5/lms9lut/X9d4drUBkfBHi5n38Oz8/Vx4/f5Kf376tXr8ZqESQmmaVOY3J6+XoKqfQX4M4eH6t378KPP4Y6V71/X/35Z/jv/w5v36pUcY2lXXanPvxXy/PcKqAAt/PmTfbpU3h4CA8PVQjVw8O/NSqJCs60rAi5tMgMcJ2yrPL83yyVZTYQ5EpLu+wurLcLO7sAg5CruM7SLrvJDP8BMJa4RijQT6gC4DS5Ck4SqgA4i1wF/YQqAM4lV0EPoQqAC8hVcIxQBQAwAKEKgMsoVkEnoQqAi8lVcGjqGyoPLuv9GFjUGmUALxFzlU9NqC0uVIlNAEORq6DJ8B8A1zMOCDWhCoAXkasgEqoAAAYgVAHwUopVEIQqAAYhV4FQBcAw5CoWTqgCYDByFUsmVAEwJLmKxRKqABiYXMUyCVUADE+uYoGEKgCAAQhVANyEYhVLI1QBcCtyFYsiVAFwQ3IVyyFUAXBbchULIVQBcHNyFUvw3dgNuLes9691VVV3awnAosRc5VOWGVtcqBKbAIBbMPwHwJ0YBGTehCoA7keuYsaEKgDuSq5iroQqAO5NrmKWhCoARiBXMT9CFQDjkKuYGaEKgNHIVcyJUAUAMAChCoAxKVYxG0IVACOTq5gHoQqA8clVzIBQBcAkyFWkTqgCYCrkKpImVAEwIXIV6RKqAAAGIFQBMC2KVSTqu7EbcG9Z79/Uqqru1hIAjom5ykcyaVlcqBKbAJIgV5Ecw38ATBDtQ4QAAA4fSURBVJRxQNIiVAEwXXIVCRGqAJg0uYpUCFUAAAMQqgCYOsUqkiBUAZAAuYrpE6oASINcxcQJVQAkQ65iyoQqAFIiVzFZQhUAiZGrmCahCoD0yFVMkFAFADAAoQqAJClWMTVCFQCpkquYFKEKgITJVUyHUAVA2uQqJuK7sRtwb1nv37yqqu7WEgCGEnOVj3DGtbhQJTYBzJJcxegM/wEADECoAmAmTK5iXEIVAPMhVzEioQqAWZGrGItQBcDcyFWMQqgCYIbkKu5PqAJgnuQq7kyoAmC25CruSagCABiAUAXAnClWcTdjhqqiKPI8z/O8KIqXHJwdOOcFAVgIuYr7GG3vvzzP9/t9/Hq/35dlWZblIAcDQIudAbmDcSpVZVnu9/vNZlNVVVVVm80mRqUrDo5f7Ha7qkGlCoAW9SpuLavGyO1ZloUQmj86y7LVatWZq/oPLopiu92e2YssG6e/AEyEetU9Le2yO9qcqtVq1fpjPcB30cHNkpUxQQD6qVdxO2POqRrw4KzxV+RYxQsA4HZGqFR1Jp6rM1YsWbVmXPW82uGtgmc6v3kATJliFTcyQqXqovx00m63a75gURRxYvux4xc1uAtAJzcDcgtTWfzzogG75sGHES0+YgQQgB7qVQxutFB1dYo607D1MADmR65iWOOEqsN7/eJKVJceXJbl4frpalQAnEmuYkDjhKoYg+piUvyizkatqNRzcJ7nq9Vqu93WQaooip58BgAtchVDGWdJhTzPN5vNdrut76rb7XatY+qclOf5brdbr9edB8cEtl6v60c2m40V1QE4n3nrDGLkpU5jcjpz/lPPwXHlz7jjcs8rLG1pVwDOJFTdwtIuuwvr7cLOLgDnk6sGt7TL7lSWVACAcZlcxQsJVQDwL7mKlxCqAOAruYqrCVUA8A25iusIVQDQJldxBaEKAGAAQhUAdFCs4lJCFQB0k6u4iFAFAEfJVZxPqAKAPnIVZxpnQ+URZb1/Mxa1mj4AZ7LjMudYXKgSmwC4glzFSYb/AOAsxgHpJ1QBAAxAqAKAcylW0UOoAoALyFUcI1QBwGXkKjoJVQBwMbmKQ0IVAFxDrqJFqAKAK8lVNAlVAAADEKoA4HqKVdSEKgB4EbmKSKgCgJeSqwhCFQAMQq5CqAKAYchVCydUAcBg5KolE6oAYEhy1WJ9N3YD7i3r/U2vqupuLQEA5mRxoUpsAuDWYrHKBWdpDP8BwPAMAi6QUAUANyFXLY1QBQC3IlctilAFADckVy2HUAUAtyVXLYRQBQAwAKEKAG5OsWoJhCoAuAe5avaEKgC4E7lq3oQqALgfuWrGhCoAuCu5aq6EKgC4N7lqloQqAIABCFUAMALFqvkRqgBgHHLVzAhVADAauWpOvhu7AfeW9f7yVlV1t5YAQPiSq1x/ZmBxoUpsAmBq5Kp5MPwHAOMzDjgDQhUATIJclTqhCgBgAEIVAEyFYlXShCoAmBC5Kl1CFQBMi1yVKKEKACZHrkqRUAUAUyRXJUeoOq1/EfZEza9T8+tR0Kl0zK9T8+tRSLNTJ3NVip2aMaEKAGAAQhUATJdBwIQIVQAwaXJVKoQqAJg6uSoJQhUAJECumr7lhqpb3DFx/muOe+RF5tepi15zfp0avfvj/nR/oc488nyj/0al8rs3yMu+JFel8uYnbbmhCgCSc5Cr/vdYLeHQ4kJV9kXz6+aDADBlVRWyrPrtt+r16yqE8vXr6tdfq8fHaux2sbxQVX3R/Lr5IABMWcxPnz+Hp6cshP96esqen8O7d0GuGt3iQhUAJO3PP8PPP4e//vo6uvLxY/bjj+GPP0ZsFCGEkC2qPJNlX/vb/Pr875rHkaM3IJUjR2+A7idx5OgNSOXI0Rswm+6/fl09PXXMV3l46H582J9+0ZEXvT8zoFIFAMl4fq6eno49FZ6eFpRgJmhZEdJUdADS939D+F9dj38+8viYlhUzFtVbAEjdr79Wz8/h48dvygTv31evXoX//EftYEyG/wAgJR8+hH/+Cb/88rUm8v599fff4cOHERtFCEIVAKTlzZvs06fw8BAeHqoQqoeH6tWr8OlTePtWmWpkhv8AIFVlWeW5LDUVQhUAwAAM/7XleV6W5eHjRVHkeZ7neVEU927TC/Q3O8VOza9HTZ2/fil26nAPqFbjU+xUCKEsy9n8+h2eo6j5G5hWj2qz/JSYZadm6HCrliXb7XYhhN1u13p8tVo137TVajVC4y5Xt7Zuf7NryXUqnp1Wj5oHJNejltj+zWZz+GBanarPVFOzXyl2qqqqzWZTN3gGv37HLgr1p0RyPYrm9ynR/OjrbHaKnZoroepfu92u/sRshar4C11fEuJhh8FrauJfs2Y7m58vKXaq9fnY6kKKPWqqPzeb4SPRTsVm9z+baKdav2/11SvRTrXEIBK/TrRHrc+9eXxKzPujb2aEqn91/kOt+VTrken/U+CwkfEvW/1scp0KB1WcZptT7FFT/c/QZh8T7VTzN+1Qop2KZ6f5yGazqU9Wop1qap21RHvU3+x0OzXjj76ZEaq+0Tn8d/gLevjxOkGr1arVkWazU+zU4b+9mp81KfaoVjf+MFSl2Km6kbvdrvOspdip/gtVop1qmtnvXi31T4nOq1LqH+YzZqL6WfI8H7sJF4szapuP7Pf75h+T61Td4LIsy7KMmw41e5Fcj6I4sbTz9oiQbKdCCFmWrdfr9XqdZVmrF4l2Ks4CzvM89qh1yhLtVBQb35rjnGKP4kmJN0YURRE/JZr9Sq5TscGtX7bUP8xnTKi6RnK/wXUE6ZxBHCXUqXipDiFsNpueZifRo7Ist9ttz3lpSaJT8RO/Lg+sVqv9ft9zU9L0OxUvadvtdrvdhhA2m81+v1+v18eicEihU7WyLPf7/clfwlR6FOs0zZPVc3Aqndput/Uv28n7+1Lp1CwJVfOX53mMILvdbh5/2aqq2u12q9Vqu92mfv/wer1erVbzOC+1OFRRn5qyLOPJGrVRw6iqqizLoiiqqgohxL9ZqYtnah6/hHme7/f75qztGXxKxLwbi75Zlm2329btfkyHUHWNnn+eTkosUNUfMf0fmql0KoqDL/2X6un3qL6YFV+EEOI1+1jjp9+p0HV57hzCqE2/U7H9rStZfwlk+p2qxY+Ik4dNv0ex5LbZbOoUVRRF6p8SIYQ8z6svN0bED/P+45Po1Fx9N3YD0pDi72hZlrEKku6VrKkzasR/lTaPuXOrBtH6xN/v9/v9vjmB7P5NupHUOzWnf5bUYv7orOUk2qPDCXz7/b6eY5pop8K356g1pyrdTs3QfefFT93J+yyicHCD6wT1n9/kOtVaiyVq9iK5HnUKXYtk9hwwQXM9U4fNnkGnquN3NSbao1n+7h2eo/DtnMUUOzVXQtU3OkNVa5W/JO5Wba4k2dI8IK1OxR7VZycOWCTdo0OtT8NEOzXLM9Vq9jw6VR2/ACfao9Zib/E0tVY0TbpT9Wd7/WyinZorb/03jm1T07ovZvqL1fbcyHPsmOl3qjqYSdD611uKPWo5vMIl2qnWmZpHp1oTj2bw69e/+naKPapO7dkyj051/ss/uU7NUladmvJGLY5bz+MemVpynapnDxxrc3I9OkeKnYrLicVNXo8dEFLrVDjV7EQ71SPRHs3vNM31L9TMCFUAAAOwpAIAwACEKgCAAQhVAAADEKoAYM7iJPexW7EIQhUAnCXLslvfXpfneXZc/0/Psqz5x7hTWZZlcRP6+PX5OyHGb+88viiK+qnWD104d/8BwFmyLOvZ+6spbhR2xRW2+eJlWcbtk5vJ5liuiqstNDcyjxt+bzab+lvqR86MVjEwHfYibv4TH48biKmE/WvMRbIAIB3hyK4+h+KCnC/8cZ2bPp35445d4uM6omcuEHpsedjW+3D+C86e4T8AuFK9IGez9hM3gA9fqjiH31U/XhRF63uvUxRFc9X1+IKdW2vEp1qt6uxF+FIVaz14uAl3q5a2aGOnOgBIQ/i2QtPaPaa+pDZ3NOqsM8XXaX3vsR3SzqlUtb79ouv7sV4ce6nD7QUHKcvNg0oVAFysKIr9fl9HmRgs6tJOnTOOlXD2+31rQ+444ekKnRvUHEalS3sRxYDYrGzt9/vWi8fjTasK7v4DgCvEKeR1/sjzfLPZ7Pf781+hzlvxe8O1ueSc7zq8qTA+frIXrcG+w7G/i5oxe9+N3QAASFVrKtX533hY7Nlut3Fu06VtOOfnNmd3bbfbw2d7Xm21WtUxq2fbZqEqCFUAcMyxlBMDxH6/PyxNXReMBl/+qtWwOA89fl2WZXz2zF7EBRTiI4djfzQZ/gOADnmeH5vnFANH5xTy6+JRTwXonHa2Hom551jpqI5QZ/aidSfjsVlit14WNQlCFQB0ODn/uvVUnLd05ou3ikPDLkkQG9aZCA+jzzm9iCOAsZHCUw+hCgA6tFZpav0xTuhuzuCON/Q1X6F/mlGWZXXtJw6rvaRS1fpZsSXNfWbq8bvWYSd7Eb70uvOp8LIy29wMvUYDAMxEK0O0RspazzaXsKoX3uxcgT0+3pyc1HnYRetUHR7WufjnbrdrLSvV04vWjwhHVk6Pr3CykUtg7z8A6NNfiel5Ni6YfvhUvYdgXV56YZmnZwO++Hh8/Z6f8sJJXYOsCz8DQhUA3NX5GzNf9JpjXdBH/NFTY04VACRvs9mMMqupXrmUIFQBwP0NHoBGHH0z8FdTsgMAGIBKFQDAAIQqAIABCFUAAAMQqgAABiBUAQAMQKgCABiAUAUAMAChCgBgAEIVAMAAhCoAgAEIVQAAAxCqAAAGIFQBAAxAqAIAGIBQBQAwgP8PVqI2g2NZD/gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ROOT\n",
    "import numpy as np\n",
    "\n",
    "# Define bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100, 200]\n",
    "\n",
    "# Initialize lists to store values\n",
    "x_vals = []\n",
    "ratio_iqr_median = []\n",
    "\n",
    "# Iterate over bins\n",
    "for i in range(len(bins) - 1):\n",
    "    lim_low = bins[i]\n",
    "    lim_hi = bins[i + 1]\n",
    "    x_vals.append(np.mean([lim_low, lim_hi]))\n",
    "\n",
    "    # Filter jet pT values within the bin\n",
    "    jet_values_in_bin = [pt for pt in gen_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Check if there are any jet pT values in the bin\n",
    "    if len(jet_values_in_bin) == 0:\n",
    "        # Append NaN values\n",
    "        ratio_iqr_median.append(np.nan)\n",
    "        continue  # Skip calculation for this bin\n",
    "\n",
    "    #  IQR and median for the jet pT values\n",
    "    jet_iqr = np.percentile(jet_values_in_bin, 75) - np.percentile(jet_values_in_bin, 25)\n",
    "    jet_median = np.median(jet_values_in_bin)\n",
    "    ratio_iqr_median_ratio = jet_iqr / jet_median\n",
    "    ratio_iqr_median.append(ratio_iqr_median_ratio)\n",
    "\n",
    "#  We need to filter out NaN values,ROOT unable to print it\n",
    "x_vals_filtered = [x for x, y in zip(x_vals, ratio_iqr_median) if not np.isnan(y)]\n",
    "ratio_iqr_median_filtered = [y for y in ratio_iqr_median if not np.isnan(y)]\n",
    "\n",
    "# Create a TGraph with filtered values\n",
    "gr_ratio = ROOT.TGraph(len(x_vals_filtered), np.array(x_vals_filtered), np.array(ratio_iqr_median_filtered))\n",
    "# Set the titles to empty strings\n",
    "gr_ratio.SetTitle(\"\")\n",
    "\n",
    "# Create a canvas\n",
    "canvas = ROOT.TCanvas(\"canvas\", \"Ratio IQR/Median vs Jet pT\", 800, 600)\n",
    "\n",
    "# Draw the graph with connecting lines\n",
    "gr_ratio.SetMarkerStyle(20)\n",
    "gr_ratio.SetMarkerColor(ROOT.kBlue)\n",
    "gr_ratio.SetLineColor(ROOT.kBlue)\n",
    "gr_ratio.GetXaxis().SetTitle(\"Jet pT (GeV)\")\n",
    "gr_ratio.GetYaxis().SetTitle(\"Response IQR/Median\")\n",
    "gr_ratio.Draw(\"APL\")\n",
    "\n",
    "# Add legend\n",
    "legend = ROOT.TLegend(0.75, 0.75, 0.9, 0.9)\n",
    "legend.AddEntry(gr_ratio, \"Jet pT\", \"p\")\n",
    "legend.Draw()\n",
    "\n",
    "# Show the canvas\n",
    "canvas.Draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59f5f143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.16457189530866465,\n",
       " 0.23566148811820167,\n",
       " 0.10994997847640825,\n",
       " 0.12499613603466397,\n",
       " 0.09782764492026934,\n",
       " 0.008076512553058246,\n",
       " nan]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_iqr_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0468d700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reconstructed particles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0be43ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_px = preds_unpacked[\"pt\"] * preds_unpacked[\"cos_phi\"] * msk_pred_particles\n",
    "reco_py = preds_unpacked[\"pt\"] * preds_unpacked[\"sin_phi\"] * msk_pred_particles\n",
    "reco_pz = preds_unpacked[\"pt\"] * np.sinh(preds_unpacked[\"eta\"]) * msk_pred_particles\n",
    "reco_phi = np.arctan2(preds_unpacked[\"sin_phi\"], preds_unpacked[\"cos_phi\"]) * msk_pred_particles\n",
    "\n",
    "reco_px_np = reco_px.detach().cpu().numpy()\n",
    "reco_py_np = reco_py.detach().cpu().numpy()\n",
    "reco_pz_np = reco_pz.detach().cpu().numpy()\n",
    "reco_phi_np = reco_phi.detach().cpu().numpy()\n",
    "\n",
    "# print(\"reco_px_np\", reco_px_np)\n",
    "# print(\"reco_py_np\", reco_py_np)\n",
    "# print(\"reco_pz_np\", reco_pz_np)\n",
    "# print(\"reco_phi_np\", reco_phi_np)\n",
    "\n",
    "reco_pred_mom = np.sqrt(np.sum(reco_px_np, axis=1)**2 + np.sum(reco_py_np, axis=1)**2 + np.sum(reco_pz_np, axis=1)**2)\n",
    "\n",
    "reco_E_np = np.sqrt(reco_px_np**2 + reco_py_np**2 + reco_pz_np**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4726630d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E Shape torch.Size([8, 140])\n",
      "px Shape (8, 140)\n",
      "py Shape (8, 140)\n",
      "pz Shape (8, 140)\n"
     ]
    }
   ],
   "source": [
    "# four-vectors\n",
    "# px_py_pz_E = np.column_stack((px_np, py_np, pz_np, E)) \n",
    "print(\"E Shape\", E_np.shape)\n",
    "print(\"px Shape\", reco_px_np.shape)\n",
    "print(\"py Shape\", reco_py_np.shape)\n",
    "print(\"pz Shape\", reco_pz_np.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da2c45e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reco_particles = []   # TODO:Change this to reco_jets\n",
    "for ip in range(E_np.shape[0]):\n",
    "    for ix in range(E_np.shape[1]):\n",
    "        px_value = float(reco_px_np[ip, ix])\n",
    "        py_value = float(reco_py_np[ip, ix])\n",
    "        pz_value = float(reco_pz_np[ip, ix])\n",
    "        E_value = float(E_np[ip, ix])\n",
    "        particle = fj.PseudoJet(px_value, py_value, pz_value, E_value)\n",
    "        reco_particles.append(particle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dc7e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reco_particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91fa6295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "num_threads = 2 \n",
    "\n",
    "# Function to perform jet clustering\n",
    "def cluster_jets(particles):\n",
    "    jetdef = fj.JetDefinition(fj.kt_algorithm, 0.4)\n",
    "#     jet_ptcut = 20\n",
    "    \n",
    "    cluster = fj.ClusterSequence(reco_particles, jetdef)\n",
    "    jets = cluster.inclusive_jets()\n",
    "    \n",
    "    return jets\n",
    "\n",
    "# Spliting particles list into chunks to process in parallel\n",
    "chunks = [particles[i::num_threads] for i in range(num_threads)]\n",
    "\n",
    "# jet clustering in parallel\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    futures = [executor.submit(cluster_jets, chunk) for chunk in chunks]\n",
    "\n",
    "reco_jets = []\n",
    "for future in futures:\n",
    "    reco_jets.extend(future.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44dfe7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jet 1 : 300.0443786682732 -0.07680514858827191 3.3264335864126062 178.89937629550695\n",
      "Jet 2 : 222.20918101299756 -0.1470556961564419 2.4753246878745903 158.9480615258217\n",
      "Jet 3 : 125.55767997928638 -0.4368473099946482 1.304198634068763 112.39688438177109\n",
      "Jet 4 : 123.22569962219377 -0.9481103182382137 5.159308632983667 143.21939274668694\n",
      "Jet 5 : 91.91404258104431 0.19410720032506176 4.015080902674011 82.22119645774364\n",
      "Jet 6 : 85.64255811919502 0.10299389329115462 5.439456108591559 64.90952751040459\n",
      "Jet 7 : 79.02298382267034 1.016008726248612 3.9238882502678027 61.098017290234566\n",
      "Jet 8 : 78.31589790001954 -0.0384854739955826 3.5694733015856808 26.981679514050484\n",
      "Jet 9 : 68.1913469902435 -0.852725034789955 2.2969349722636463 88.16780805587769\n",
      "Jet 10 : 60.70152055931527 -0.27922158179920553 0.9146087573068801 1.573972463607788\n",
      "Jet 11 : 60.46928953760651 -0.8349615750064344 3.0202978875821502 74.79603409767151\n",
      "Jet 12 : 56.423886589632495 -0.12533308011225433 0.3893149460565359 42.54418283700943\n",
      "Jet 13 : 46.58084055382665 -0.282975704917431 0.5397013245806991 47.838824927806854\n",
      "Jet 14 : 46.46124471002878 1.1388792377062724 2.656159178279144 81.02297538518906\n",
      "Jet 15 : 41.16910992478955 0.19493322402786467 4.4032845252045405 16.44643858075142\n",
      "Jet 16 : 39.10506101083583 0.6393988487663842 4.841447856656068 53.575881481170654\n",
      "Jet 17 : 36.47180390116792 0.9975677266588164 5.163937071887002 56.80807614326477\n",
      "Jet 18 : 36.37010923990018 -1.34932671170068 5.340536231516677 69.35115820169449\n",
      "Jet 19 : 35.44124754989516 -0.14952793433598965 0.8889564092763744 24.864668995141983\n",
      "Jet 20 : 32.64286759168401 -0.2431164661067115 2.9497701146687545 0.0\n",
      "Jet 21 : 31.483906159743338 0.39844882270372056 4.479773424860516 42.3736756592989\n",
      "Jet 22 : 26.49536721206364 0.6008151887053825 0.5498119709646131 32.02804673463106\n",
      "Jet 23 : 26.439381021611815 0.31168200936332113 6.100076364412716 282.79235631227493\n",
      "Jet 24 : 21.86185722815568 1.6883780884543875 3.6184467758228003 55.84342169761658\n",
      "Jet 25 : 19.95104790692272 -0.5519131096941027 3.147469478866196 0.0\n",
      "Jet 26 : 18.65670352633628 -1.5094746872786202 4.581640420849397 29.47779443860054\n",
      "Jet 27 : 17.550774655776376 -0.5883113937964205 1.9712336992839472 18.051491022109985\n",
      "Jet 28 : 17.3377123780078 0.005699309001019831 3.7916860617071935 8.989815473556519\n",
      "Jet 29 : 17.01985645682168 1.3225023181159503 3.907266702615776 32.75563961267471\n",
      "Jet 30 : 16.6862414378037 -1.5667599360059588 6.202596940737393 38.68908631801605\n",
      "Jet 31 : 16.557208399905083 0.8713423565771857 1.4684162476148608 21.999448895454407\n",
      "Jet 32 : 16.316888297252145 1.309668664699206 4.610876536172379 22.46303728222847\n",
      "Jet 33 : 15.70981424412329 -1.1080069426826247 3.977052456877059 21.611653327941895\n",
      "Jet 34 : 13.983266694244003 -0.810648300187146 0.5545731588669544 16.416304886341095\n",
      "Jet 35 : 13.005725098417052 0.9547169843583436 5.987407362661132 9.352128565311432\n",
      "Jet 36 : 12.897213228047464 -0.5351707232450696 6.1600637185287805 13.338355273008347\n",
      "Jet 37 : 8.532314166082521 1.553580262562049 5.096335137179304 18.71728989481926\n",
      "Jet 38 : 7.75845214011108 0.016215989741404778 1.3480645184362139 0.0\n",
      "Jet 39 : 7.622584109361332 -0.03305700860024391 3.303511755637851 0.7702242843806744\n",
      "Jet 40 : 7.102937019555338 2.0147450079695908 0.06953621337542412 30.176403626799583\n",
      "Jet 41 : 5.653726561521143 0.032176518923835565 4.424600041268105 0.0\n",
      "Jet 42 : 5.246661876634955 0.009690885804638482 2.759688453345826 0.0\n",
      "Jet 43 : 5.066021296437965 -0.029956434063949766 1.2985733316564911 0.0\n",
      "Jet 44 : 4.479077589002345 -0.022815351921117746 1.4638040410283455 0.7035882472991943\n",
      "Jet 45 : 4.376788340999267 -0.0041796686265957545 2.839486502601709 0.0\n",
      "Jet 46 : 4.335582315909892 -0.1539767306983532 0.9960276809850221 0.0\n",
      "Jet 47 : 4.271417515013848 0.13077906690130023 3.297687469604559 0.0\n",
      "Jet 48 : 3.635800763340221 0.3317758100127404 0.7806779705048363 0.0\n",
      "Jet 49 : 3.503117532688864 1.7322880226575335 4.062280508754882 9.024858474731445\n",
      "Jet 50 : 3.0951619723611743 -0.06629820234092682 3.984768335339611 0.0\n",
      "Jet 51 : 3.0451915648226326 -0.09397893099661121 5.6407954411581205 0.2779206335544586\n",
      "Jet 52 : 2.7130238395268123 -0.10749370766434217 2.173620582579254 0.12510856986045837\n",
      "Jet 53 : 2.5934445592701483 2.2101519853269296 4.177765011204597 10.10023283958435\n",
      "Jet 54 : 2.410907229725535 -0.08673524703426173 2.567410932139165 0.0\n",
      "Jet 55 : 1.9625344967958558 -0.0008036758223980419 5.330689736334479 0.0\n",
      "Jet 56 : 1.9100023318139223 2.5766022483481588 5.966407789406046 7.3549346923828125\n",
      "Jet 57 : 1.514454633106741 -0.012706521805531817 4.313197073320856 0.0\n",
      "Jet 58 : 1.479027825111195 0.03971584811390021 2.1298769178439234 0.0\n",
      "Jet 59 : 1.421956406381842 -1.7668462172179171 0.5572898350329026 3.4579540491104126\n",
      "Jet 60 : 1.4042001081521813 -0.09672947664625292 1.2326917582826722 0.0\n",
      "Jet 61 : 1.3743283551676397 -2.4612141954276 0.4355765287450909 5.7215107679367065\n",
      "Jet 62 : 1.316107538182475 0.0005922405273548058 2.1021365598239554 0.0\n",
      "Jet 63 : 1.3152622442390485 0.12580289199961822 6.057823611755636 0.0\n",
      "Jet 64 : 0.8140545421810638 -2.703433726142336 1.5332201654958009 6.031766414642334\n",
      "Jet 65 : 0.685395209915844 -2.2415604252426027 1.4576626035657103 2.279679298400879\n",
      "Jet 66 : 0.4303185447242848 0.01035514757778029 0.9789677232179567 0.0\n",
      "Jet 67 : 0.2900174289630765 -3.406787800972981 5.900288045735893 1.1533018946647644\n",
      "Jet 68 : 0.15430459470035837 0.17620859315841522 3.956123060064472 0.0\n",
      "Jet 69 : 0.15227572177993154 -0.09028251956030511 0.13993299425413333 0.0\n",
      "Jet 70 : 0.0 100000.0 0.0 0.0\n",
      "Jet 71 : 300.0443786682732 -0.07680514858827191 3.3264335864126062 178.89937629550695\n",
      "Jet 72 : 222.20918101299756 -0.1470556961564419 2.4753246878745903 158.9480615258217\n",
      "Jet 73 : 125.55767997928638 -0.4368473099946482 1.304198634068763 112.39688438177109\n",
      "Jet 74 : 123.22569962219377 -0.9481103182382137 5.159308632983667 143.21939274668694\n",
      "Jet 75 : 91.91404258104431 0.19410720032506176 4.015080902674011 82.22119645774364\n",
      "Jet 76 : 85.64255811919502 0.10299389329115462 5.439456108591559 64.90952751040459\n",
      "Jet 77 : 79.02298382267034 1.016008726248612 3.9238882502678027 61.098017290234566\n",
      "Jet 78 : 78.31589790001954 -0.0384854739955826 3.5694733015856808 26.981679514050484\n",
      "Jet 79 : 68.1913469902435 -0.852725034789955 2.2969349722636463 88.16780805587769\n",
      "Jet 80 : 60.70152055931527 -0.27922158179920553 0.9146087573068801 1.573972463607788\n",
      "Jet 81 : 60.46928953760651 -0.8349615750064344 3.0202978875821502 74.79603409767151\n",
      "Jet 82 : 56.423886589632495 -0.12533308011225433 0.3893149460565359 42.54418283700943\n",
      "Jet 83 : 46.58084055382665 -0.282975704917431 0.5397013245806991 47.838824927806854\n",
      "Jet 84 : 46.46124471002878 1.1388792377062724 2.656159178279144 81.02297538518906\n",
      "Jet 85 : 41.16910992478955 0.19493322402786467 4.4032845252045405 16.44643858075142\n",
      "Jet 86 : 39.10506101083583 0.6393988487663842 4.841447856656068 53.575881481170654\n",
      "Jet 87 : 36.47180390116792 0.9975677266588164 5.163937071887002 56.80807614326477\n",
      "Jet 88 : 36.37010923990018 -1.34932671170068 5.340536231516677 69.35115820169449\n",
      "Jet 89 : 35.44124754989516 -0.14952793433598965 0.8889564092763744 24.864668995141983\n",
      "Jet 90 : 32.64286759168401 -0.2431164661067115 2.9497701146687545 0.0\n",
      "Jet 91 : 31.483906159743338 0.39844882270372056 4.479773424860516 42.3736756592989\n",
      "Jet 92 : 26.49536721206364 0.6008151887053825 0.5498119709646131 32.02804673463106\n",
      "Jet 93 : 26.439381021611815 0.31168200936332113 6.100076364412716 282.79235631227493\n",
      "Jet 94 : 21.86185722815568 1.6883780884543875 3.6184467758228003 55.84342169761658\n",
      "Jet 95 : 19.95104790692272 -0.5519131096941027 3.147469478866196 0.0\n",
      "Jet 96 : 18.65670352633628 -1.5094746872786202 4.581640420849397 29.47779443860054\n",
      "Jet 97 : 17.550774655776376 -0.5883113937964205 1.9712336992839472 18.051491022109985\n",
      "Jet 98 : 17.3377123780078 0.005699309001019831 3.7916860617071935 8.989815473556519\n",
      "Jet 99 : 17.01985645682168 1.3225023181159503 3.907266702615776 32.75563961267471\n",
      "Jet 100 : 16.6862414378037 -1.5667599360059588 6.202596940737393 38.68908631801605\n",
      "Jet 101 : 16.557208399905083 0.8713423565771857 1.4684162476148608 21.999448895454407\n",
      "Jet 102 : 16.316888297252145 1.309668664699206 4.610876536172379 22.46303728222847\n",
      "Jet 103 : 15.70981424412329 -1.1080069426826247 3.977052456877059 21.611653327941895\n",
      "Jet 104 : 13.983266694244003 -0.810648300187146 0.5545731588669544 16.416304886341095\n",
      "Jet 105 : 13.005725098417052 0.9547169843583436 5.987407362661132 9.352128565311432\n",
      "Jet 106 : 12.897213228047464 -0.5351707232450696 6.1600637185287805 13.338355273008347\n",
      "Jet 107 : 8.532314166082521 1.553580262562049 5.096335137179304 18.71728989481926\n",
      "Jet 108 : 7.75845214011108 0.016215989741404778 1.3480645184362139 0.0\n",
      "Jet 109 : 7.622584109361332 -0.03305700860024391 3.303511755637851 0.7702242843806744\n",
      "Jet 110 : 7.102937019555338 2.0147450079695908 0.06953621337542412 30.176403626799583\n",
      "Jet 111 : 5.653726561521143 0.032176518923835565 4.424600041268105 0.0\n",
      "Jet 112 : 5.246661876634955 0.009690885804638482 2.759688453345826 0.0\n",
      "Jet 113 : 5.066021296437965 -0.029956434063949766 1.2985733316564911 0.0\n",
      "Jet 114 : 4.479077589002345 -0.022815351921117746 1.4638040410283455 0.7035882472991943\n",
      "Jet 115 : 4.376788340999267 -0.0041796686265957545 2.839486502601709 0.0\n",
      "Jet 116 : 4.335582315909892 -0.1539767306983532 0.9960276809850221 0.0\n",
      "Jet 117 : 4.271417515013848 0.13077906690130023 3.297687469604559 0.0\n",
      "Jet 118 : 3.635800763340221 0.3317758100127404 0.7806779705048363 0.0\n",
      "Jet 119 : 3.503117532688864 1.7322880226575335 4.062280508754882 9.024858474731445\n",
      "Jet 120 : 3.0951619723611743 -0.06629820234092682 3.984768335339611 0.0\n",
      "Jet 121 : 3.0451915648226326 -0.09397893099661121 5.6407954411581205 0.2779206335544586\n",
      "Jet 122 : 2.7130238395268123 -0.10749370766434217 2.173620582579254 0.12510856986045837\n",
      "Jet 123 : 2.5934445592701483 2.2101519853269296 4.177765011204597 10.10023283958435\n",
      "Jet 124 : 2.410907229725535 -0.08673524703426173 2.567410932139165 0.0\n",
      "Jet 125 : 1.9625344967958558 -0.0008036758223980419 5.330689736334479 0.0\n",
      "Jet 126 : 1.9100023318139223 2.5766022483481588 5.966407789406046 7.3549346923828125\n",
      "Jet 127 : 1.514454633106741 -0.012706521805531817 4.313197073320856 0.0\n",
      "Jet 128 : 1.479027825111195 0.03971584811390021 2.1298769178439234 0.0\n",
      "Jet 129 : 1.421956406381842 -1.7668462172179171 0.5572898350329026 3.4579540491104126\n",
      "Jet 130 : 1.4042001081521813 -0.09672947664625292 1.2326917582826722 0.0\n",
      "Jet 131 : 1.3743283551676397 -2.4612141954276 0.4355765287450909 5.7215107679367065\n",
      "Jet 132 : 1.316107538182475 0.0005922405273548058 2.1021365598239554 0.0\n",
      "Jet 133 : 1.3152622442390485 0.12580289199961822 6.057823611755636 0.0\n",
      "Jet 134 : 0.8140545421810638 -2.703433726142336 1.5332201654958009 6.031766414642334\n",
      "Jet 135 : 0.685395209915844 -2.2415604252426027 1.4576626035657103 2.279679298400879\n",
      "Jet 136 : 0.4303185447242848 0.01035514757778029 0.9789677232179567 0.0\n",
      "Jet 137 : 0.2900174289630765 -3.406787800972981 5.900288045735893 1.1533018946647644\n",
      "Jet 138 : 0.15430459470035837 0.17620859315841522 3.956123060064472 0.0\n",
      "Jet 139 : 0.15227572177993154 -0.09028251956030511 0.13993299425413333 0.0\n",
      "Jet 140 : 0.0 100000.0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "for i, jet in enumerate(reco_jets):\n",
    "    print(\"Jet\", i+1, \":\", jet.pt(), jet.eta(), jet.phi(), jet.e())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84b009c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Jets: 140\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Jets:\", len(reco_jets))\n",
    "\n",
    "\n",
    "reco_jet_pt = [jet.pt() for jet in reco_jets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49736597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TCanvas::Constructor>: Deleting canvas with same name: canvas\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ROOT\n",
    "import numpy as np\n",
    "\n",
    "# Define bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100, 200]\n",
    "\n",
    "# Initialize lists to store values\n",
    "x_vals = []\n",
    "ratio_iqr_median = []\n",
    "\n",
    "# Iterate over bins\n",
    "for i in range(len(bins) - 1):\n",
    "    lim_low = bins[i]\n",
    "    lim_hi = bins[i + 1]\n",
    "    x_vals.append(np.mean([lim_low, lim_hi]))\n",
    "\n",
    "    # Filter gen_jet pT values within the bin\n",
    "    gen_jet_values_in_bin = [pt for pt in gen_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Filter reco_jet pT values within the bin\n",
    "    reco_jet_values_in_bin = [pt for pt in reco_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Check if there are any values in both gen_jet_pt and reco_jet_pt\n",
    "    if len(gen_jet_values_in_bin) == 0 or len(reco_jet_values_in_bin) == 0:\n",
    "        # Append NaN values\n",
    "        ratio_iqr_median.append(np.nan)\n",
    "        continue\n",
    "\n",
    "    # Calculate IQR and median for reco_jet_pt / gen_jet_pt values\n",
    "    ratio_values_in_bin = [reco / gen for reco, gen in zip(reco_jet_values_in_bin, gen_jet_values_in_bin)]\n",
    "    ratio_iqr = np.percentile(ratio_values_in_bin, 75) - np.percentile(ratio_values_in_bin, 25)\n",
    "    ratio_median = np.median(ratio_values_in_bin)\n",
    "    ratio_iqr_median_ratio = ratio_iqr / ratio_median\n",
    "    ratio_iqr_median.append(ratio_iqr_median_ratio)\n",
    "\n",
    "# Filter out NaN values\n",
    "x_vals_filtered = [x for x, y in zip(x_vals, ratio_iqr_median) if not np.isnan(y)]\n",
    "ratio_iqr_median_filtered = [y for y in ratio_iqr_median if not np.isnan(y)]\n",
    "\n",
    "# Create a TGraph with filtered values\n",
    "gr_ratio = ROOT.TGraph(len(x_vals_filtered), np.array(x_vals_filtered), np.array(ratio_iqr_median_filtered))\n",
    "\n",
    "# Set the titles to empty strings\n",
    "gr_ratio.SetTitle(\"\")\n",
    "\n",
    "# Create a canvas\n",
    "canvas = ROOT.TCanvas(\"canvas\", \"Ratio IQR/Median vs Jet pT\", 800, 600)\n",
    "\n",
    "# Draw the graph with connecting lines\n",
    "gr_ratio.SetMarkerStyle(20)\n",
    "gr_ratio.SetMarkerColor(ROOT.kBlue)\n",
    "gr_ratio.SetLineColor(ROOT.kBlue)\n",
    "gr_ratio.GetXaxis().SetTitle(\"Jet PT, Gen (GeV)\")\n",
    "gr_ratio.GetYaxis().SetTitle(\"Response IQR/Median\")\n",
    "gr_ratio.Draw(\"APL\")\n",
    "\n",
    "# Get the X and Y axes\n",
    "xaxis = gr_ratio.GetXaxis()\n",
    "yaxis = gr_ratio.GetYaxis()\n",
    "\n",
    "# Set tick length for all four axes\n",
    "xaxis.SetTickLength(0.03)\n",
    "yaxis.SetTickLength(0.03)\n",
    "\n",
    "\n",
    "# Add legend\n",
    "legend = ROOT.TLegend(0.75, 0.75, 0.9, 0.9)\n",
    "legend.AddEntry(gr_ratio, \"Pred FP\", \"p\")\n",
    "legend.Draw()\n",
    "\n",
    "# Show the canvas\n",
    "canvas.Draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b80ef895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin 1: Number of events (Gen Jet): 9, Number of events (Reco Jet): 24\n",
      "Bin 2: Number of events (Gen Jet): 10, Number of events (Reco Jet): 6\n",
      "Bin 3: Number of events (Gen Jet): 9, Number of events (Reco Jet): 12\n",
      "Bin 4: Number of events (Gen Jet): 4, Number of events (Reco Jet): 8\n",
      "Bin 5: Number of events (Gen Jet): 5, Number of events (Reco Jet): 10\n",
      "Bin 6: Number of events (Gen Jet): 2, Number of events (Reco Jet): 4\n",
      "Bin 7: Number of events (Gen Jet): 0, Number of events (Reco Jet): 4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100, 200]\n",
    "\n",
    "# Iterate over bins\n",
    "for i in range(len(bins) - 1):\n",
    "    lim_low = bins[i]\n",
    "    lim_hi = bins[i + 1]\n",
    "\n",
    "    # Filter gen_jet pT values within the bin\n",
    "    gen_jet_values_in_bin = [pt for pt in gen_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Filter reco_jet pT values within the bin\n",
    "    reco_jet_values_in_bin = [pt for pt in reco_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Calculate the number of events in the bin\n",
    "    num_events_gen_jet = len(gen_jet_values_in_bin)\n",
    "    num_events_reco_jet = len(reco_jet_values_in_bin)\n",
    "\n",
    "    print(f\"Bin {i+1}: Number of events (Gen Jet): {num_events_gen_jet}, Number of events (Reco Jet): {num_events_reco_jet}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203a0d91",
   "metadata": {},
   "source": [
    "# Quantization INT8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d2e0d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/ao/quantization/observer.py:1207: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.qconfig = torch.ao.quantization.get_default_qconfig('onednn')\n",
    "custom_module_config = {\n",
    "        \"float_to_observed_custom_module_class\": {torch.nn.MultiheadAttention: QuantizeableMultiheadAttention},\n",
    "        \"observed_to_quantized_custom_module_class\": {QuantizeableMultiheadAttention: QuantizedMultiheadAttention},\n",
    "}\n",
    "\n",
    "model_prepared = torch.ao.quantization.prepare(model, prepare_custom_config_dict=custom_module_config)\n",
    "\n",
    "#calibrate on data\n",
    "num_events_to_calibrate = 100\n",
    "for ind in range(max_events_train,max_events_train+num_events_to_calibrate):\n",
    "    _X = torch.unsqueeze(torch.tensor(ds_train[ind][\"X\"]).to(torch.float32), 0)\n",
    "    _mask = _X[:, :, 0]!=0\n",
    "    model_prepared(_X, _mask)\n",
    "\n",
    "model_int8 = torch.ao.quantization.convert(model_prepared,convert_custom_config_dict=custom_module_config,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12a705b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizeFeaturesStub(\n",
       "  (quants): ModuleList(\n",
       "    (0): Quantize(scale=tensor([0.0078]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "    (1): Quantize(scale=tensor([0.0365]), zero_point=tensor([127]), dtype=torch.quint8)\n",
       "    (2): Quantize(scale=tensor([0.0358]), zero_point=tensor([117]), dtype=torch.quint8)\n",
       "    (3): Quantize(scale=tensor([0.0078]), zero_point=tensor([128]), dtype=torch.quint8)\n",
       "    (4): Quantize(scale=tensor([0.0078]), zero_point=tensor([127]), dtype=torch.quint8)\n",
       "    (5): Quantize(scale=tensor([0.0311]), zero_point=tensor([102]), dtype=torch.quint8)\n",
       "    (6): Quantize(scale=tensor([23.6720]), zero_point=tensor([121]), dtype=torch.quint8)\n",
       "    (7): Quantize(scale=tensor([24.5268]), zero_point=tensor([126]), dtype=torch.quint8)\n",
       "    (8): Quantize(scale=tensor([31.3419]), zero_point=tensor([128]), dtype=torch.quint8)\n",
       "    (9): Quantize(scale=tensor([0.0122]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "    (10): Quantize(scale=tensor([3.2046]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "    (11): Quantize(scale=tensor([0.2198]), zero_point=tensor([44]), dtype=torch.quint8)\n",
       "    (12): Quantize(scale=tensor([0.4827]), zero_point=tensor([176]), dtype=torch.quint8)\n",
       "    (13): Quantize(scale=tensor([6.6886]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "    (14): Quantize(scale=tensor([7.5963]), zero_point=tensor([110]), dtype=torch.quint8)\n",
       "    (15): Quantize(scale=tensor([4.3417]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "    (16): Quantize(scale=tensor([3.4612]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "    (17-19): 3 x Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_int8.quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "999e620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_quantized = torch.quantize_per_tensor((X_features_padded[:, :, 0]!=0).to(torch.float32), 1, 0, torch.quint8)\n",
    "preds = model_int8(X_features_padded, mask_quantized)\n",
    "preds = preds[0].detach(), preds[1].detach()\n",
    "preds_unpacked_int8 = unpack_predictions(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "272d479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_int8 = mlpf_loss(targets_unpacked, preds_unpacked_int8, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d010742f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Final total loss')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(2), [loss[\"Total\"].detach().numpy(), loss_int8[\"Total\"].detach().numpy()])\n",
    "plt.xticks(range(2), [\"fp32\", \"int8\"])\n",
    "plt.ylabel(\"Final total loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2214072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_pred_int8 = preds_unpacked_int8[\"pt\"][msk_true_particles].numpy()\n",
    "eta_pred_int8 = preds_unpacked_int8[\"eta\"][msk_true_particles].numpy()\n",
    "sphi_pred_int8 = preds_unpacked_int8[\"sin_phi\"][msk_true_particles].numpy()\n",
    "cphi_pred_int8 = preds_unpacked_int8[\"cos_phi\"][msk_true_particles].numpy()\n",
    "energy_pred_int8 = preds_unpacked_int8[\"energy\"][msk_true_particles].numpy()\n",
    "\n",
    "px = preds_unpacked_int8[\"pt\"] * preds_unpacked_int8[\"cos_phi\"] * msk_true_particles\n",
    "py = preds_unpacked_int8[\"pt\"] * preds_unpacked_int8[\"sin_phi\"] * msk_true_particles\n",
    "pred_met_int8 = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07044dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f463c7343a0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(pred_met/true_met, bins=np.linspace(0,5,61), histtype=\"step\", lw=2, label=\"fp32\");\n",
    "plt.hist(pred_met_int8/true_met, bins=np.linspace(0,5,61), histtype=\"step\", lw=2, label=\"int8\");\n",
    "plt.xlabel(\"reco_met / gen_met\")\n",
    "plt.ylabel(\"number of events / bin\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b3255af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the 3-momentum for the quantized particles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7fe474c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "px_int8 = preds_unpacked_int8[\"pt\"] * preds_unpacked_int8[\"cos_phi\"] * msk_true_particles\n",
    "py_int8 = preds_unpacked_int8[\"pt\"] * preds_unpacked_int8[\"sin_phi\"] * msk_true_particles\n",
    "pz_int8 = preds_unpacked_int8[\"pt\"] * np.sinh(preds_unpacked_int8[\"eta\"]) * msk_true_particles\n",
    "phi_int8 = np.arctan2(preds_unpacked_int8[\"sin_phi\"], preds_unpacked_int8[\"cos_phi\"]) * msk_true_particles\n",
    "\n",
    "px_np_int8 = px_int8.detach().cpu().numpy()\n",
    "py_np_int8 = py_int8.detach().cpu().numpy()\n",
    "pz_np_int8 = pz_int8.detach().cpu().numpy()\n",
    "phi_np_int8 = phi_int8.detach().cpu().numpy()\n",
    "\n",
    "# print(\"px_np\", px_np)\n",
    "# print(\"py_np\", py_np)\n",
    "# print(\"pz_np\", pz_np)\n",
    "# print(\"phi_np\", phi_np)\n",
    "\n",
    "quantized_mom = np.sqrt(np.sum(px_np_int8, axis=1)**2 + np.sum(py_np_int8, axis=1)**2 + np.sum(pz_np_int8, axis=1)**2)\n",
    "int8_E_np = np.sqrt(px_np_int8**2 + py_np_int8**2 + pz_np_int8**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f59b8cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int8_E Shape (8, 140)\n",
      "px Shape (8, 140)\n",
      "py Shape (8, 140)\n",
      "pz Shape (8, 140)\n"
     ]
    }
   ],
   "source": [
    "# four-vectors\n",
    "# px_py_pz_E = np.column_stack((px_np, py_np, pz_np, E)) \n",
    "print(\"int8_E Shape\", int8_E_np.shape)\n",
    "print(\"px Shape\", px_np_int8.shape)\n",
    "print(\"py Shape\", py_np_int8.shape)\n",
    "print(\"pz Shape\", pz_np_int8.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cbb0d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INT8_Jets_particles = []   # TODO:Change this to reco_jets\n",
    "for ip in range(int8_E_np.shape[0]):\n",
    "    for ix in range(int8_E_np.shape[1]):\n",
    "        px_value = float(px_np_int8[ip, ix])\n",
    "        py_value = float(py_np_int8[ip, ix])\n",
    "        pz_value = float(pz_np_int8[ip, ix])\n",
    "        E_value = float(int8_E_np[ip, ix])\n",
    "        particle = fj.PseudoJet(px_value, py_value, pz_value, E_value)\n",
    "        INT8_Jets_particles.append(particle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "983511e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INT8_Jets_particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1602ea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "num_threads = 2 \n",
    "\n",
    "def cluster_jets(particles):\n",
    "    jetdef = fj.JetDefinition(fj.kt_algorithm, 0.4)\n",
    "#     jet_ptcut = 20\n",
    "    \n",
    "    cluster = fj.ClusterSequence(INT8_Jets_particles, jetdef)\n",
    "    jets = cluster.inclusive_jets()\n",
    "    \n",
    "    return jets\n",
    "\n",
    "chunks = [particles[i::num_threads] for i in range(num_threads)]\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    futures = [executor.submit(cluster_jets, chunk) for chunk in chunks]\n",
    "\n",
    "INT8_reco_jets = []\n",
    "for future in futures:\n",
    "    INT8_reco_jets.extend(future.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39c90542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jet 1 : 164.42952213618435 0.3926765121449177 4.199457232023997 180.62611862272024\n",
      "Jet 2 : 153.18492336431248 0.4511340057079356 5.502643815807107 170.650888890028\n",
      "Jet 3 : 146.0562941312004 0.0020195846297116067 2.2815417072040787 149.82266646623611\n",
      "Jet 4 : 81.4373191610412 -0.6045800523648036 1.4076930270458103 97.04933765530586\n",
      "Jet 5 : 71.14595457676722 -0.2084446702300491 3.120308478670253 73.40654351562262\n",
      "Jet 6 : 70.43214859365756 -0.83032509158901 3.0327009814159642 97.31454293429852\n",
      "Jet 7 : 68.23551101063 -0.8228210120575103 5.1347252197001705 94.06371347606182\n",
      "Jet 8 : 67.18570430055726 -0.8253791453077005 3.465155770630184 92.10354053974152\n",
      "Jet 9 : 65.50210468729698 -0.6464460351076992 0.9588064476795798 81.24768215604126\n",
      "Jet 10 : 60.632488034850546 -0.3088427519954824 0.45179842220362487 64.47542501613498\n",
      "Jet 11 : 60.478163801542266 -0.9053255105139616 2.412686387594787 87.1069940328598\n",
      "Jet 12 : 58.28583337266997 0.16779204026011488 6.055010514626902 60.63845878839493\n",
      "Jet 13 : 49.924264956888216 -1.318204990941941 5.304119582326181 100.20164474844933\n",
      "Jet 14 : 49.636183083721804 0.5701225893094115 4.76747930109579 58.49178984761238\n",
      "Jet 15 : 49.570828358931074 -0.23823560204836627 3.692353525360984 52.396077973768115\n",
      "Jet 16 : 46.460247872087514 1.1155303878230658 2.675124962613099 79.04676359891891\n",
      "Jet 17 : 41.69938773314605 0.9617268841477564 5.030224505527934 62.87959402799606\n",
      "Jet 18 : 36.675021405720095 0.5896957387197331 0.4461445430031345 43.729505591094494\n",
      "Jet 19 : 29.645802313307804 1.0646386570619162 4.040470276146598 48.58762073516846\n",
      "Jet 20 : 27.470432528500247 0.9548582054304016 1.3716019224567966 41.41062917315867\n",
      "Jet 21 : 22.732497654594965 -1.3855445419591923 6.117380955673536 48.502828016877174\n",
      "Jet 22 : 22.32426962395093 1.1320465008763185 4.520436103794432 38.35252618789673\n",
      "Jet 23 : 18.381663492878182 -0.9510334831074249 3.986739749981208 27.6240536570549\n",
      "Jet 24 : 16.725506024172443 -0.5221279631091045 1.9908109298339063 19.268824100494385\n",
      "Jet 25 : 15.336950019952047 -1.4079484678902934 4.568646372744365 33.405126214027405\n",
      "Jet 26 : 14.135778810726089 -0.8679845519464092 0.39769999977412596 19.882785595953465\n",
      "Jet 27 : 12.624841209363987 1.6405286265079753 0.12368954615600648 33.90884831547737\n",
      "Jet 28 : 12.025339107398132 2.002859854144506 3.5420636534577685 45.39374566078186\n",
      "Jet 29 : 12.005983421250958 0.31534707802261325 2.7847879041450283 12.641893863677979\n",
      "Jet 30 : 11.907921496703322 -0.7172517548008206 6.16152760666095 15.28988429903984\n",
      "Jet 31 : 8.70063684152442 0.5697299061476695 2.4748567594599797 10.183793008327484\n",
      "Jet 32 : 8.56810892901903 -1.1221784893437372 1.9815813671476696 14.563021302223206\n",
      "Jet 33 : 6.5269842637887 -0.3191934123418245 4.344839125563258 6.956683933734894\n",
      "Jet 34 : 4.489211222452477 -0.15521626833271146 1.3885483323741794 4.546754285693169\n",
      "Jet 35 : 4.007514259953989 0.04030930495954473 4.9905257192996615 4.0322509706020355\n",
      "Jet 36 : 3.6302814451946275 -1.3516734534663766 0.6679245460223443 7.51518389582634\n",
      "Jet 37 : 3.310187365119269 -1.8132284862530175 5.566187582154702 10.416056632995605\n",
      "Jet 38 : 3.166839692529695 0.9244072153319536 3.4052858522006906 4.6295710653066635\n",
      "Jet 39 : 2.75333582786901 -2.24250688996573 4.557693030238729 13.111191272735596\n",
      "Jet 40 : 2.6046470545494116 -1.604174151058623 1.9815874425320108 6.751433372497559\n",
      "Jet 41 : 2.1475739858139025 -1.838093585098995 1.0944632609719769 6.919125080108643\n",
      "Jet 42 : 1.9392244491507618 0.31475244403239333 1.311340813820907 2.036078691482544\n",
      "Jet 43 : 1.931141517409835 2.034526377010767 4.581769435477118 7.528860539197922\n",
      "Jet 44 : 1.6691306365663554 2.579933230024985 5.423483774149512 11.076380729675293\n",
      "Jet 45 : 1.6503837547042421 -2.038172765274482 1.5396318876453792 6.446933671832085\n",
      "Jet 46 : 1.5811469763807213 1.4669277047167704 5.5310712894775085 3.6217834651470184\n",
      "Jet 47 : 1.5141380661122135 -0.7281020552615434 5.7501954816582135 1.9445430636405945\n",
      "Jet 48 : 1.5033732480231174 -1.8553057642545707 5.122372608916256 4.93131198361516\n",
      "Jet 49 : 1.2697420318857504 1.8595920300915456 2.591175002866359 4.179711163043976\n",
      "Jet 50 : 1.1422883768203307 1.4936227867922311 3.3596622555059135 2.674936354160309\n",
      "Jet 51 : 1.1315161171336032 1.743517028748977 0.5097131631567887 3.333627700805664\n",
      "Jet 52 : 1.0116331555650098 -1.6648447416576981 2.9099982401055198 2.7699373960494995\n",
      "Jet 53 : 0.9856510312019643 -2.4974325074423676 1.5447469769468343 6.029006481170654\n",
      "Jet 54 : 0.8506739758906902 -1.6159696129926835 2.4493998592845156 2.227493464946747\n",
      "Jet 55 : 0.7979977735342544 1.7398943039432297 5.144666639336189 2.343224883079529\n",
      "Jet 56 : 0.74821500553571 -2.152086769843546 2.8782872049350097 3.2618558406829834\n",
      "Jet 57 : 0.6052136555215928 -1.8570356194098687 0.6097045384400315 1.9853602647781372\n",
      "Jet 58 : 0.5517584474340269 -2.166400858454978 0.10613741113795783 2.4391605854034424\n",
      "Jet 59 : 0.5201007093483561 -1.3604695441827268 1.6227698885863722 1.0808709561824799\n",
      "Jet 60 : 0.5036622788162594 -2.0676177949600993 5.961127985623328 2.0228219032287598\n",
      "Jet 61 : 0.4667619897045285 1.7590969623798296 1.585172551500231 1.3954763412475586\n",
      "Jet 62 : 0.460113336185665 -3.1306800765714855 1.4986106738144662 5.275942325592041\n",
      "Jet 63 : 0.29888359637287343 -3.1254794742816268 1.9404686368293271 3.4094738960266113\n",
      "Jet 64 : 0.29840362465159104 -2.3649435430913957 0.6292256147417644 1.6020383834838867\n",
      "Jet 65 : 0.2831854115733277 2.981169665204294 4.550361451268685 2.7980971336364746\n",
      "Jet 66 : 0.15362926600504487 -2.9590000125816873 4.052816002210134 1.4848694801330566\n",
      "Jet 67 : 0.1153894116661996 -2.506289628147729 6.149544599606768 0.7120062708854675\n",
      "Jet 68 : 0.08944339788331965 3.856476600664238 2.9722386866726893 2.116210699081421\n",
      "Jet 69 : 0.06886226943109171 3.4522270340596224 5.907149402775147 1.08810293674469\n",
      "Jet 70 : 0.0 100000.0 0.0 0.0\n",
      "Jet 71 : 164.42952213618435 0.3926765121449177 4.199457232023997 180.62611862272024\n",
      "Jet 72 : 153.18492336431248 0.4511340057079356 5.502643815807107 170.650888890028\n",
      "Jet 73 : 146.0562941312004 0.0020195846297116067 2.2815417072040787 149.82266646623611\n",
      "Jet 74 : 81.4373191610412 -0.6045800523648036 1.4076930270458103 97.04933765530586\n",
      "Jet 75 : 71.14595457676722 -0.2084446702300491 3.120308478670253 73.40654351562262\n",
      "Jet 76 : 70.43214859365756 -0.83032509158901 3.0327009814159642 97.31454293429852\n",
      "Jet 77 : 68.23551101063 -0.8228210120575103 5.1347252197001705 94.06371347606182\n",
      "Jet 78 : 67.18570430055726 -0.8253791453077005 3.465155770630184 92.10354053974152\n",
      "Jet 79 : 65.50210468729698 -0.6464460351076992 0.9588064476795798 81.24768215604126\n",
      "Jet 80 : 60.632488034850546 -0.3088427519954824 0.45179842220362487 64.47542501613498\n",
      "Jet 81 : 60.478163801542266 -0.9053255105139616 2.412686387594787 87.1069940328598\n",
      "Jet 82 : 58.28583337266997 0.16779204026011488 6.055010514626902 60.63845878839493\n",
      "Jet 83 : 49.924264956888216 -1.318204990941941 5.304119582326181 100.20164474844933\n",
      "Jet 84 : 49.636183083721804 0.5701225893094115 4.76747930109579 58.49178984761238\n",
      "Jet 85 : 49.570828358931074 -0.23823560204836627 3.692353525360984 52.396077973768115\n",
      "Jet 86 : 46.460247872087514 1.1155303878230658 2.675124962613099 79.04676359891891\n",
      "Jet 87 : 41.69938773314605 0.9617268841477564 5.030224505527934 62.87959402799606\n",
      "Jet 88 : 36.675021405720095 0.5896957387197331 0.4461445430031345 43.729505591094494\n",
      "Jet 89 : 29.645802313307804 1.0646386570619162 4.040470276146598 48.58762073516846\n",
      "Jet 90 : 27.470432528500247 0.9548582054304016 1.3716019224567966 41.41062917315867\n",
      "Jet 91 : 22.732497654594965 -1.3855445419591923 6.117380955673536 48.502828016877174\n",
      "Jet 92 : 22.32426962395093 1.1320465008763185 4.520436103794432 38.35252618789673\n",
      "Jet 93 : 18.381663492878182 -0.9510334831074249 3.986739749981208 27.6240536570549\n",
      "Jet 94 : 16.725506024172443 -0.5221279631091045 1.9908109298339063 19.268824100494385\n",
      "Jet 95 : 15.336950019952047 -1.4079484678902934 4.568646372744365 33.405126214027405\n",
      "Jet 96 : 14.135778810726089 -0.8679845519464092 0.39769999977412596 19.882785595953465\n",
      "Jet 97 : 12.624841209363987 1.6405286265079753 0.12368954615600648 33.90884831547737\n",
      "Jet 98 : 12.025339107398132 2.002859854144506 3.5420636534577685 45.39374566078186\n",
      "Jet 99 : 12.005983421250958 0.31534707802261325 2.7847879041450283 12.641893863677979\n",
      "Jet 100 : 11.907921496703322 -0.7172517548008206 6.16152760666095 15.28988429903984\n",
      "Jet 101 : 8.70063684152442 0.5697299061476695 2.4748567594599797 10.183793008327484\n",
      "Jet 102 : 8.56810892901903 -1.1221784893437372 1.9815813671476696 14.563021302223206\n",
      "Jet 103 : 6.5269842637887 -0.3191934123418245 4.344839125563258 6.956683933734894\n",
      "Jet 104 : 4.489211222452477 -0.15521626833271146 1.3885483323741794 4.546754285693169\n",
      "Jet 105 : 4.007514259953989 0.04030930495954473 4.9905257192996615 4.0322509706020355\n",
      "Jet 106 : 3.6302814451946275 -1.3516734534663766 0.6679245460223443 7.51518389582634\n",
      "Jet 107 : 3.310187365119269 -1.8132284862530175 5.566187582154702 10.416056632995605\n",
      "Jet 108 : 3.166839692529695 0.9244072153319536 3.4052858522006906 4.6295710653066635\n",
      "Jet 109 : 2.75333582786901 -2.24250688996573 4.557693030238729 13.111191272735596\n",
      "Jet 110 : 2.6046470545494116 -1.604174151058623 1.9815874425320108 6.751433372497559\n",
      "Jet 111 : 2.1475739858139025 -1.838093585098995 1.0944632609719769 6.919125080108643\n",
      "Jet 112 : 1.9392244491507618 0.31475244403239333 1.311340813820907 2.036078691482544\n",
      "Jet 113 : 1.931141517409835 2.034526377010767 4.581769435477118 7.528860539197922\n",
      "Jet 114 : 1.6691306365663554 2.579933230024985 5.423483774149512 11.076380729675293\n",
      "Jet 115 : 1.6503837547042421 -2.038172765274482 1.5396318876453792 6.446933671832085\n",
      "Jet 116 : 1.5811469763807213 1.4669277047167704 5.5310712894775085 3.6217834651470184\n",
      "Jet 117 : 1.5141380661122135 -0.7281020552615434 5.7501954816582135 1.9445430636405945\n",
      "Jet 118 : 1.5033732480231174 -1.8553057642545707 5.122372608916256 4.93131198361516\n",
      "Jet 119 : 1.2697420318857504 1.8595920300915456 2.591175002866359 4.179711163043976\n",
      "Jet 120 : 1.1422883768203307 1.4936227867922311 3.3596622555059135 2.674936354160309\n",
      "Jet 121 : 1.1315161171336032 1.743517028748977 0.5097131631567887 3.333627700805664\n",
      "Jet 122 : 1.0116331555650098 -1.6648447416576981 2.9099982401055198 2.7699373960494995\n",
      "Jet 123 : 0.9856510312019643 -2.4974325074423676 1.5447469769468343 6.029006481170654\n",
      "Jet 124 : 0.8506739758906902 -1.6159696129926835 2.4493998592845156 2.227493464946747\n",
      "Jet 125 : 0.7979977735342544 1.7398943039432297 5.144666639336189 2.343224883079529\n",
      "Jet 126 : 0.74821500553571 -2.152086769843546 2.8782872049350097 3.2618558406829834\n",
      "Jet 127 : 0.6052136555215928 -1.8570356194098687 0.6097045384400315 1.9853602647781372\n",
      "Jet 128 : 0.5517584474340269 -2.166400858454978 0.10613741113795783 2.4391605854034424\n",
      "Jet 129 : 0.5201007093483561 -1.3604695441827268 1.6227698885863722 1.0808709561824799\n",
      "Jet 130 : 0.5036622788162594 -2.0676177949600993 5.961127985623328 2.0228219032287598\n",
      "Jet 131 : 0.4667619897045285 1.7590969623798296 1.585172551500231 1.3954763412475586\n",
      "Jet 132 : 0.460113336185665 -3.1306800765714855 1.4986106738144662 5.275942325592041\n",
      "Jet 133 : 0.29888359637287343 -3.1254794742816268 1.9404686368293271 3.4094738960266113\n",
      "Jet 134 : 0.29840362465159104 -2.3649435430913957 0.6292256147417644 1.6020383834838867\n",
      "Jet 135 : 0.2831854115733277 2.981169665204294 4.550361451268685 2.7980971336364746\n",
      "Jet 136 : 0.15362926600504487 -2.9590000125816873 4.052816002210134 1.4848694801330566\n",
      "Jet 137 : 0.1153894116661996 -2.506289628147729 6.149544599606768 0.7120062708854675\n",
      "Jet 138 : 0.08944339788331965 3.856476600664238 2.9722386866726893 2.116210699081421\n",
      "Jet 139 : 0.06886226943109171 3.4522270340596224 5.907149402775147 1.08810293674469\n",
      "Jet 140 : 0.0 100000.0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "for i, jet in enumerate(INT8_reco_jets):\n",
    "    print(\"Jet\", i+1, \":\", jet.pt(), jet.eta(), jet.phi(), jet.e())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74cd0cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Jets: 140\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Jets:\", len(INT8_reco_jets))\n",
    "\n",
    "\n",
    "INT8_jet_pt = [jet.pt() for jet in INT8_reco_jets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "125547ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TCanvas::Constructor>: Deleting canvas with same name: canvas\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ROOT\n",
    "import numpy as np\n",
    "\n",
    "# Define bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100, 200]\n",
    "\n",
    "# Initialize lists to store values\n",
    "x_vals = []\n",
    "reco_ratio_iqr_median = []\n",
    "int8_ratio_iqr_median = []\n",
    "\n",
    "# Iterate over bins\n",
    "for i in range(len(bins) - 1):\n",
    "    lim_low = bins[i]\n",
    "    lim_hi = bins[i + 1]\n",
    "    x_vals.append(np.mean([lim_low, lim_hi]))\n",
    "\n",
    "    # Filter gen_jet_pt values within the bin\n",
    "    gen_jet_values_in_bin = [pt for pt in gen_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Filter reco_jet_pt values within the bin\n",
    "    reco_jet_values_in_bin = [pt for pt in reco_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Filter INT8_jet_pt values within the bin\n",
    "    INT8_jet_values_in_bin = [pt for pt in INT8_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Check if there are any values in both gen_jet_pt and reco_jet_pt\n",
    "    if len(gen_jet_values_in_bin) == 0 or len(reco_jet_values_in_bin) == 0 or len(INT8_jet_values_in_bin) == 0:\n",
    "        # Append NaN values\n",
    "        reco_ratio_iqr_median.append(np.nan)\n",
    "        int8_ratio_iqr_median.append(np.nan)\n",
    "        continue\n",
    "\n",
    "    # Calculate IQR and median for reco_jet_pt / gen_jet_pt values\n",
    "    reco_ratio_values_in_bin = [reco / gen for reco, gen in zip(reco_jet_values_in_bin, gen_jet_values_in_bin)]\n",
    "    reco_ratio_iqr = np.percentile(reco_ratio_values_in_bin, 75) - np.percentile(reco_ratio_values_in_bin, 25)\n",
    "    reco_ratio_median = np.median(reco_ratio_values_in_bin)\n",
    "    reco_ratio_iqr_median_ratio = reco_ratio_iqr / reco_ratio_median\n",
    "    reco_ratio_iqr_median.append(reco_ratio_iqr_median_ratio)\n",
    "\n",
    "    # Calculate IQR and median for INT8_jet_pt / gen_jet_pt values\n",
    "    int8_ratio_values_in_bin = [int8 / gen for int8, gen in zip(INT8_jet_values_in_bin, gen_jet_values_in_bin)]\n",
    "    int8_ratio_iqr = np.percentile(int8_ratio_values_in_bin, 75) - np.percentile(int8_ratio_values_in_bin, 25)\n",
    "    int8_ratio_median = np.median(int8_ratio_values_in_bin)\n",
    "    int8_ratio_iqr_median_ratio = int8_ratio_iqr / int8_ratio_median\n",
    "    int8_ratio_iqr_median.append(int8_ratio_iqr_median_ratio)\n",
    "\n",
    "# Filter out NaN values\n",
    "x_vals_filtered_reco = [x for x, y in zip(x_vals, reco_ratio_iqr_median) if not np.isnan(y)]\n",
    "reco_ratio_iqr_median_filtered = [y for y in reco_ratio_iqr_median if not np.isnan(y)]\n",
    "\n",
    "x_vals_filtered_int8 = [x for x, y in zip(x_vals, int8_ratio_iqr_median) if not np.isnan(y)]\n",
    "int8_ratio_iqr_median_filtered = [y for y in int8_ratio_iqr_median if not np.isnan(y)]\n",
    "\n",
    "# Create TGraphs with filtered values\n",
    "gr_reco_ratio = ROOT.TGraph(len(x_vals_filtered_reco), np.array(x_vals_filtered_reco), np.array(reco_ratio_iqr_median_filtered))\n",
    "gr_int8_ratio = ROOT.TGraph(len(x_vals_filtered_int8), np.array(x_vals_filtered_int8), np.array(int8_ratio_iqr_median_filtered))\n",
    "\n",
    "# Set the titles to empty strings\n",
    "gr_reco_ratio.SetTitle(\"\")\n",
    "gr_int8_ratio.SetTitle(\"\")\n",
    "\n",
    "# Create a canvas\n",
    "canvas = ROOT.TCanvas(\"canvas\", \"Ratio IQR/Median vs Jet pT\", 800, 600)\n",
    "\n",
    "# Draw the first graph (Reco_jet_pt/gen_jet_pt)\n",
    "gr_reco_ratio.SetMarkerStyle(20)\n",
    "gr_reco_ratio.SetMarkerColor(ROOT.kBlue)\n",
    "gr_reco_ratio.SetLineColor(ROOT.kBlue)\n",
    "gr_reco_ratio.GetXaxis().SetTitle(\"Jet P_{T,gen} (GeV)\")\n",
    "gr_reco_ratio.GetYaxis().SetTitle(\"Response IQR / Median\")\n",
    "gr_reco_ratio.Draw(\"APL\")\n",
    "\n",
    "# Draw the second graph (INT8_jet_pt/gen_jet_pt) on the same canvas\n",
    "gr_int8_ratio.SetMarkerStyle(20)\n",
    "gr_int8_ratio.SetMarkerColor(ROOT.kRed)\n",
    "gr_int8_ratio.SetLineColor(ROOT.kRed)\n",
    "gr_int8_ratio.Draw(\"PL same\")\n",
    "\n",
    "# Get the X and Y axes\n",
    "xaxis = gr_reco_ratio.GetXaxis()\n",
    "yaxis = gr_reco_ratio.GetYaxis()\n",
    "\n",
    "# Set tick length for all four axes\n",
    "xaxis.SetTickLength(0.03)\n",
    "yaxis.SetTickLength(0.03)\n",
    "\n",
    "# Set y-axis range to show both plots\n",
    "min_y = min(min(reco_ratio_iqr_median_filtered), min(int8_ratio_iqr_median_filtered))\n",
    "max_y = max(max(reco_ratio_iqr_median_filtered), max(int8_ratio_iqr_median_filtered))\n",
    "yaxis.SetRangeUser(min_y * 0.9, max_y * 1.1)\n",
    "\n",
    "# Add legend\n",
    "legend = ROOT.TLegend(0.75, 0.75, 0.9, 0.9)\n",
    "legend.AddEntry(gr_reco_ratio, \"Pred_FP32\", \"lp\")\n",
    "legend.AddEntry(gr_int8_ratio, \"Pred_INT8\", \"lp\")\n",
    "legend.Draw()\n",
    "\n",
    "# Show the canvas\n",
    "canvas.Draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6726f15b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf9e55e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e07f45f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
