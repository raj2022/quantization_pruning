{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d827d310",
   "metadata": {},
   "source": [
    "Task:\n",
    "1. Try to plot the Jet and MET pT response curve with increased statistics\n",
    " * each event should have multiple jets so 1000 events -> few thousand jets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "113ecb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.30/02\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import ROOT\n",
    "\n",
    "import vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d985af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import tensorflow_datasets as tfds\n",
    "import torch_geometric\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a55701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08a0192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "data_dir = \"../../mlpf/tensorflow_datasets/\"\n",
    "dataset = \"clic_edm_ttbar_pf\"\n",
    "\n",
    "# Load dataset\n",
    "builder = tfds.builder(dataset, data_dir=data_dir)\n",
    "ds_train = builder.as_data_source(split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3efc1998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800800"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ea4423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_FEATURES_TRK = [\n",
    "    \"elemtype\",\n",
    "    \"pt\",\n",
    "    \"eta\",\n",
    "    \"sin_phi\",\n",
    "    \"cos_phi\",\n",
    "    \"p\",\n",
    "    \"chi2\",\n",
    "    \"ndf\",\n",
    "    \"dEdx\",\n",
    "    \"dEdxError\",\n",
    "    \"radiusOfInnermostHit\",\n",
    "    \"tanLambda\",\n",
    "    \"D0\",\n",
    "    \"omega\",\n",
    "    \"Z0\",\n",
    "    \"time\",\n",
    "]\n",
    "X_FEATURES_CL = [\n",
    "    \"elemtype\",\n",
    "    \"et\",\n",
    "    \"eta\",\n",
    "    \"sin_phi\",\n",
    "    \"cos_phi\",\n",
    "    \"energy\",\n",
    "    \"position.x\",\n",
    "    \"position.y\",\n",
    "    \"position.z\",\n",
    "    \"iTheta\",\n",
    "    \"energy_ecal\",\n",
    "    \"energy_hcal\",\n",
    "    \"energy_other\",\n",
    "    \"num_hits\",\n",
    "    \"sigma_x\",\n",
    "    \"sigma_y\",\n",
    "    \"sigma_z\",\n",
    "]\n",
    "Y_FEATURES = [\"cls_id\", \"charge\", \"pt\", \"eta\", \"sin_phi\", \"cos_phi\", \"energy\"]\n",
    "Y_CLASSES = [0, 211, 130, 22, 11, 13]\n",
    "\n",
    "INPUT_DIM = max(len(X_FEATURES_TRK), len(X_FEATURES_CL))\n",
    "NUM_CLASSES = len(Y_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8f1650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JP 2024-02-29: currently torch int8 onnx export does not work with MultiheadAttention because of the following:\n",
    "# - it uses q_scaling_product.mul_scalar which is not supported in ONNX opset 17: the fix is to just remove the q_scaling_product\n",
    "# - somehow, the \"need_weights\" option confuses the ONNX exporter because the multiheaded attention layer then returns a tuple: the fix is to make the MHA always return just the attended values only\n",
    "# I lifted these two modules directly from the pytorch code and made the modifications here.\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as nnF\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class QuantizeableMultiheadAttention(nn.MultiheadAttention):\n",
    "    _FLOAT_MODULE = nn.MultiheadAttention\n",
    "\n",
    "    r\"\"\"Quantizable implementation of the MultiheadAttention.\n",
    "\n",
    "    Note::\n",
    "        Please, refer to :class:`~torch.nn.MultiheadAttention` for more\n",
    "        information\n",
    "\n",
    "    Allows the model to jointly attend to information from different\n",
    "    representation subspaces.\n",
    "    See reference: Attention Is All You Need\n",
    "\n",
    "    The original MHA module is not quantizable.\n",
    "    This reimplements it by explicitly instantiating the linear layers.\n",
    "\n",
    "    .. math::\n",
    "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
    "        \\text{where} head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "\n",
    "    Args:\n",
    "        embed_dim: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n",
    "        bias: add bias as module parameter. Default: True.\n",
    "        add_bias_kv: add bias to the key and value sequences at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        kdim: total number of features in key. Default: None.\n",
    "        vdim: total number of features in value. Default: None.\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
    "\n",
    "    Note that if :attr:`kdim` and :attr:`vdim` are None, they will be set\n",
    "    to :attr:`embed_dim` such that query, key, and value have the same\n",
    "    number of features.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> import torch.ao.nn.quantizable as nnqa\n",
    "        >>> multihead_attn = nnqa.MultiheadAttention(embed_dim, num_heads)\n",
    "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "\n",
    "    Note::\n",
    "        Please, follow the quantization flow to convert the quantizable MHA.\n",
    "    \"\"\"\n",
    "    __constants__ = ['batch_first']\n",
    "\n",
    "    def __init__(self, embed_dim: int, num_heads: int,\n",
    "                 dropout: float = 0., bias: bool = True,\n",
    "                 add_bias_kv: bool = False, add_zero_attn: bool = False,\n",
    "                 kdim: Optional[int] = None, vdim: Optional[int] = None, batch_first: bool = False,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, num_heads, dropout,\n",
    "                         bias, add_bias_kv,\n",
    "                         add_zero_attn, kdim, vdim, batch_first,\n",
    "                         **factory_kwargs)\n",
    "        self.linear_Q = nn.Linear(self.embed_dim, self.embed_dim, bias=bias, **factory_kwargs)\n",
    "        self.linear_K = nn.Linear(self.kdim, self.embed_dim, bias=bias, **factory_kwargs)\n",
    "        self.linear_V = nn.Linear(self.vdim, self.embed_dim, bias=bias, **factory_kwargs)\n",
    "        # for the type: ignore, see https://github.com/pytorch/pytorch/issues/58969\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=bias, **factory_kwargs)  # type: ignore[assignment]\n",
    "\n",
    "        # Functionals\n",
    "        # self.q_scaling_product = torch.ao.nn.quantized.FloatFunctional()\n",
    "        # note: importing torch.ao.nn.quantized at top creates a circular import\n",
    "\n",
    "        # Quant/Dequant\n",
    "        self.quant_attn_output = torch.ao.quantization.QuantStub()\n",
    "        self.quant_attn_output_weights = torch.ao.quantization.QuantStub()\n",
    "        self.dequant_q = torch.ao.quantization.DeQuantStub()\n",
    "        self.dequant_k = torch.ao.quantization.DeQuantStub()\n",
    "        self.dequant_v = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def _get_name(self):\n",
    "        return 'QuantizableMultiheadAttention'\n",
    "\n",
    "    @classmethod\n",
    "    def from_float(cls, other):\n",
    "        assert type(other) == cls._FLOAT_MODULE\n",
    "        assert hasattr(other, 'qconfig'), \"The float module must have 'qconfig'\"\n",
    "        # Setting the dropout to 0.0!\n",
    "        observed = cls(other.embed_dim, other.num_heads, other.dropout,\n",
    "                       (other.in_proj_bias is not None),\n",
    "                       (other.bias_k is not None),\n",
    "                       other.add_zero_attn, other.kdim, other.vdim,\n",
    "                       other.batch_first)\n",
    "        observed.bias_k = other.bias_k\n",
    "        observed.bias_v = other.bias_v\n",
    "        observed.qconfig = other.qconfig\n",
    "\n",
    "        # Set the linear weights\n",
    "        # for the type: ignores, see https://github.com/pytorch/pytorch/issues/58969\n",
    "        observed.out_proj.weight = other.out_proj.weight  # type: ignore[has-type]\n",
    "        observed.out_proj.bias = other.out_proj.bias  # type: ignore[has-type]\n",
    "        if other._qkv_same_embed_dim:\n",
    "            # Use separate params\n",
    "            bias = other.in_proj_bias\n",
    "            _start = 0\n",
    "            _end = _start + other.embed_dim\n",
    "            weight = other.in_proj_weight[_start:_end, :]\n",
    "            if bias is not None:\n",
    "                bias = torch.nn.Parameter(bias[_start:_end], bias.requires_grad)\n",
    "            observed.linear_Q.weight = torch.nn.Parameter(weight,\n",
    "                                                          weight.requires_grad)\n",
    "            observed.linear_Q.bias = bias\n",
    "\n",
    "            bias = other.in_proj_bias\n",
    "            _start = _end\n",
    "            _end = _start + other.embed_dim\n",
    "            weight = other.in_proj_weight[_start:_end, :]\n",
    "            if bias is not None:\n",
    "                bias = torch.nn.Parameter(bias[_start:_end], bias.requires_grad)\n",
    "            observed.linear_K.weight = torch.nn.Parameter(weight,\n",
    "                                                          weight.requires_grad)\n",
    "            observed.linear_K.bias = bias\n",
    "\n",
    "            bias = other.in_proj_bias\n",
    "            _start = _end\n",
    "            weight = other.in_proj_weight[_start:, :]\n",
    "            if bias is not None:\n",
    "                bias = torch.nn.Parameter(bias[_start:], bias.requires_grad)\n",
    "            observed.linear_V.weight = torch.nn.Parameter(weight,\n",
    "                                                          weight.requires_grad)\n",
    "            observed.linear_V.bias = bias\n",
    "        else:\n",
    "            observed.linear_Q.weight = nn.Parameter(other.q_proj_weight)\n",
    "            observed.linear_K.weight = nn.Parameter(other.k_proj_weight)\n",
    "            observed.linear_V.weight = nn.Parameter(other.v_proj_weight)\n",
    "            if other.in_proj_bias is None:\n",
    "                observed.linear_Q.bias = None  # type: ignore[assignment]\n",
    "                observed.linear_K.bias = None  # type: ignore[assignment]\n",
    "                observed.linear_V.bias = None  # type: ignore[assignment]\n",
    "            else:\n",
    "                observed.linear_Q.bias = nn.Parameter(other.in_proj_bias[0:other.embed_dim])\n",
    "                observed.linear_K.bias = nn.Parameter(other.in_proj_bias[other.embed_dim:(other.embed_dim * 2)])\n",
    "                observed.linear_V.bias = nn.Parameter(other.in_proj_bias[(other.embed_dim * 2):])\n",
    "        observed.eval()\n",
    "        # Explicit prepare\n",
    "        observed = torch.ao.quantization.prepare(observed, inplace=True)\n",
    "        return observed\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def dequantize(self):\n",
    "        r\"\"\"Utility to convert the quantized MHA back to float.\n",
    "\n",
    "        The motivation for this is that it is not trivial to conver the weights\n",
    "        from the format that is used in the quantized version back to the\n",
    "        float.\n",
    "        \"\"\"\n",
    "        fp = self._FLOAT_MODULE(self.embed_dim, self.num_heads, self.dropout,\n",
    "                                (self.linear_Q._weight_bias()[1] is not None),\n",
    "                                (self.bias_k is not None),\n",
    "                                self.add_zero_attn, self.kdim, self.vdim, self.batch_first)\n",
    "        assert fp._qkv_same_embed_dim == self._qkv_same_embed_dim\n",
    "        if self.bias_k is not None:\n",
    "            fp.bias_k = nn.Parameter(self.bias_k.dequantize())\n",
    "        if self.bias_v is not None:\n",
    "            fp.bias_v = nn.Parameter(self.bias_v.dequantize())\n",
    "\n",
    "        # Set the linear weights\n",
    "        # Note: Because the linear layers are quantized, mypy does not nkow how\n",
    "        # to deal with them -- might need to ignore the typing checks.\n",
    "        # for the type: ignore[has-type], see https://github.com/pytorch/pytorch/issues/58969\n",
    "        w, b = self.out_proj._weight_bias()  # type: ignore[operator, has-type]\n",
    "        fp.out_proj.weight = nn.Parameter(w.dequantize())\n",
    "        if b is not None:\n",
    "            fp.out_proj.bias = nn.Parameter(b)\n",
    "\n",
    "        wQ, bQ = self.linear_Q._weight_bias()  # type: ignore[operator]\n",
    "        wQ = wQ.dequantize()\n",
    "        wK, bK = self.linear_K._weight_bias()  # type: ignore[operator]\n",
    "        wK = wK.dequantize()\n",
    "        wV, bV = self.linear_V._weight_bias()  # type: ignore[operator]\n",
    "        wV = wV.dequantize()\n",
    "        if fp._qkv_same_embed_dim:\n",
    "            # Use separate params\n",
    "            _start = 0\n",
    "            _end = _start + fp.embed_dim\n",
    "            fp.in_proj_weight[_start:_end, :] = wQ\n",
    "            if fp.in_proj_bias is not None:\n",
    "                assert all(bQ == 0)\n",
    "                fp.in_proj_bias[_start:_end] = bQ\n",
    "\n",
    "            _start = _end\n",
    "            _end = _start + fp.embed_dim\n",
    "            fp.in_proj_weight[_start:_end, :] = wK\n",
    "            if fp.in_proj_bias is not None:\n",
    "                assert all(bK == 0)\n",
    "                fp.in_proj_bias[_start:_end] = bK\n",
    "\n",
    "            _start = _end\n",
    "            fp.in_proj_weight[_start:, :] = wV\n",
    "            if fp.in_proj_bias is not None:\n",
    "                assert all(bV == 0)\n",
    "                fp.in_proj_bias[_start:] = bV\n",
    "        else:\n",
    "            fp.q_proj_weight = nn.Parameter(wQ)\n",
    "            fp.k_proj_weight = nn.Parameter(wK)\n",
    "            fp.v_proj_weight = nn.Parameter(wV)\n",
    "            if fp.in_proj_bias is None:\n",
    "                self.linear_Q.bias = None\n",
    "                self.linear_K.bias = None\n",
    "                self.linear_V.bias = None\n",
    "            else:\n",
    "                fp.in_proj_bias[0:fp.embed_dim] = bQ\n",
    "                fp.in_proj_bias[fp.embed_dim:(fp.embed_dim * 2)] = bK\n",
    "                fp.in_proj_bias[(fp.embed_dim * 2):] = bV\n",
    "\n",
    "        return fp\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_observed(cls, other):\n",
    "        # The whole flow is float -> observed -> quantized\n",
    "        # This class does float -> observed only\n",
    "        # See nn.quantized.MultiheadAttention\n",
    "        raise NotImplementedError(\"It looks like you are trying to prepare an \"\n",
    "                                  \"MHA module. Please, see \"\n",
    "                                  \"the examples on quantizable MHAs.\")\n",
    "\n",
    "    def forward(self,\n",
    "                query: Tensor,\n",
    "                key: Tensor,\n",
    "                value: Tensor,\n",
    "                key_padding_mask: Optional[Tensor] = None,\n",
    "                need_weights: bool = True,\n",
    "                attn_mask: Optional[Tensor] = None,\n",
    "                average_attn_weights: bool = True,\n",
    "                is_causal: bool = False) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        r\"\"\"\n",
    "    Note::\n",
    "        Please, refer to :func:`~torch.nn.MultiheadAttention.forward` for more\n",
    "        information\n",
    "\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. When given a binary mask and a value is True,\n",
    "            the corresponding value on the attention layer will be ignored.\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
    "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
    "\n",
    "    Shape:\n",
    "        - Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n",
    "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
    "          If a BoolTensor is provided, the positions with the\n",
    "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked\n",
    "          positions. If a BoolTensor is provided, positions with ``True``\n",
    "          is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "          is provided, it will be added to the attention weight.\n",
    "        - is_causal: If specified, applies a causal mask as attention mask. Mutually exclusive with providing attn_mask.\n",
    "          Default: ``False``.\n",
    "        - average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across\n",
    "          heads. Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an\n",
    "          effect when ``need_weights=True.``. Default: True (i.e. average weights across heads)\n",
    "\n",
    "        - Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n",
    "        - attn_output_weights: If ``average_attn_weights=True``, returns attention weights averaged\n",
    "          across heads of shape :math:`(N, L, S)`, where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. If ``average_attn_weights=False``, returns attention weights per\n",
    "          head of shape :math:`(N, num_heads, L, S)`.\n",
    "        \"\"\"\n",
    "        return self._forward_impl(query, key, value, key_padding_mask,\n",
    "                                  need_weights, attn_mask, average_attn_weights,\n",
    "                                  is_causal)\n",
    "\n",
    "    def _forward_impl(self,\n",
    "                      query: Tensor,\n",
    "                      key: Tensor,\n",
    "                      value: Tensor,\n",
    "                      key_padding_mask: Optional[Tensor] = None,\n",
    "                      need_weights: bool = True,\n",
    "                      attn_mask: Optional[Tensor] = None,\n",
    "                      average_attn_weights: bool = True,\n",
    "                      is_causal: bool = False) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        # This version will not deal with the static key/value pairs.\n",
    "        # Keeping it here for future changes.\n",
    "        #\n",
    "        # TODO: This method has some duplicate lines with the\n",
    "        # `torch.nn.functional.multi_head_attention`. Will need to refactor.\n",
    "        static_k = None\n",
    "        static_v = None\n",
    "\n",
    "        if attn_mask is not None and is_causal:\n",
    "            raise AssertionError(\"Only allow causal mask or attn_mask\")\n",
    "\n",
    "        if is_causal:\n",
    "            raise AssertionError(\"causal mask not supported by AO MHA module\")\n",
    "\n",
    "        if self.batch_first:\n",
    "            query, key, value = (x.transpose(0, 1) for x in (query, key, value))\n",
    "\n",
    "        tgt_len, bsz, embed_dim_to_check = query.size()\n",
    "        assert self.embed_dim == embed_dim_to_check\n",
    "        # allow MHA to have different sizes for the feature dimension\n",
    "        assert key.size(0) == value.size(0) and key.size(1) == value.size(1)\n",
    "\n",
    "        head_dim = self.embed_dim // self.num_heads\n",
    "        assert head_dim * self.num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        scaling = float(head_dim) ** -0.5\n",
    "\n",
    "        q = self.linear_Q(query)\n",
    "        k = self.linear_K(key)\n",
    "        v = self.linear_V(value)\n",
    "\n",
    "        #JP fix here: disabled this\n",
    "        # q = self.q_scaling_product.mul_scalar(q, scaling)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.uint8:\n",
    "                warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "                attn_mask = attn_mask.to(torch.bool)\n",
    "            assert attn_mask.is_floating_point() or attn_mask.dtype == torch.bool, \\\n",
    "                f'Only float and bool types are supported for attn_mask, not {attn_mask.dtype}'\n",
    "\n",
    "            if attn_mask.dim() == 2:\n",
    "                attn_mask = attn_mask.unsqueeze(0)\n",
    "                if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:\n",
    "                    raise RuntimeError('The size of the 2D attn_mask is not correct.')\n",
    "            elif attn_mask.dim() == 3:\n",
    "                if list(attn_mask.size()) != [bsz * self.num_heads, query.size(0), key.size(0)]:\n",
    "                    raise RuntimeError('The size of the 3D attn_mask is not correct.')\n",
    "            else:\n",
    "                raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "            # attn_mask's dim is 3 now.\n",
    "\n",
    "        # convert ByteTensor key_padding_mask to bool\n",
    "        if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n",
    "            warnings.warn(\"Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "            key_padding_mask = key_padding_mask.to(torch.bool)\n",
    "        if self.bias_k is not None and self.bias_v is not None:\n",
    "            if static_k is None and static_v is None:\n",
    "\n",
    "                # Explicitly assert that bias_k and bias_v are not None\n",
    "                # in a way that TorchScript can understand.\n",
    "                bias_k = self.bias_k\n",
    "                assert bias_k is not None\n",
    "                bias_v = self.bias_v\n",
    "                assert bias_v is not None\n",
    "\n",
    "                k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
    "                v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
    "                if attn_mask is not None:\n",
    "                    attn_mask = nnF.pad(attn_mask, (0, 1))\n",
    "                if key_padding_mask is not None:\n",
    "                    key_padding_mask = nnF.pad(key_padding_mask, (0, 1))\n",
    "            else:\n",
    "                assert static_k is None, \"bias cannot be added to static key.\"\n",
    "                assert static_v is None, \"bias cannot be added to static value.\"\n",
    "        else:\n",
    "            assert self.bias_k is None\n",
    "            assert self.bias_v is None\n",
    "\n",
    "        q = q.contiguous().view(tgt_len, bsz * self.num_heads, head_dim).transpose(0, 1)\n",
    "        if k is not None:\n",
    "            k = k.contiguous().view(-1, bsz * self.num_heads, head_dim).transpose(0, 1)\n",
    "        if v is not None:\n",
    "            v = v.contiguous().view(-1, bsz * self.num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "        if static_k is not None:\n",
    "            assert static_k.size(0) == bsz * self.num_heads\n",
    "            assert static_k.size(2) == head_dim\n",
    "            k = static_k\n",
    "\n",
    "        if static_v is not None:\n",
    "            assert static_v.size(0) == bsz * self.num_heads\n",
    "            assert static_v.size(2) == head_dim\n",
    "            v = static_v\n",
    "\n",
    "        src_len = k.size(1)\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            assert key_padding_mask.size(0) == bsz\n",
    "            assert key_padding_mask.size(1) == src_len\n",
    "\n",
    "        if self.add_zero_attn:\n",
    "            src_len += 1\n",
    "            k_zeros = torch.zeros((k.size(0), 1) + k.size()[2:])\n",
    "            if k.is_quantized:\n",
    "                k_zeros = torch.quantize_per_tensor(k_zeros, k.q_scale(), k.q_zero_point(), k.dtype)\n",
    "            k = torch.cat([k, k_zeros], dim=1)\n",
    "            v_zeros = torch.zeros((v.size(0), 1) + k.size()[2:])\n",
    "            if v.is_quantized:\n",
    "                v_zeros = torch.quantize_per_tensor(v_zeros, v.q_scale(), v.q_zero_point(), v.dtype)\n",
    "            v = torch.cat([v, v_zeros], dim=1)\n",
    "\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = nnF.pad(attn_mask, (0, 1))\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = nnF.pad(key_padding_mask, (0, 1))\n",
    "\n",
    "        # Leaving the quantized zone here\n",
    "        q = self.dequant_q(q)\n",
    "        k = self.dequant_k(k)\n",
    "        v = self.dequant_v(v)\n",
    "        attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "        assert list(attn_output_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                attn_output_weights.masked_fill_(attn_mask, float('-inf'))\n",
    "            else:\n",
    "                attn_output_weights += attn_mask\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            attn_output_weights = attn_output_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_output_weights = attn_output_weights.masked_fill(\n",
    "                key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "                float('-inf'),\n",
    "            )\n",
    "            attn_output_weights = attn_output_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        attn_output_weights = nnF.softmax(\n",
    "            attn_output_weights, dim=-1)\n",
    "        attn_output_weights = nnF.dropout(attn_output_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.bmm(attn_output_weights, v)\n",
    "        assert list(attn_output.size()) == [bsz * self.num_heads, tgt_len, head_dim]\n",
    "        if self.batch_first:\n",
    "            attn_output = attn_output.view(bsz, tgt_len, self.embed_dim)\n",
    "        else:\n",
    "            attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, self.embed_dim)\n",
    "\n",
    "        # Reentering the quantized zone\n",
    "        attn_output = self.quant_attn_output(attn_output)\n",
    "        # for the type: ignore[has-type], see https://github.com/pytorch/pytorch/issues/58969\n",
    "        attn_output = self.out_proj(attn_output)  # type: ignore[has-type]\n",
    "\n",
    "        #JP fix: removed need_weights part from here, return attn_output instead of tuple\n",
    "        return attn_output\n",
    "\n",
    "class QuantizedMultiheadAttention(QuantizeableMultiheadAttention):\n",
    "    _FLOAT_MODULE = torch.ao.nn.quantizable.MultiheadAttention\n",
    "\n",
    "    def _get_name(self):\n",
    "        return \"QuantizedMultiheadAttention\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_float(cls, other):\n",
    "        # The whole flow is float -> observed -> quantized\n",
    "        # This class does observed -> quantized only\n",
    "        raise NotImplementedError(\"It looks like you are trying to convert a \"\n",
    "                                  \"non-observed MHA module. Please, see \"\n",
    "                                  \"the examples on quantizable MHAs.\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_observed(cls, other):\n",
    "        converted = torch.ao.quantization.convert(other, mapping=None,\n",
    "                                                  inplace=False,\n",
    "                                                  remove_qconfig=True,\n",
    "                                                  convert_custom_config_dict=None)\n",
    "        converted.__class__ = cls\n",
    "        # Remove the parameters for the bias_k and bias_v to quantize them\n",
    "        # TODO: This is a potential source of accuracy drop.\n",
    "        #       quantized cat takes the scale and zp of the first\n",
    "        #       element, which might lose the precision in the bias_k\n",
    "        #       and the bias_v (which are cat'ed with k/v being first).\n",
    "        if converted.bias_k is not None:\n",
    "            bias_k = converted._parameters.pop('bias_k')\n",
    "            sc, zp = torch._choose_qparams_per_tensor(bias_k,\n",
    "                                                      reduce_range=False)\n",
    "            bias_k = torch.quantize_per_tensor(bias_k, sc, zp, torch.quint8)\n",
    "            setattr(converted, 'bias_k', bias_k)  # noqa: B010\n",
    "\n",
    "        if converted.bias_v is not None:\n",
    "            bias_v = converted._parameters.pop('bias_v')\n",
    "            sc, zp = torch._choose_qparams_per_tensor(bias_k,  # type: ignore[possibly-undefined]\n",
    "                                                      reduce_range=False)\n",
    "            bias_v = torch.quantize_per_tensor(bias_v, sc, zp, torch.quint8)\n",
    "            setattr(converted, 'bias_v', bias_v)  # noqa: B010\n",
    "\n",
    "        del converted.in_proj_weight\n",
    "        del converted.in_proj_bias\n",
    "\n",
    "        return converted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52bf9733",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss, as described in https://arxiv.org/abs/1708.02002.\n",
    "    It is essentially an enhancement to cross entropy loss and is\n",
    "    useful for classification tasks when there is a large class imbalance.\n",
    "    x is expected to contain raw, unnormalized scores for each class.\n",
    "    y is expected to contain class labels.\n",
    "    Shape:\n",
    "        - x: (batch_size, C) or (batch_size, C, d1, d2, ..., dK), K > 0.\n",
    "        - y: (batch_size,) or (batch_size, d1, d2, ..., dK), K > 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, alpha = None, gamma = 0.0, reduction = \"mean\", ignore_index = -100\n",
    "    ):\n",
    "        \"\"\"Constructor.\n",
    "        Args:\n",
    "            alpha (Tensor, optional): Weights for each class. Defaults to None.\n",
    "            gamma (float, optional): A constant, as described in the paper.\n",
    "                Defaults to 0.\n",
    "            reduction (str, optional): 'mean', 'sum' or 'none'.\n",
    "                Defaults to 'mean'.\n",
    "            ignore_index (int, optional): class label to ignore.\n",
    "                Defaults to -100.\n",
    "        \"\"\"\n",
    "        if reduction not in (\"mean\", \"sum\", \"none\"):\n",
    "            raise ValueError('Reduction must be one of: \"mean\", \"sum\", \"none\".')\n",
    "\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "        self.nll_loss = nn.NLLLoss(weight=alpha, reduction=\"none\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        arg_keys = [\"alpha\", \"gamma\", \"reduction\"]\n",
    "        arg_vals = [self.__dict__[k] for k in arg_keys]\n",
    "        arg_strs = [f\"{k}={v!r}\" for k, v in zip(arg_keys, arg_vals)]\n",
    "        arg_str = \", \".join(arg_strs)\n",
    "        return f\"{type(self).__name__}({arg_str})\"\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        if x.ndim > 2:\n",
    "            # (N, C, d1, d2, ..., dK) --> (N * d1 * ... * dK, C)\n",
    "            c = x.shape[1]\n",
    "            x = x.permute(0, *range(2, x.ndim), 1).reshape(-1, c)\n",
    "            # (N, d1, d2, ..., dK) --> (N * d1 * ... * dK,)\n",
    "            y = y.view(-1)\n",
    "\n",
    "        # compute weighted cross entropy term: -alpha * log(pt)\n",
    "        # (alpha is already part of self.nll_loss)\n",
    "        log_p = F.log_softmax(x, dim=-1)\n",
    "        ce = self.nll_loss(log_p, y)\n",
    "\n",
    "        # get true class column from each row\n",
    "        # this is slow due to indexing\n",
    "        # all_rows = torch.arange(len(x))\n",
    "        # log_pt = log_p[all_rows, y]\n",
    "        log_pt = torch.gather(log_p, 1, y.unsqueeze(axis=-1)).squeeze(axis=-1)\n",
    "\n",
    "        # compute focal term: (1 - pt)^gamma\n",
    "        pt = log_pt.exp()\n",
    "        focal_term = (1 - pt) ** self.gamma\n",
    "\n",
    "        # the full loss: -alpha * ((1 - pt)^gamma) * log(pt)\n",
    "        loss = focal_term * ce\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            loss = loss.sum()\n",
    "\n",
    "        return loss\n",
    "        \n",
    "def mlpf_loss(y, ypred, mask):\n",
    "    loss = {}\n",
    "    loss_obj_id = FocalLoss(gamma=2.0, reduction=\"none\")\n",
    "\n",
    "    msk_true_particle = torch.unsqueeze((y[\"cls_id\"] != 0).to(dtype=torch.float32), axis=-1)\n",
    "    nelem = torch.sum(mask)\n",
    "    npart = torch.sum(y[\"cls_id\"] != 0)\n",
    "    \n",
    "    ypred[\"momentum\"] = ypred[\"momentum\"] * msk_true_particle\n",
    "    y[\"momentum\"] = y[\"momentum\"] * msk_true_particle\n",
    "\n",
    "    ypred[\"cls_id_onehot\"] = ypred[\"cls_id_onehot\"].permute((0, 2, 1))\n",
    "\n",
    "    loss_classification = loss_obj_id(ypred[\"cls_id_onehot\"], y[\"cls_id\"]).reshape(y[\"cls_id\"].shape)\n",
    "    loss_regression = torch.nn.functional.huber_loss(ypred[\"momentum\"], y[\"momentum\"], reduction=\"none\")\n",
    "    \n",
    "    # average over all elements that were not padded\n",
    "    loss[\"Classification\"] = loss_classification.sum() / npart\n",
    "    \n",
    "    mom_normalizer = y[\"momentum\"][y[\"cls_id\"] != 0].std(axis=0)\n",
    "    reg_losses = loss_regression[y[\"cls_id\"] != 0]\n",
    "    # average over all true particles\n",
    "    loss[\"Regression\"] = (reg_losses / mom_normalizer).sum() / npart\n",
    "\n",
    "    px = ypred[\"momentum\"][..., 0:1] * ypred[\"momentum\"][..., 3:4] * msk_true_particle\n",
    "    py = ypred[\"momentum\"][..., 0:1] * ypred[\"momentum\"][..., 2:3] * msk_true_particle\n",
    "    pred_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)\n",
    "\n",
    "    px = y[\"momentum\"][..., 0:1] * y[\"momentum\"][..., 3:4] * msk_true_particle\n",
    "    py = y[\"momentum\"][..., 0:1] * y[\"momentum\"][..., 2:3] * msk_true_particle\n",
    "    true_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)\n",
    "    loss[\"MET\"] = torch.nn.functional.huber_loss(pred_met, true_met).mean()\n",
    "\n",
    "    loss[\"Total\"] = loss[\"Classification\"] + loss[\"Regression\"]\n",
    "    # loss[\"Total\"] += 0.1*loss[\"MET\"]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbf53040",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizeFeaturesStub(torch.ao.quantization.QuantStub):\n",
    "    def __init__(self, num_feats):\n",
    "        super().__init__()\n",
    "        self.num_feats = num_feats\n",
    "        self.quants = torch.nn.ModuleList()\n",
    "        for ifeat in range(self.num_feats):\n",
    "            self.quants.append(torch.ao.quantization.QuantStub())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.quants[ifeat](x[..., ifeat:ifeat+1]) for ifeat in range(self.num_feats)], axis=-1)\n",
    "        \n",
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim=128,\n",
    "        num_heads=2,\n",
    "        width=128,\n",
    "        dropout_mha=0.1,\n",
    "        dropout_ff=0.1,\n",
    "        attention_type=\"efficient\",\n",
    "    ):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "\n",
    "        self.attention_type = attention_type\n",
    "        self.act = nn.ReLU\n",
    "        self.mha = torch.nn.MultiheadAttention(embedding_dim, num_heads, dropout=dropout_mha, batch_first=True)\n",
    "        self.norm0 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.norm1 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.seq = torch.nn.Sequential(\n",
    "            nn.Linear(embedding_dim, width), self.act(), nn.Linear(width, embedding_dim), self.act()\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(dropout_ff)\n",
    "\n",
    "        self.add0 = torch.ao.nn.quantized.FloatFunctional()\n",
    "        self.add1 = torch.ao.nn.quantized.FloatFunctional()\n",
    "        self.mul = torch.ao.nn.quantized.FloatFunctional()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        mha_out = self.mha(x, x, x, need_weights=False)[0]\n",
    "        x = self.add0.add(x, mha_out)\n",
    "        x = self.norm0(x)\n",
    "        x = self.add1.add(x, self.seq(x))\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(x)\n",
    "        # x = self.mul.mul(x, mask.unsqueeze(-1))\n",
    "        return x\n",
    "\n",
    "class RegressionOutput(nn.Module):\n",
    "    def __init__(self, embed_dim, width, act, dropout):\n",
    "        super(RegressionOutput, self).__init__()\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "        self.nn = ffn(embed_dim, 1, width, act, dropout)\n",
    "\n",
    "    def forward(self, elems, x, orig_value):\n",
    "        nn_out = self.nn(x)\n",
    "        nn_out = self.dequant(nn_out)\n",
    "        return orig_value + nn_out\n",
    "\n",
    "def ffn(input_dim, output_dim, width, act, dropout):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, width),\n",
    "        act(),\n",
    "        torch.nn.LayerNorm(width),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(width, output_dim),\n",
    "    )\n",
    "\n",
    "def transform_batch(Xbatch):\n",
    "    Xbatch = Xbatch.clone()\n",
    "    Xbatch[..., 1] = torch.log(Xbatch[..., 1])\n",
    "    Xbatch[..., 5] = torch.log(Xbatch[..., 5])\n",
    "    Xbatch[torch.isnan(Xbatch)] = 0.0\n",
    "    Xbatch[torch.isinf(Xbatch)] = 0.0\n",
    "    return Xbatch\n",
    "    \n",
    "def unpack_target(y):\n",
    "    ret = {}\n",
    "    ret[\"cls_id\"] = y[..., 0].long()\n",
    "\n",
    "    for i, feat in enumerate(Y_FEATURES):\n",
    "        if i >= 2:  # skip the cls and charge as they are defined above\n",
    "            ret[feat] = y[..., i].to(dtype=torch.float32)\n",
    "    ret[\"phi\"] = torch.atan2(ret[\"sin_phi\"], ret[\"cos_phi\"])\n",
    "    \n",
    "    # note ~ momentum = [\"pt\", \"eta\", \"sin_phi\", \"cos_phi\", \"energy\"]\n",
    "    ret[\"momentum\"] = y[..., 2:7].to(dtype=torch.float32)\n",
    "    ret[\"p4\"] = torch.cat(\n",
    "        [ret[\"pt\"].unsqueeze(1), ret[\"eta\"].unsqueeze(1), ret[\"phi\"].unsqueeze(1), ret[\"energy\"].unsqueeze(1)], axis=1\n",
    "    )\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def unpack_predictions(preds):\n",
    "    ret = {}\n",
    "    ret[\"cls_id_onehot\"], ret[\"momentum\"] = preds\n",
    "\n",
    "    ret[\"pt\"] = ret[\"momentum\"][..., 0]\n",
    "    ret[\"eta\"] = ret[\"momentum\"][..., 1]\n",
    "    ret[\"sin_phi\"] = ret[\"momentum\"][..., 2]\n",
    "    ret[\"cos_phi\"] = ret[\"momentum\"][..., 3]\n",
    "    ret[\"energy\"] = ret[\"momentum\"][..., 4]\n",
    "\n",
    "    ret[\"cls_id\"] = torch.argmax(ret[\"cls_id_onehot\"], axis=-1)\n",
    "    ret[\"phi\"] = torch.atan2(ret[\"sin_phi\"], ret[\"cos_phi\"])\n",
    "    ret[\"p4\"] = torch.cat(\n",
    "        [\n",
    "            ret[\"pt\"].unsqueeze(axis=-1),\n",
    "            ret[\"eta\"].unsqueeze(axis=-1),\n",
    "            ret[\"phi\"].unsqueeze(axis=-1),\n",
    "            ret[\"energy\"].unsqueeze(axis=-1),\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "    return ret\n",
    "\n",
    "class MLPF(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=16,\n",
    "        num_classes=6,\n",
    "        num_convs=2,\n",
    "        dropout_ff=0.0,\n",
    "        dropout_conv_reg_mha=0.0,\n",
    "        dropout_conv_reg_ff=0.0,\n",
    "        dropout_conv_id_mha=0.0,\n",
    "        dropout_conv_id_ff=0.0,\n",
    "        num_heads=16,\n",
    "        head_dim=16,\n",
    "        elemtypes=[0,1,2],\n",
    "    ):\n",
    "        super(MLPF, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.act = nn.ReLU\n",
    "        self.elemtypes = elemtypes\n",
    "        self.num_elemtypes = len(self.elemtypes)\n",
    "\n",
    "        embedding_dim = num_heads * head_dim\n",
    "        width = num_heads * head_dim\n",
    "        \n",
    "        self.nn0_id = ffn(self.input_dim, embedding_dim, width, self.act, dropout_ff)\n",
    "        self.nn0_reg = ffn(self.input_dim, embedding_dim, width, self.act, dropout_ff)\n",
    "        \n",
    "        self.conv_id = nn.ModuleList()\n",
    "        self.conv_reg = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_convs):\n",
    "            self.conv_id.append(\n",
    "                SelfAttentionLayer(\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    width=width,\n",
    "                    dropout_mha=dropout_conv_id_mha,\n",
    "                    dropout_ff=dropout_conv_id_ff,\n",
    "                )\n",
    "            )\n",
    "            self.conv_reg.append(\n",
    "                SelfAttentionLayer(\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    width=width,\n",
    "                    dropout_mha=dropout_conv_reg_mha,\n",
    "                    dropout_ff=dropout_conv_reg_ff,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        decoding_dim = self.input_dim + embedding_dim\n",
    "\n",
    "        # DNN that acts on the node level to predict the PID\n",
    "        self.nn_id = ffn(decoding_dim, num_classes, width, self.act, dropout_ff)\n",
    "\n",
    "        # elementwise DNN for node momentum regression\n",
    "        embed_dim = decoding_dim + num_classes\n",
    "        self.nn_pt = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_eta = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_sin_phi = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_cos_phi = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_energy = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        \n",
    "        self.quant = QuantizeFeaturesStub(self.input_dim + len(self.elemtypes))\n",
    "        self.dequant_id = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, X_features, mask):\n",
    "        Xfeat_transformed = transform_batch(X_features)\n",
    "        Xfeat_normed = self.quant(Xfeat_transformed)\n",
    "\n",
    "        embeddings_id, embeddings_reg = [], []\n",
    "        embedding_id = self.nn0_id(Xfeat_normed)\n",
    "        embedding_reg = self.nn0_reg(Xfeat_normed)\n",
    "        for num, conv in enumerate(self.conv_id):\n",
    "            conv_input = embedding_id if num == 0 else embeddings_id[-1]\n",
    "            out_padded = conv(conv_input, mask)\n",
    "            embeddings_id.append(out_padded)\n",
    "        for num, conv in enumerate(self.conv_reg):\n",
    "            conv_input = embedding_reg if num == 0 else embeddings_reg[-1]\n",
    "            out_padded = conv(conv_input, mask)\n",
    "            embeddings_reg.append(out_padded)\n",
    "\n",
    "        final_embedding_id = torch.cat([Xfeat_normed] + [embeddings_id[-1]], axis=-1)\n",
    "        preds_id = self.nn_id(final_embedding_id)\n",
    "\n",
    "        final_embedding_reg = torch.cat([Xfeat_normed] + [embeddings_reg[-1]] + [preds_id], axis=-1)\n",
    "        preds_pt = self.nn_pt(X_features, final_embedding_reg, X_features[..., 1:2])\n",
    "        preds_eta = self.nn_eta(X_features, final_embedding_reg, X_features[..., 2:3])\n",
    "        preds_sin_phi = self.nn_sin_phi(X_features, final_embedding_reg, X_features[..., 3:4])\n",
    "        preds_cos_phi = self.nn_cos_phi(X_features, final_embedding_reg, X_features[..., 4:5])\n",
    "        preds_energy = self.nn_energy(X_features, final_embedding_reg, X_features[..., 5:6])\n",
    "        preds_momentum = torch.cat([preds_pt, preds_eta, preds_sin_phi, preds_cos_phi, preds_energy], axis=-1)\n",
    "        \n",
    "        preds_id = self.dequant_id(preds_id)\n",
    "        return preds_id, preds_momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20fdd196",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.32: : 80080it [1:27:12, 15.30it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss=0.36\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "max_events_train = 10000\n",
    "max_events_eval = 10000\n",
    "events_per_batch = 1000\n",
    "nepochs = 1\n",
    "batch_size = 50  # Reduce batch size\n",
    "\n",
    "# Define your model and optimizer\n",
    "# model = MLPF(input_dim=INPUT_DIM, num_classes=NUM_CLASSES).to(device=device)\n",
    "# optimizer = torch.optim.Adam(lr=1e-4, params=model.parameters())\n",
    "\n",
    "# Define a function to stream data from the dataset\n",
    "def stream_data(dataset, batch_size):\n",
    "    current_index = 0\n",
    "    while current_index < len(dataset):\n",
    "        batch = []\n",
    "        for _ in range(batch_size):\n",
    "            if current_index >= len(dataset):\n",
    "                break\n",
    "            batch.append(dataset[current_index])\n",
    "            current_index += 1\n",
    "        yield batch\n",
    "\n",
    "\n",
    "# Training \n",
    "loss_vals_epochs = []\n",
    "best_loss = float('inf')\n",
    "patience = 3  # Number of epochs to wait for improvement\n",
    "no_improvement = 0\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    loss_vals_steps = []\n",
    "    \n",
    "    # Generate indices for batches\n",
    "    inds_train = range(0, max_events_train, events_per_batch * 10)\n",
    "    \n",
    "    # Create tqdm instance to show progress\n",
    "    data_stream = stream_data(ds_train, batch_size)\n",
    "    data_stream = tqdm.tqdm(data_stream, total=len(inds_train))\n",
    "    \n",
    "    for batch in data_stream:\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "        X_features = [torch.tensor(elem[\"X\"]).to(torch.float32) for elem in batch]\n",
    "        y_targets = [torch.tensor(elem[\"ygen\"]).to(torch.float32) for elem in batch]\n",
    "\n",
    "        X_features_padded = pad_sequence(X_features, batch_first=True).to(device=device)\n",
    "        y_targets_padded = pad_sequence(y_targets, batch_first=True).to(device=device)\n",
    "        mask = X_features_padded[:, :, 0] != 0\n",
    "\n",
    "        preds = model(X_features_padded, mask)\n",
    "        preds_unpacked = unpack_predictions(preds)\n",
    "        targets_unpacked = unpack_target(y_targets_padded)\n",
    "\n",
    "        loss = mlpf_loss(targets_unpacked, preds_unpacked, mask)\n",
    "        loss[\"Total\"].backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_vals_steps.append(loss[\"Total\"].detach().cpu().item())\n",
    "\n",
    "        # Update tqdm progress bar\n",
    "        data_stream.set_description(f\"Epoch {epoch}, Loss: {loss_vals_steps[-1]:.2f}\")\n",
    "\n",
    "    epoch_loss = np.mean(loss_vals_steps)\n",
    "    loss_vals_epochs.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch}, loss={epoch_loss:.2f}\")\n",
    "\n",
    "    # Check for improvement in validation loss\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        no_improvement = 0\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "\n",
    "    # If no improvement for 'patience' epochs, stop training\n",
    "    if no_improvement >= patience:\n",
    "        print(\"Early stopping: No improvement for\", patience, \"epochs\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a6575a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.unpack_predictions(preds)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdfa6a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_features 10\n",
      "y_features 10\n"
     ]
    }
   ],
   "source": [
    "#  print(ind+events_per_batch)\n",
    "# print(ind)\n",
    "# print(batch_inds)\n",
    "print(f'X_features',len(X_features))\n",
    "print(f'y_features',len(y_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2d1fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the model back on CPU\n",
    "model = model.to(device=\"cpu\")\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "ds_elems = [ds_train[i] for i in range(max_events_train, max_events_train + max_events_eval)]\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32  \n",
    "for i in range(0, len(ds_elems), batch_size):\n",
    "    batch_elems = ds_elems[i:i + batch_size]\n",
    "    # input features\n",
    "    X_features = [torch.tensor(elem[\"X\"]).to(torch.float32) for elem in batch_elems]\n",
    "    X_features_padded = pad_sequence(X_features, batch_first=True)\n",
    "    #  target labels\n",
    "    y_targets = [torch.tensor(elem[\"ygen\"]).to(torch.float32) for elem in batch_elems]\n",
    "    y_targets_padded = pad_sequence(y_targets, batch_first=True)\n",
    "    #  mask for the batch\n",
    "    mask = X_features_padded[:, :, 0] != 0\n",
    "    #  model prediction, loss computation\n",
    "    preds = model(X_features_padded, mask)\n",
    "    preds = preds[0].detach(), preds[1].detach()\n",
    "    # Update mask for the batch\n",
    "    mask = X_features_padded[:, :, 0] != 0\n",
    "    # Unpack predictions and targets for the batch\n",
    "    preds_unpacked = unpack_predictions(preds)\n",
    "    targets_unpacked = unpack_target(y_targets_padded)\n",
    "    # append to a list \n",
    "    all_preds.append(preds_unpacked)\n",
    "    all_targets.append(targets_unpacked)\n",
    "    # Compute loss for the batch\n",
    "    loss = mlpf_loss(targets_unpacked, preds_unpacked, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3b971cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds_unpacked' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/sraj/ipykernel_2742669/1055632696.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds_unpacked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'preds_unpacked' is not defined"
     ]
    }
   ],
   "source": [
    "preds_unpacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2419cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "msk_true_particles = targets_unpacked[\"cls_id\"]!=0\n",
    "msk_pred_particles = preds_unpacked[\"cls_id\"]!=0\n",
    "\n",
    "\n",
    "pt_target = targets_unpacked[\"pt\"][msk_true_particles].numpy()\n",
    "pt_pred = preds_unpacked[\"pt\"][msk_true_particles].numpy()\n",
    "\n",
    "eta_target = targets_unpacked[\"eta\"][msk_true_particles].numpy()\n",
    "eta_pred = preds_unpacked[\"eta\"][msk_true_particles].numpy()\n",
    "\n",
    "sphi_target = targets_unpacked[\"sin_phi\"][msk_true_particles].numpy()\n",
    "sphi_pred = preds_unpacked[\"sin_phi\"][msk_true_particles].numpy()\n",
    "\n",
    "cphi_target = targets_unpacked[\"cos_phi\"][msk_true_particles].numpy()\n",
    "cphi_pred = preds_unpacked[\"cos_phi\"][msk_true_particles].numpy()\n",
    "\n",
    "energy_target = targets_unpacked[\"energy\"][msk_true_particles].numpy()\n",
    "energy_pred = preds_unpacked[\"energy\"][msk_true_particles].numpy()\n",
    "\n",
    "px = preds_unpacked[\"pt\"] * preds_unpacked[\"cos_phi\"] * msk_true_particles\n",
    "py = preds_unpacked[\"pt\"] * preds_unpacked[\"sin_phi\"] * msk_true_particles\n",
    "pred_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)\n",
    "\n",
    "px = targets_unpacked[\"pt\"] * targets_unpacked[\"cos_phi\"] * msk_true_particles\n",
    "py = targets_unpacked[\"pt\"] * targets_unpacked[\"sin_phi\"] * msk_true_particles\n",
    "true_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "408088f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cls_id': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 4, 1, 1, 1,\n",
       "          1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          3, 0, 0, 2, 2, 3, 3, 3, 3, 0, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3,\n",
       "          3, 3, 3, 0, 2, 3, 2, 2, 2, 2, 0, 0, 3, 0, 3, 3, 3, 3, 3, 2, 3, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0],\n",
       "         [1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 3, 0, 3, 0, 2, 3, 0,\n",
       "          3, 0, 0, 0, 3, 0, 3, 3, 3, 0, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 0, 0, 3,\n",
       "          3, 3, 3, 0, 3, 3, 0, 3, 0, 3, 3, 3, 2, 2, 0, 3, 3, 3, 3, 3, 3, 3, 3, 2,\n",
       "          3, 3, 3, 3, 0, 0, 3, 3, 2, 3, 3, 3, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0],\n",
       "         [5, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 3, 3,\n",
       "          0, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 2, 3, 3, 3, 3, 3,\n",
       "          3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "          0, 2, 0],\n",
       "         [5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 0, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "          1, 1, 4, 0, 0, 0, 1, 1, 0, 1, 0, 3, 2, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          3, 3, 3, 3, 3, 0, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "          0, 3, 2, 2, 2, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0]]),\n",
       " 'pt': tensor([[6.3097e+00, 2.4933e+01, 1.0572e+01, 1.0860e+01, 8.1536e+00, 8.0992e+00,\n",
       "          7.4669e+00, 6.1094e+00, 5.6848e+00, 4.9554e+00, 4.3373e+00, 3.6187e+00,\n",
       "          3.8369e+00, 3.4931e+00, 3.4192e+00, 2.8234e+00, 2.6763e+00, 2.6101e+00,\n",
       "          2.3716e+00, 2.3660e+00, 2.3423e+00, 2.0973e+00, 1.8225e+00, 1.7767e+00,\n",
       "          1.7219e+00, 1.4958e+00, 1.4205e+00, 1.4065e+00, 1.3335e+00, 1.3135e+00,\n",
       "          1.2690e+00, 1.0663e+00, 9.7952e-01, 8.3271e-01, 9.4661e-01, 8.9904e-01,\n",
       "          8.9714e-01, 6.5815e-01, 6.4900e-01, 6.0931e-01, 5.1535e-01, 4.3476e-01,\n",
       "          3.6808e-01, 3.6109e-01, 3.4783e-01, 3.1451e-01, 2.7818e-01, 1.9012e-01,\n",
       "          2.4456e-01, 0.0000e+00, 2.1458e-01, 0.0000e+00, 1.6038e-01, 1.1412e-01,\n",
       "          0.0000e+00, 3.7312e+00, 5.3955e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3855e-01,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          8.7078e-02, 0.0000e+00, 0.0000e+00, 5.5610e+00, 1.3106e+00, 5.9431e-01,\n",
       "          2.4906e-01, 2.7542e+00, 6.8721e-01, 0.0000e+00, 8.9483e-01, 4.0773e-01,\n",
       "          0.0000e+00, 2.8758e-01, 4.9329e-01, 1.0804e+00, 4.4490e-01, 1.2563e-01,\n",
       "          1.7516e-01, 1.3887e+00, 2.9779e-01, 0.0000e+00, 1.0056e+00, 4.3872e-01,\n",
       "          1.4983e-01, 1.0170e-01, 2.7148e-01, 0.0000e+00, 1.0979e+00, 1.8806e-01,\n",
       "          3.4160e+00, 2.1689e+00, 1.8448e+00, 4.6308e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.4622e+00, 0.0000e+00, 1.6185e-01, 6.7944e-01, 1.7848e+01, 3.0255e+01,\n",
       "          2.6586e+00, 4.9641e+00, 8.5890e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [9.5393e+00, 5.8424e+00, 1.4071e+01, 8.9284e+00, 9.0806e+00, 2.4479e+00,\n",
       "          5.2434e+00, 5.5317e+00, 1.8565e+00, 3.9730e+00, 7.5611e+00, 3.3957e+00,\n",
       "          2.8545e+00, 5.5011e+00, 1.7422e+00, 1.4363e+00, 1.3392e+00, 1.2846e+00,\n",
       "          4.2873e+00, 1.2698e+00, 1.2678e+00, 1.2893e+00, 1.0736e+00, 1.0773e+00,\n",
       "          1.0261e+00, 9.8498e-01, 9.1349e-01, 8.3860e-01, 8.5929e-01, 7.4958e-01,\n",
       "          7.8668e-01, 6.5937e-01, 6.0709e-01, 5.1991e-01, 5.3362e-01, 4.9559e-01,\n",
       "          4.2669e-01, 4.3451e-01, 3.2458e-01, 3.2530e-01, 2.8872e-01, 3.0898e-01,\n",
       "          2.7338e-01, 2.4456e-01, 2.1497e-01, 1.5977e-01, 0.0000e+00, 7.9764e-02,\n",
       "          1.5885e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.7260e-02,\n",
       "          0.0000e+00, 2.2175e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.5859e-01,\n",
       "          0.0000e+00, 3.3537e-01, 0.0000e+00, 7.5327e+00, 4.6658e-01, 0.0000e+00,\n",
       "          9.2157e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7840e-01, 0.0000e+00,\n",
       "          5.1159e-01, 1.0730e+00, 2.0730e-01, 0.0000e+00, 2.3340e-01, 5.0560e-01,\n",
       "          1.4020e+00, 0.0000e+00, 9.2479e-01, 1.2832e+00, 4.0703e-01, 7.6916e-02,\n",
       "          9.0208e-01, 2.1965e-01, 2.7626e-01, 0.0000e+00, 0.0000e+00, 3.6236e-01,\n",
       "          3.7169e+00, 5.3192e-01, 4.5301e+00, 0.0000e+00, 3.0519e-01, 2.1958e+00,\n",
       "          0.0000e+00, 1.9024e+00, 0.0000e+00, 1.3558e+00, 2.2252e-01, 1.9847e-01,\n",
       "          2.7579e+01, 3.3316e+00, 0.0000e+00, 4.9719e-01, 7.0150e-01, 2.5319e-01,\n",
       "          5.0691e-01, 6.0460e-01, 9.0451e-01, 6.5379e-01, 7.8424e-01, 6.0901e+00,\n",
       "          4.8165e+00, 1.1442e+00, 4.5818e+00, 2.5690e+00, 0.0000e+00, 0.0000e+00,\n",
       "          3.4908e+00, 3.1832e+00, 2.3651e+01, 1.1408e+00, 2.2020e+00, 6.3292e+00,\n",
       "          6.9006e+00, 1.2207e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "         [7.2962e+01, 1.6543e+01, 7.8199e+00, 1.0215e+01, 6.0554e+00, 5.5412e+00,\n",
       "          5.9149e+00, 4.4323e+00, 4.4247e+00, 3.9789e+00, 3.8700e+00, 3.5335e+00,\n",
       "          3.5021e+00, 3.2587e+00, 3.4310e+00, 2.9314e+00, 2.8897e+00, 2.7793e+00,\n",
       "          2.6610e+00, 1.6273e+00, 2.4130e+00, 2.0974e+00, 1.8769e+00, 1.7321e+00,\n",
       "          1.6585e+00, 1.7086e+00, 1.6781e+00, 1.7274e+00, 1.5999e+00, 1.5225e+00,\n",
       "          1.5591e+00, 1.5028e+00, 1.5938e+00, 1.2622e+00, 1.2334e+00, 1.2357e+00,\n",
       "          1.2130e+00, 1.1630e+00, 1.0841e+00, 1.0851e+00, 9.3756e-01, 8.9039e-01,\n",
       "          8.3696e-01, 8.1374e-01, 8.0055e-01, 7.6175e-01, 7.2400e-01, 7.3806e-01,\n",
       "          7.8336e-01, 6.2637e-01, 5.1382e-01, 4.7905e-01, 4.5181e-01, 4.3525e-01,\n",
       "          4.2060e-01, 3.6744e-01, 2.8813e-01, 2.2890e-01, 1.6164e-01, 0.0000e+00,\n",
       "          0.0000e+00, 1.6391e-01, 0.0000e+00, 0.0000e+00, 6.8750e-02, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 7.9752e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0366e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.9740e-01, 2.1317e-01,\n",
       "          2.1570e-01, 4.0330e-01, 4.1336e-01, 0.0000e+00, 1.6010e+00, 3.1777e-01,\n",
       "          0.0000e+00, 2.6431e-01, 2.4836e-01, 1.5174e-01, 3.0844e-01, 0.0000e+00,\n",
       "          5.5018e-02, 4.6748e-01, 3.2315e-01, 7.9984e-01, 7.6310e-01, 9.4855e-01,\n",
       "          4.0916e-01, 1.0279e+00, 0.0000e+00, 1.1426e+00, 2.5775e-01, 1.6452e-01,\n",
       "          2.5083e+00, 4.0897e-01, 1.0074e+00, 2.4097e-01, 3.9956e-01, 2.5201e-01,\n",
       "          4.1883e-01, 2.1187e+00, 3.8853e+00, 2.1338e+00, 1.8309e-01, 1.4860e-01,\n",
       "          6.1955e-01, 4.4589e-01, 1.0028e+00, 0.0000e+00, 2.8201e+00, 1.0344e+00,\n",
       "          5.2597e-01, 1.8066e+00, 7.2049e-01, 2.8347e-01, 5.1903e-01, 1.9781e+00,\n",
       "          3.0557e+00, 2.2041e+00, 3.0854e+00, 2.2681e+00, 4.9521e+00, 3.6897e+00,\n",
       "          0.0000e+00, 2.3593e+00, 0.0000e+00],\n",
       "         [6.1855e+00, 2.2390e-01, 2.2696e+00, 4.2224e+01, 1.3236e+01, 1.9205e+01,\n",
       "          1.5310e+01, 1.1772e+01, 5.0272e+00, 1.0785e+01, 7.8147e+00, 6.3902e+00,\n",
       "          6.0722e+00, 5.1483e+00, 3.6571e+00, 3.2444e+00, 2.9045e+00, 7.5684e+00,\n",
       "          2.7915e+00, 2.5262e+00, 2.5849e+00, 2.4978e+00, 1.7189e-01, 2.1141e+00,\n",
       "          2.0086e+00, 1.4748e+00, 1.4288e+00, 8.7021e-01, 1.2167e+00, 0.0000e+00,\n",
       "          1.1613e+00, 1.0110e+00, 3.3888e-01, 7.0289e-01, 6.6200e-01, 6.4732e-01,\n",
       "          6.0245e-01, 5.5518e-01, 5.6841e-01, 5.3709e-01, 5.4326e-01, 4.2015e-01,\n",
       "          4.0192e-01, 0.0000e+00, 3.5512e-01, 3.5442e-01, 3.0506e-01, 0.0000e+00,\n",
       "          2.6010e-01, 2.3211e-01, 1.0673e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          1.2638e-01, 1.3040e-01, 0.0000e+00, 6.3599e-02, 0.0000e+00, 4.2071e-02,\n",
       "          4.5048e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          2.9613e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.3855e+00,\n",
       "          0.0000e+00, 2.5129e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          3.1713e-01, 4.4191e-01, 8.4293e-01, 4.4309e-01, 2.8354e-01, 0.0000e+00,\n",
       "          7.6715e-01, 1.8973e-01, 3.0909e-01, 2.0401e+00, 2.0587e-01, 2.1117e+00,\n",
       "          6.3851e-01, 1.0277e+00, 1.5418e+00, 5.6567e-01, 1.1284e-01, 1.4033e-01,\n",
       "          1.1442e+00, 1.7268e-01, 6.0877e-01, 8.5054e-01, 2.7486e-01, 8.3717e+00,\n",
       "          0.0000e+00, 3.0035e-01, 7.6633e-01, 7.9772e+00, 5.9011e+00, 1.4759e+00,\n",
       "          1.5210e+00, 1.3700e+00, 4.2716e-01, 0.0000e+00, 2.7519e-01, 1.3584e+00,\n",
       "          8.1038e-01, 9.9819e-01, 8.2040e-01, 9.5610e-01, 7.5878e+00, 1.9914e+00,\n",
       "          1.6606e+00, 2.1465e+00, 2.0106e+01, 4.1094e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00, 0.0000e+00, 0.0000e+00]]),\n",
       " 'eta': tensor([[-6.0218e-01,  1.7781e-01, -6.5972e-01, -3.4017e-01, -6.1056e-02,\n",
       "          -1.2473e+00,  1.1807e-01, -4.3533e-01, -1.3823e+00, -4.2811e-02,\n",
       "          -1.1539e+00,  2.2863e-01, -2.2490e-01,  1.6575e+00, -9.6438e-04,\n",
       "           1.1161e+00, -1.3951e+00,  1.9373e-01,  6.9095e-01,  6.0464e-01,\n",
       "          -1.2069e+00, -1.0357e+00, -1.1755e+00,  1.4577e-02,  2.5346e+00,\n",
       "           2.1683e-01, -1.3705e+00, -1.2862e+00,  1.2884e+00,  1.3075e-01,\n",
       "          -3.2276e-01, -1.4268e+00, -4.8229e-01, -2.4284e-01, -3.9586e-01,\n",
       "           5.8184e-02, -1.2744e+00,  8.1711e-01, -3.1366e-01, -1.1770e-01,\n",
       "           2.1464e+00, -5.4308e-01, -4.1940e-01, -1.7118e+00,  2.1433e+00,\n",
       "          -1.3780e+00,  1.1923e+00,  1.1523e-01, -1.4406e+00,  0.0000e+00,\n",
       "           1.4333e+00,  0.0000e+00, -4.4451e-01,  5.9723e-01,  0.0000e+00,\n",
       "           2.0438e+00,  2.2342e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00, -5.2797e-02,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00, -7.7629e-01,  0.0000e+00,  0.0000e+00, -6.1422e-01,\n",
       "          -5.5038e-01, -2.4725e-01, -1.5100e-01, -9.8615e-02,  1.7746e-01,\n",
       "           0.0000e+00,  1.1880e+00,  1.5046e+00,  0.0000e+00, -1.1676e+00,\n",
       "          -9.1515e-01, -8.0686e-01, -4.8074e-01,  2.9689e-01,  1.1871e+00,\n",
       "          -6.9680e-01, -5.0918e-01,  0.0000e+00, -1.0667e+00,  4.0641e-01,\n",
       "           9.4020e-01,  1.8452e+00,  2.8141e-03,  0.0000e+00,  5.8807e-01,\n",
       "           1.2099e-01,  1.0916e+00, -7.6134e-01, -7.5364e-01, -3.2945e-01,\n",
       "           0.0000e+00,  0.0000e+00, -3.6480e-02,  0.0000e+00,  2.1060e+00,\n",
       "          -1.1760e+00,  6.6434e-02,  3.5964e-02, -6.8438e-01, -6.5925e-01,\n",
       "           1.9971e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00],\n",
       "         [-1.6976e-01,  5.4885e-01,  9.2309e-01, -2.6094e-01, -1.5754e+00,\n",
       "           5.1874e-01, -4.9596e-01, -3.9558e-01, -1.3573e-01, -1.9456e-01,\n",
       "           5.7542e-01,  9.3951e-01,  1.4863e+00,  5.4241e-01, -1.0635e+00,\n",
       "          -3.8343e-01, -1.8995e+00,  1.1176e+00,  5.4230e-01, -1.4819e+00,\n",
       "          -1.2322e+00, -1.9945e+00, -1.0474e+00,  9.2041e-01,  1.3546e+00,\n",
       "           4.8656e-02, -1.4065e+00,  6.6060e-01, -5.2652e-01,  5.9507e-01,\n",
       "          -5.2774e-01, -1.5495e+00, -1.9052e+00, -1.2437e+00,  2.1291e+00,\n",
       "           8.1293e-01, -5.1736e-01,  1.6380e+00,  1.1589e+00,  1.1439e+00,\n",
       "          -2.6129e+00, -1.4248e+00, -2.5533e+00,  1.4963e-01, -1.0490e+00,\n",
       "          -2.2057e-01,  0.0000e+00,  9.9191e-02, -2.0595e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4099e+00,  0.0000e+00,\n",
       "          -1.7444e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.2387e-01,\n",
       "           0.0000e+00,  2.3851e-02,  0.0000e+00,  8.9095e-01,  9.4904e-01,\n",
       "           0.0000e+00,  1.2812e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           2.1321e+00,  0.0000e+00,  1.2103e+00, -1.5960e+00, -1.6999e+00,\n",
       "           0.0000e+00,  3.6563e-01,  6.9321e-01,  9.7652e-01,  0.0000e+00,\n",
       "           1.2112e+00, -1.3567e+00, -8.2009e-01, -8.0916e-01, -7.6819e-01,\n",
       "          -7.2124e-01,  2.2392e-01,  0.0000e+00,  0.0000e+00,  3.9879e-01,\n",
       "           5.9484e-01,  8.4884e-01,  9.3139e-01,  0.0000e+00,  6.8093e-01,\n",
       "           6.8603e-01,  0.0000e+00,  4.5736e-01,  0.0000e+00, -1.4273e+00,\n",
       "           6.6121e-01, -1.3628e+00, -4.9897e-01, -1.3123e+00,  0.0000e+00,\n",
       "           5.9831e-01,  9.3032e-01,  4.5029e-01,  5.2120e-01,  5.9130e-01,\n",
       "           9.6578e-01,  9.3767e-01,  8.8678e-01,  8.7392e-01,  8.6071e-01,\n",
       "          -2.7229e-01,  5.4250e-01,  4.8887e-01,  0.0000e+00,  0.0000e+00,\n",
       "          -2.0483e-01, -1.7632e-01, -2.2020e-01, -2.0696e-01,  5.2873e-01,\n",
       "           5.6401e-01,  5.0135e-01,  5.1801e-01,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00],\n",
       "         [-4.3977e-01,  9.0563e-01, -3.5700e-01,  2.3697e-01,  3.4972e-02,\n",
       "          -2.7193e-01, -5.4441e-01, -8.3889e-02, -1.9047e-01,  8.5180e-01,\n",
       "           1.0207e+00,  9.8689e-01,  8.5293e-01, -3.8203e-01,  4.3167e-01,\n",
       "           6.1325e-01,  9.0802e-01, -9.1908e-02, -9.7016e-02, -4.4354e-01,\n",
       "          -4.6982e-01, -5.0524e-01, -1.4933e-01, -4.1393e-01, -7.7310e-01,\n",
       "          -1.1089e-01, -3.9825e-01, -4.6510e-01,  8.7093e-02,  1.2169e+00,\n",
       "           1.5858e-01,  1.4419e-01, -2.3187e-01,  1.0851e+00,  3.9124e-01,\n",
       "          -3.6087e-01, -3.7015e-01, -3.3426e-01,  3.8771e-01, -1.1380e+00,\n",
       "           9.0760e-02,  1.1614e+00,  1.2147e+00, -2.6926e-01, -2.9472e-01,\n",
       "          -2.7922e-01,  5.5536e-01, -7.4209e-01, -1.8265e-01,  7.6982e-01,\n",
       "          -8.2556e-01,  9.3369e-01,  4.5837e-01, -2.2531e-01, -4.5297e-01,\n",
       "           1.4167e-01,  3.5725e-01, -2.4360e-01,  2.1219e-04,  0.0000e+00,\n",
       "           0.0000e+00, -1.5731e+00,  0.0000e+00,  0.0000e+00,  5.0309e-01,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.5306e-01,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  5.5761e-02,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00, -7.1504e-01, -6.2375e-01, -3.5449e-01,\n",
       "           5.2452e-02,  1.3046e-01,  0.0000e+00,  2.3353e-01,  2.8198e-01,\n",
       "           0.0000e+00,  4.5481e-01,  7.0368e-01,  8.8723e-01,  9.6490e-01,\n",
       "           0.0000e+00,  1.7015e+00,  1.1789e+00, -7.5005e-01, -5.8419e-01,\n",
       "          -5.2485e-01, -5.1498e-01, -3.8763e-01, -7.1703e-02,  0.0000e+00,\n",
       "           9.4400e-01, -1.1639e+00, -2.8474e-03,  1.0579e-01,  2.8430e-01,\n",
       "           3.2553e-01, -2.1639e-01,  2.2683e-01,  1.1387e+00, -6.4652e-01,\n",
       "          -5.6779e-01, -3.4187e-01, -2.3995e-01,  7.8409e-01,  1.0551e+00,\n",
       "          -1.3224e+00, -3.9085e-01,  4.7422e-01,  0.0000e+00,  1.0000e+00,\n",
       "          -4.0972e-01,  1.1385e+00,  1.9872e-01, -2.9088e-01, -5.4241e-01,\n",
       "          -1.3320e-02, -2.8924e-01, -1.9563e-01, -1.4567e-01,  6.4620e-02,\n",
       "           1.0716e-01,  8.3029e-01,  8.3417e-01,  0.0000e+00, -2.6480e-02,\n",
       "           0.0000e+00],\n",
       "         [ 5.2444e-01, -3.9961e-01, -1.1387e-01,  1.6433e-01, -2.0802e-01,\n",
       "          -2.0642e-01, -1.3894e-01,  1.7223e-01, -2.6823e-01,  4.0612e-01,\n",
       "          -1.6076e+00,  3.1588e-01,  3.8368e-01, -3.0499e-01, -2.4991e-01,\n",
       "           3.8431e-01,  6.0650e-01,  6.4997e-01,  1.4811e-01,  1.0201e-01,\n",
       "           8.0318e-01,  6.7861e-02, -1.3962e+00,  3.1391e-01, -9.4977e-02,\n",
       "          -1.1425e+00, -1.7253e-01,  5.3368e-01, -1.4011e+00,  0.0000e+00,\n",
       "           1.2376e-01, -1.4321e-01, -1.8328e+00,  1.1637e-01, -2.9778e-01,\n",
       "           5.3962e-01, -1.0252e+00, -1.4944e+00, -2.0121e-01, -1.2204e+00,\n",
       "          -1.1521e+00,  9.5248e-02,  1.0825e-01,  0.0000e+00, -1.3002e+00,\n",
       "           1.0390e-01, -1.4921e+00,  0.0000e+00,  5.2703e-01, -9.2036e-01,\n",
       "           2.2910e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.2686e-01,\n",
       "           7.9510e-01,  0.0000e+00, -1.2333e+00,  0.0000e+00, -2.2944e+00,\n",
       "          -1.1061e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00, -1.7544e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  1.5798e-01,  0.0000e+00,\n",
       "           3.0200e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00, -1.3934e+00, -1.0313e+00, -9.4318e-01, -6.3409e-01,\n",
       "          -3.0953e-01,  0.0000e+00,  2.3901e-01,  4.8622e-01,  6.0857e-01,\n",
       "           7.4785e-01, -1.0919e+00, -6.3143e-01, -5.4351e-01,  3.6886e-02,\n",
       "           3.4009e-01,  7.1866e-01, -5.8196e-01, -1.6805e-01,  5.2265e-02,\n",
       "           6.2445e-01,  1.5588e-01,  2.7493e-01, -1.1265e+00,  6.6546e-01,\n",
       "           0.0000e+00,  4.3432e-01, -1.6295e+00,  1.1179e-01, -1.6296e+00,\n",
       "          -2.6639e-01, -1.0939e-01,  4.1132e-01, -1.6520e-01,  0.0000e+00,\n",
       "          -1.2952e+00,  2.4316e-01,  3.2419e-01, -1.1595e+00, -3.3013e-02,\n",
       "          -4.5831e-02, -2.2241e-01, -2.5676e-01, -1.9887e-01, -1.8398e-01,\n",
       "           2.0621e-01,  2.0598e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00]]),\n",
       " 'sin_phi': tensor([[-0.2892, -0.2581, -0.4719, -0.8828, -0.9074,  0.7548, -0.2169, -0.8640,\n",
       "           0.8142, -0.9110,  0.8408, -0.2997, -0.8773, -0.9089,  0.9951, -0.8812,\n",
       "           0.8743,  0.9993, -0.9460, -0.0402,  0.8715,  0.6703,  0.7687, -0.9819,\n",
       "          -0.9519,  0.9852,  0.4207,  0.8681, -0.8626, -0.9763, -0.1507,  0.7783,\n",
       "           0.9877, -0.9625, -0.7947,  1.0000,  0.9839, -0.2300,  0.9654, -0.9726,\n",
       "          -0.9940, -0.8190, -0.8069,  0.3596, -0.9951,  0.9303,  0.8398, -0.7550,\n",
       "           0.9994,  0.0000, -0.8472,  0.0000,  0.9997, -0.8491,  0.0000, -0.9278,\n",
       "          -0.9049,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.8858,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          -0.3638,  0.0000,  0.0000, -0.3389, -0.4068, -0.7114, -0.9220,  0.9957,\n",
       "          -1.0000,  0.0000,  0.0232, -0.1374,  0.0000, -0.8331, -0.6100, -0.2130,\n",
       "          -0.1621,  0.4356, -0.8284, -0.2090, -0.3219,  0.0000,  0.6490, -0.9957,\n",
       "           0.1865, -0.5837,  0.9797,  0.0000, -0.6860, -0.5706, -0.9996, -0.3434,\n",
       "          -0.5509, -0.7591,  0.0000,  0.0000, -0.8487,  0.0000, -0.7697,  0.7364,\n",
       "           0.9985,  0.9998, -0.4824, -0.4656, -0.9878,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [-0.5084, -1.0000,  0.7986, -0.4908,  0.5901, -0.9948,  0.9882,  0.9996,\n",
       "          -0.5618, -0.5218, -0.9984,  0.7223,  0.6829, -0.9972,  0.7214,  0.1185,\n",
       "          -0.9281,  0.7274, -0.9992,  0.9389,  0.3588,  0.6279,  0.6595,  0.9963,\n",
       "           0.9900, -0.4501,  0.8531,  0.2597,  0.8949,  0.4815, -0.3444, -0.5446,\n",
       "          -0.9305, -0.5754,  0.8360, -0.9762, -0.4915,  0.5888, -0.3956, -0.5058,\n",
       "          -0.3293,  0.9684, -0.2302,  0.6226,  0.8482, -0.2441,  0.0000, -0.8069,\n",
       "           0.5182,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.5915,  0.0000,  0.1557,  0.0000,  0.0000,\n",
       "           0.0000, -0.4748,  0.0000,  0.1124,  0.0000,  0.7546, -0.9312,  0.0000,\n",
       "           0.9345,  0.0000,  0.0000,  0.0000,  0.4133,  0.0000,  0.6571,  0.7338,\n",
       "           0.6945,  0.0000,  0.9662,  0.7466,  0.7789,  0.0000,  0.7846,  0.7670,\n",
       "           0.5710, -0.3414, -0.5886, -0.8881, -0.8185,  0.0000,  0.0000,  0.7465,\n",
       "          -0.9982, -0.0495,  0.7441,  0.0000, -0.8514, -0.9953,  0.0000, -0.9951,\n",
       "           0.0000,  0.8123, -0.9830,  0.5546,  0.9934,  0.7289,  0.0000, -0.9874,\n",
       "           0.7914, -0.9955, -0.9983, -0.9996,  0.8659,  0.8608,  0.6874,  0.7863,\n",
       "           0.7446, -0.5611, -0.9996, -1.0000,  0.0000,  0.0000, -0.5637, -0.4914,\n",
       "          -0.5259, -0.5460, -0.9990, -0.9997, -0.9989, -0.9979,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.6914, -0.4789, -0.9767,  0.9871, -0.8503, -0.3324, -0.6135, -0.9581,\n",
       "          -0.9936, -0.3105, -0.4652, -0.3834, -0.4066, -0.9401,  0.9867,  0.9900,\n",
       "          -0.5281, -0.9563, -0.9555, -0.9677, -0.4546, -0.9513, -0.8937, -0.9955,\n",
       "          -0.9651, -0.8679, -0.9726, -0.9236, -0.9993,  0.0248,  0.9144, -0.8606,\n",
       "          -0.9748, -0.3371,  0.9847, -0.9720, -0.8676, -0.9629,  0.9801, -0.9385,\n",
       "          -0.9953, -0.7031, -0.2669, -0.2696, -0.9456, -0.7480,  0.3802, -0.8434,\n",
       "          -0.9999,  1.0000, -0.4083, -0.5678, -0.9359,  0.9984, -0.9327,  0.7768,\n",
       "           0.9547, -0.9483, -0.4758,  0.0000,  0.0000, -0.7991,  0.0000,  0.0000,\n",
       "          -0.4096,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000, -0.8281,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000, -0.9825,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          -0.8998, -0.3379, -0.9629, -0.1870, -0.9934,  0.0000, -0.9666,  0.9563,\n",
       "           0.0000,  0.6557,  0.9393,  0.9604,  0.6647,  0.0000,  0.1825,  0.8477,\n",
       "          -0.8342, -0.9980, -0.9562, -0.9717, -0.6730, -0.9394,  0.0000, -0.1060,\n",
       "          -0.8647,  0.3934, -0.9249,  0.9953, -0.6709, -0.4508, -0.8371,  0.9996,\n",
       "          -0.9836, -0.9992, -0.9927, -0.8748,  0.9943,  0.8524, -0.6781, -0.9866,\n",
       "           0.9984,  0.0000, -0.1557, -0.9436, -0.5042,  0.9838, -0.9342, -0.9826,\n",
       "          -0.9903, -0.8530, -0.8395, -0.8473, -0.9369, -0.9464, -0.4179, -0.4461,\n",
       "           0.0000, -0.9598,  0.0000],\n",
       "         [-0.4588, -0.9995, -0.9783,  0.0748,  0.8595,  0.8738,  0.8960,  0.0313,\n",
       "           0.9017, -0.9969,  0.2403, -0.9929, -0.9999,  0.9119,  0.8739, -0.9924,\n",
       "          -0.4981, -0.4967, -0.0401, -1.0000, -0.6843,  0.2308, -0.0048, -0.9425,\n",
       "          -0.9995,  0.2596,  0.3054,  0.2395,  0.4257,  0.0000, -0.9998, -0.9998,\n",
       "          -0.2306,  0.6140, -0.3410, -0.7539,  0.9935, -0.2069, -0.9894,  0.8362,\n",
       "           0.3286,  0.6052, -0.1257,  0.0000,  0.9969, -0.9454,  0.2367,  0.0000,\n",
       "          -0.7947,  0.9996, -0.9919,  0.0000,  0.0000,  0.0000,  0.1513, -0.0678,\n",
       "           0.0000, -0.9996,  0.0000, -0.6610, -0.4769,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000, -0.6154,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0813,  0.0000, -0.9997,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.2636,  0.8638, -0.6898, -0.4891,  0.6447,  0.0000, -0.4495, -0.5046,\n",
       "          -0.9958, -0.5908, -0.8674,  0.2831,  0.1873, -0.9961, -0.9459,  0.0035,\n",
       "          -0.3723, -0.4807, -0.9976, -0.2244, -0.3455, -0.1998, -0.1718, -0.5562,\n",
       "           0.0000, -0.8656, -0.7900,  0.0701,  0.2453,  0.7865,  0.9250, -0.9996,\n",
       "          -0.9938,  0.0000,  0.7947, -0.9915, -0.9934,  0.3126, -0.9997, -0.9999,\n",
       "           0.8804,  0.8780,  0.9069,  0.9236,  0.0465,  0.0317,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]]),\n",
       " 'cos_phi': tensor([[ 0.9573, -0.9661,  0.8816, -0.4698, -0.4203,  0.6560, -0.9762, -0.5035,\n",
       "           0.5806, -0.4125,  0.5413, -0.9540, -0.4800,  0.4169, -0.0987,  0.4727,\n",
       "           0.4854,  0.0380, -0.3240, -0.9992,  0.4904,  0.7421,  0.6396, -0.1893,\n",
       "           0.3065, -0.1714,  0.9072,  0.4964,  0.5059,  0.2162,  0.9886,  0.6279,\n",
       "          -0.1567, -0.2713, -0.6070,  0.0093,  0.1788, -0.9732, -0.2608, -0.2327,\n",
       "           0.1094, -0.5738, -0.5907,  0.9331,  0.0993,  0.3668,  0.5429,  0.6557,\n",
       "          -0.0337,  0.0000, -0.5313,  0.0000,  0.0240, -0.5283,  0.0000,  0.3731,\n",
       "           0.4255,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.4640,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          -0.9315,  0.0000,  0.0000,  0.9408,  0.9135, -0.7028, -0.3871,  0.0925,\n",
       "           0.0049,  0.0000, -0.9997, -0.9905,  0.0000,  0.5531,  0.7924,  0.9770,\n",
       "           0.9868, -0.9002,  0.5602,  0.9779,  0.9468,  0.0000,  0.7608,  0.0930,\n",
       "          -0.9825,  0.8120, -0.2006,  0.0000, -0.7276, -0.8212, -0.0268,  0.9392,\n",
       "           0.8345, -0.6510,  0.0000,  0.0000, -0.5289,  0.0000, -0.6384,  0.6766,\n",
       "          -0.0542, -0.0212,  0.8759,  0.8850,  0.1555,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.8611, -0.0096, -0.6019,  0.8713, -0.8073, -0.1017,  0.1532, -0.0274,\n",
       "           0.8273,  0.8530, -0.0558, -0.6916,  0.7305, -0.0747, -0.6925,  0.9930,\n",
       "          -0.3722, -0.6862,  0.0397, -0.3443, -0.9334, -0.7783, -0.7517,  0.0858,\n",
       "          -0.1409,  0.8930, -0.5218,  0.9657,  0.4462, -0.8765, -0.9388,  0.8387,\n",
       "          -0.3663, -0.8179,  0.5487, -0.2170,  0.8709,  0.8083, -0.9184, -0.8626,\n",
       "           0.9442, -0.2493, -0.9731, -0.7826,  0.5296, -0.9697,  0.0000,  0.5906,\n",
       "          -0.8553,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000, -0.8063,  0.0000, -0.9878,  0.0000,  0.0000,\n",
       "           0.0000,  0.8801,  0.0000,  0.9937,  0.0000, -0.6562,  0.3645,  0.0000,\n",
       "          -0.3559,  0.0000,  0.0000,  0.0000,  0.9106,  0.0000,  0.7538, -0.6793,\n",
       "          -0.7195,  0.0000, -0.2579, -0.6653, -0.6272,  0.0000,  0.6200, -0.6417,\n",
       "          -0.8210,  0.9399,  0.8084, -0.4597, -0.5745,  0.0000,  0.0000, -0.6654,\n",
       "           0.0594,  0.9988, -0.6681,  0.0000, -0.5245, -0.0973,  0.0000,  0.0994,\n",
       "           0.0000, -0.5832, -0.1839, -0.8321,  0.1146, -0.6846,  0.0000,  0.1580,\n",
       "          -0.6113, -0.0950,  0.0580, -0.0286, -0.5002, -0.5090, -0.7263, -0.6179,\n",
       "          -0.6675,  0.8277,  0.0272,  0.0093,  0.0000,  0.0000,  0.8260,  0.8709,\n",
       "           0.8506,  0.8378, -0.0448, -0.0234, -0.0469, -0.0652,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.7225, -0.8778, -0.2145,  0.1600,  0.5263,  0.9431,  0.7897,  0.2864,\n",
       "          -0.1128, -0.9506, -0.8852, -0.9236, -0.9136,  0.3409, -0.1627, -0.1413,\n",
       "          -0.8492,  0.2925,  0.2951, -0.2520,  0.8907, -0.3083,  0.4486, -0.0944,\n",
       "           0.2618,  0.4967, -0.2326, -0.3833, -0.0379, -0.9997,  0.4049,  0.5093,\n",
       "          -0.2233, -0.9415, -0.1743,  0.2350, -0.4973, -0.2700, -0.1987,  0.3452,\n",
       "           0.0969, -0.7111, -0.9637,  0.9630, -0.3254,  0.6637, -0.9249, -0.5373,\n",
       "          -0.0167, -0.0039,  0.9128, -0.8232, -0.3522,  0.0559, -0.3607, -0.6297,\n",
       "          -0.2976, -0.3174, -0.8795,  0.0000,  0.0000,  0.6012,  0.0000,  0.0000,\n",
       "           0.9123,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.5605,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.1863,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          -0.4363,  0.9412,  0.2699, -0.9824, -0.1148,  0.0000,  0.2562, -0.2925,\n",
       "           0.0000, -0.7550, -0.3431, -0.2787, -0.7471,  0.0000, -0.9832, -0.5305,\n",
       "           0.5515, -0.0625, -0.2927, -0.2363,  0.7397,  0.3428,  0.0000, -0.9944,\n",
       "           0.5023, -0.9194,  0.3802,  0.0963,  0.7415,  0.8926, -0.5470,  0.0288,\n",
       "           0.1805,  0.0399, -0.1208,  0.4844, -0.1067, -0.5228,  0.7349, -0.1632,\n",
       "          -0.0574,  0.0000, -0.9878, -0.3310, -0.8636,  0.1791, -0.3567,  0.1859,\n",
       "           0.1386,  0.5219, -0.5433, -0.5312,  0.3495,  0.3230, -0.9085, -0.8950,\n",
       "           0.0000,  0.2806,  0.0000],\n",
       "         [ 0.8885, -0.0308,  0.2071, -0.9972,  0.5111,  0.4862,  0.4441, -0.9995,\n",
       "           0.4324, -0.0785,  0.9707, -0.1190, -0.0161,  0.4105,  0.4861, -0.1233,\n",
       "           0.8671,  0.8679, -0.9992, -0.0036,  0.7292, -0.9730,  1.0000,  0.3343,\n",
       "           0.0313,  0.9657,  0.9522,  0.9709,  0.9049,  0.0000, -0.0204, -0.0198,\n",
       "           0.9730,  0.7893,  0.9401,  0.6570,  0.1139,  0.9784, -0.1452,  0.5484,\n",
       "           0.9445, -0.7961, -0.9921,  0.0000,  0.0783, -0.3260,  0.9716,  0.0000,\n",
       "          -0.6071, -0.0278,  0.1268,  0.0000,  0.0000,  0.0000,  0.9885,  0.9977,\n",
       "           0.0000,  0.0267,  0.0000, -0.7504,  0.8789,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.7882,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000, -0.9967,  0.0000,  0.0225,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.9646,  0.5038,  0.7240,  0.8722,  0.7644,  0.0000,  0.8933,  0.8633,\n",
       "           0.0918,  0.8068,  0.4975,  0.9591,  0.9823, -0.0881,  0.3244,  1.0000,\n",
       "          -0.9281,  0.8769,  0.0697, -0.9745,  0.9384, -0.9798,  0.9851,  0.8310,\n",
       "           0.0000,  0.5008,  0.6131, -0.9975,  0.9695,  0.6175,  0.3798,  0.0287,\n",
       "          -0.1108,  0.0000,  0.6070,  0.1304,  0.1143,  0.9499,  0.0245, -0.0141,\n",
       "           0.4741,  0.4786,  0.4213,  0.3834, -0.9989, -0.9995,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]]),\n",
       " 'energy': tensor([[ 7.5050, 25.3286, 12.9586, 11.4953,  8.1700, 15.2607,  7.5203,  6.7629,\n",
       "          12.0385,  4.9845,  7.5609,  3.7163,  3.9369,  9.4970,  3.4220,  4.8638,\n",
       "           5.7331,  2.6629,  2.9638,  2.8153,  4.2680,  3.3294,  3.3669,  1.7824,\n",
       "          10.9266,  1.5375,  2.9801,  2.7428,  2.6059,  1.3320,  1.3429,  2.3528,\n",
       "           1.1045,  0.8687,  1.0312,  0.9113,  1.9678,  0.9012,  0.6953,  0.6292,\n",
       "           2.2342,  0.5196,  0.4245,  1.1445,  1.5034,  0.6780,  0.5196,  0.2369,\n",
       "           0.5630,  0.0000,  0.4955,  0.0000,  0.2250,  0.1942,  0.0000, 14.6744,\n",
       "           2.5521,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1387,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.1147,  0.0000,  0.0000,  6.6448,  1.5206,  0.6126,  0.2519,  2.7676,\n",
       "           0.6981,  0.0000,  1.6041,  0.9632,  0.0000,  0.5069,  0.7147,  1.4516,\n",
       "           0.4973,  0.1312,  0.3138,  1.7397,  0.3372,  0.0000,  1.6341,  0.4755,\n",
       "           0.2211,  0.3299,  0.2715,  0.0000,  1.3857,  0.1894,  5.7390,  2.8719,\n",
       "           2.3980,  4.9739,  0.0000,  0.0000,  1.4632,  0.0000,  0.6747,  1.2059,\n",
       "          17.8883, 30.2746,  3.3059,  6.1028,  3.2223,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 9.6781,  6.7462, 20.5044,  9.2351, 22.8818,  2.7882,  5.9032,  5.9718,\n",
       "           1.8788,  4.0509,  8.8488,  5.0099,  6.6340,  6.3320,  2.8273,  1.5495,\n",
       "           4.6015,  2.1784,  4.9353,  2.9418,  2.3625,  4.8500,  1.7241,  1.8262,\n",
       "           2.3189,  0.9960,  1.9810,  1.0378,  0.9910,  0.8972,  1.2993,  1.6285,\n",
       "           2.0899,  0.9865,  2.2791,  1.1521,  0.5048,  1.1684,  0.5849,  0.5794,\n",
       "           1.9844,  0.8398,  1.7726,  0.2840,  0.3717,  0.2151,  0.0000,  0.1609,\n",
       "           6.3489,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.1242,  0.0000,  0.6538,  0.0000,  0.0000,\n",
       "           0.0000,  0.2949,  0.0000,  0.3355,  0.0000, 10.7263,  0.6930,  0.0000,\n",
       "           2.0324,  0.0000,  0.0000,  0.0000,  0.7632,  0.0000,  0.9343,  2.7554,\n",
       "           0.5862,  0.0000,  0.2492,  0.6320,  2.1254,  0.0000,  1.6903,  2.6567,\n",
       "           0.5517,  0.1035,  1.1816,  0.2793,  0.2832,  0.0000,  0.0000,  0.3916,\n",
       "           4.3941,  0.7353,  6.6413,  0.0000,  0.3787,  2.7330,  0.0000,  2.1049,\n",
       "           0.0000,  2.9877,  0.2730,  0.4131, 31.0882,  6.6551,  0.0000,  0.5889,\n",
       "           1.0276,  0.2793,  0.5773,  0.7134,  1.3602,  0.9629,  1.1133,  8.5715,\n",
       "           6.7140,  1.1869,  5.2727,  2.8824,  0.0000,  0.0000,  3.5643,  3.2328,\n",
       "          24.2315,  1.1653,  2.5170,  7.3628,  7.7862, 13.8904,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [80.1315, 23.8034,  8.3762, 10.5144,  6.0607,  5.7473,  6.8147,  4.4501,\n",
       "           4.6019,  5.5135,  6.0689,  5.4003,  4.8571,  3.5341,  3.7879,  3.5348,\n",
       "           4.1942,  2.7946,  2.6772,  1.7955,  2.7293,  2.3749,  1.9030,  1.9463,\n",
       "           2.1838,  1.7248,  1.8183,  1.9227,  1.6120,  2.7996,  1.6542,  1.5967,\n",
       "           1.6427,  2.0858,  1.3363,  1.3244,  1.3045,  1.2365,  1.1749,  1.9310,\n",
       "           0.9517,  1.5677,  1.5405,  0.8549,  0.9705,  0.9330,  0.8501,  0.9610,\n",
       "           0.8086,  0.8331,  0.7129,  0.7172,  0.5192,  0.4587,  0.4850,  0.3965,\n",
       "           0.5812,  0.2739,  0.2136,  0.0000,  0.0000,  0.4352,  0.0000,  0.0000,\n",
       "           0.1597,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.9226,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  1.0476,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.5034,  0.2560,  0.2294,  0.4039,  0.4169,  0.0000,  1.6449,  0.3305,\n",
       "           0.0000,  0.2921,  0.3128,  0.2155,  0.4635,  0.0000,  0.1558,  0.8317,\n",
       "           0.4184,  0.9402,  0.8706,  1.0771,  0.4403,  1.0306,  0.0000,  1.6907,\n",
       "           0.4529,  0.1645,  2.5262,  0.4256,  1.0615,  0.2466,  0.4104,  0.4338,\n",
       "           0.5095,  2.4695,  4.1145,  2.1955,  0.2423,  0.2393,  1.2450,  0.4804,\n",
       "           1.1176,  0.0000,  4.3517,  1.1224,  0.9053,  1.8424,  0.7513,  0.3262,\n",
       "           0.5191,  2.0615,  3.1143,  2.2276,  3.0918,  2.2812,  6.7594,  5.0496,\n",
       "           0.0000,  2.3642,  0.0000],\n",
       "         [ 7.0566,  0.2794,  2.4695, 42.7961, 13.5325, 19.6381, 15.4867, 11.9568,\n",
       "           5.2110, 11.6874, 20.3054,  6.7131,  6.5262,  5.3914,  3.7745,  3.4897,\n",
       "           3.4581,  9.2242,  2.8650,  2.5432,  3.4673,  2.5075,  0.3940,  2.2734,\n",
       "           2.2251,  2.5506,  1.4568,  1.1126,  2.6234,  0.0000,  1.1785,  1.0214,\n",
       "           1.4354,  0.7213,  0.7055,  0.7569,  1.0687,  1.3900,  0.5965,  0.9990,\n",
       "           0.9557,  0.4445,  0.4277,  0.0000,  0.7138,  0.3827,  0.7261,  0.0000,\n",
       "           0.3282,  0.3653,  0.1068,  0.0000,  0.0000,  0.0000,  0.2008,  0.2229,\n",
       "           0.0000,  0.1830,  0.0000,  0.2108,  0.7681,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0881,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  9.5039,  0.0000,  2.6284,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.6781,  0.6985,  1.2465,  0.5352,  0.2972,  0.0000,  0.7892,  0.2126,\n",
       "           0.3681,  2.6842,  0.3413,  2.5470,  0.7352,  1.0284,  1.6318,  0.7181,\n",
       "           0.1325,  0.1423,  1.1457,  0.2075,  0.6162,  0.8829,  0.4685, 10.2957,\n",
       "           0.0000,  0.3293,  2.2544,  8.0283, 15.6596,  1.5285,  1.5301,  1.4875,\n",
       "           0.4330,  0.0000,  0.5401,  1.3987,  0.8533,  1.7478,  0.8208,  0.9571,\n",
       "           7.7763,  2.0574,  1.6935,  2.1829, 20.5354,  4.1968,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]]),\n",
       " 'phi': tensor([[-0.2934, -2.8805, -0.4915, -2.0598, -2.0046,  0.8553, -2.9230, -2.0984,\n",
       "           0.9513, -1.9960,  0.9988, -2.8372, -2.0714, -1.1408,  1.6696, -1.0785,\n",
       "           1.0639,  1.5328, -1.9008, -3.1014,  1.0582,  0.7347,  0.8768, -1.7613,\n",
       "          -1.2592,  1.7431,  0.4342,  1.0514, -1.0403, -1.3529, -0.1513,  0.8920,\n",
       "           1.7281, -1.8455, -2.2231,  1.5615,  1.3910, -2.9096,  1.8347, -1.8056,\n",
       "          -1.4612, -2.1819, -2.2027,  0.3678, -1.4713,  1.1952,  0.9969, -0.8557,\n",
       "           1.6045,  0.0000, -2.1309,  0.0000,  1.5468, -2.1273,  0.0000, -1.1885,\n",
       "          -1.1312,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -2.0533,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          -2.7693,  0.0000,  0.0000, -0.3457, -0.4189, -2.3501, -1.9683,  1.4781,\n",
       "          -1.5659,  0.0000,  3.1183, -3.0037,  0.0000, -0.9848, -0.6561, -0.2147,\n",
       "          -0.1629,  2.6909, -0.9762, -0.2105, -0.3277,  0.0000,  0.7063, -1.4777,\n",
       "           2.9540, -0.6233,  1.7728,  0.0000, -2.3856, -2.5343, -1.5976, -0.3505,\n",
       "          -0.5835, -2.2797,  0.0000,  0.0000, -2.1281,  0.0000, -2.2632,  0.8277,\n",
       "           1.6250,  1.5920, -0.5034, -0.4843, -1.4147,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [-0.5334, -1.5804,  2.2167, -0.5131,  2.5104, -1.6727,  1.4170,  1.5982,\n",
       "          -0.5966, -0.5490, -1.6266,  2.3345,  0.7518, -1.6456,  2.3357,  0.1188,\n",
       "          -1.9522,  2.3270, -1.5311,  1.9223,  2.7746,  2.4627,  2.4215,  1.4849,\n",
       "           1.7122, -0.4669,  2.1198,  0.2627,  1.1082,  2.6393, -2.7900, -0.5759,\n",
       "          -1.9458, -2.5285,  0.9899, -1.7895, -0.5138,  0.6296, -2.7349, -2.6112,\n",
       "          -0.3356,  1.8227, -2.9093,  2.4696,  1.0127, -2.8950,  0.0000, -0.9389,\n",
       "           2.5969,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  2.5087,  0.0000,  2.9853,  0.0000,  0.0000,\n",
       "           0.0000, -0.4947,  0.0000,  0.1126,  0.0000,  2.2866, -1.1977,  0.0000,\n",
       "           1.9347,  0.0000,  0.0000,  0.0000,  0.4260,  0.0000,  0.7169,  2.3176,\n",
       "           2.3739,  0.0000,  1.8316,  2.2986,  2.2487,  0.0000,  0.9020,  2.2675,\n",
       "           2.5339, -0.3484, -0.6294, -2.0485, -2.1828,  0.0000,  0.0000,  2.2988,\n",
       "          -1.5114, -0.0495,  2.3024,  0.0000, -2.1230, -1.6683,  0.0000, -1.4713,\n",
       "           0.0000,  2.1934, -1.7557,  2.5537,  1.4559,  2.3249,  0.0000, -1.4121,\n",
       "           2.2284, -1.6659, -1.5128, -1.5994,  2.0946,  2.1048,  2.3837,  2.2368,\n",
       "           2.3016, -0.5958, -1.5436, -1.5615,  0.0000,  0.0000, -0.5988, -0.5137,\n",
       "          -0.5537, -0.5776, -1.6156, -1.5942, -1.6177, -1.6360,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.7634, -2.6422, -1.7870,  1.4101, -1.0165, -0.3388, -0.6605, -1.2804,\n",
       "          -1.6838, -2.8259, -2.6578, -2.7481, -2.7229, -1.2229,  1.7343,  1.7125,\n",
       "          -2.5852, -1.2739, -1.2713, -1.8255, -0.4719, -1.8842, -1.1056, -1.6653,\n",
       "          -1.3059, -1.0509, -1.8055, -1.9642, -1.6087,  3.1168,  1.1539, -1.0364,\n",
       "          -1.7960, -2.7978,  1.7460, -1.3335, -2.0913, -1.8442,  1.7708, -1.2184,\n",
       "          -1.4737, -2.3619, -2.8714, -0.2730, -1.9023, -0.8451,  2.7516, -2.1381,\n",
       "          -1.5875,  1.5747, -0.4206, -2.5377, -1.9307,  1.5149, -1.9398,  2.2520,\n",
       "           1.8730, -1.8938, -2.6457,  0.0000,  0.0000, -0.9258,  0.0000,  0.0000,\n",
       "          -0.4220,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000, -0.9758,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000, -1.3834,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          -2.0222, -0.3447, -1.2975, -2.9535, -1.6858,  0.0000, -1.3117,  1.8676,\n",
       "           0.0000,  2.4265,  1.9210,  1.8533,  2.4145,  0.0000,  2.9580,  2.1300,\n",
       "          -0.9867, -1.6333, -1.8678, -1.8094, -0.7382, -1.2209,  0.0000, -3.0354,\n",
       "          -1.0445,  2.7372, -1.1808,  1.4743, -0.7354, -0.4676, -2.1496,  1.5420,\n",
       "          -1.3893, -1.5309, -1.6919, -1.0651,  1.6777,  2.1210, -0.7452, -1.7347,\n",
       "           1.6282,  0.0000, -2.9853, -1.9081, -2.6132,  1.3908, -1.9355, -1.3838,\n",
       "          -1.4317, -1.0217, -2.1452, -2.1308, -1.2137, -1.2419, -2.7105, -2.6791,\n",
       "           0.0000, -1.2864,  0.0000],\n",
       "         [-0.4767, -1.6016, -1.3622,  3.0667,  1.0343,  1.0630,  1.1107,  3.1103,\n",
       "           1.1237, -1.6494,  0.2427, -1.6901, -1.5869,  1.1478,  1.0632, -1.6944,\n",
       "          -0.5214, -0.5198, -3.1015, -1.5744, -0.7536,  2.9087, -0.0048, -1.2299,\n",
       "          -1.5395,  0.2626,  0.3104,  0.2418,  0.4397,  0.0000, -1.5912, -1.5906,\n",
       "          -0.2327,  0.6611, -0.3480, -0.8539,  1.4566, -0.2085, -1.7165,  0.9904,\n",
       "           0.3349,  2.4916, -3.0156,  0.0000,  1.4924, -1.9029,  0.2390,  0.0000,\n",
       "          -2.2232,  1.5986, -1.4437,  0.0000,  0.0000,  0.0000,  0.1518, -0.0679,\n",
       "           0.0000, -1.5441,  0.0000, -2.4195, -0.4972,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000, -0.6629,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  3.0602,  0.0000, -1.5483,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.2668,  1.0428, -0.7612, -0.5110,  0.7007,  0.0000, -0.4662, -0.5289,\n",
       "          -1.4789, -0.6321, -1.0500,  0.2870,  0.1884, -1.6590, -1.2404,  0.0035,\n",
       "          -2.7601, -0.5014, -1.5011, -2.9153, -0.3527, -2.9404, -0.1727, -0.5898,\n",
       "           0.0000, -1.0463, -0.9109,  3.0714,  0.2478,  0.9052,  1.1812, -1.5421,\n",
       "          -1.6819,  0.0000,  0.9185, -1.4401, -1.4563,  0.3179, -1.5463, -1.5849,\n",
       "           1.0768,  1.0717,  1.1359,  1.1773,  3.0950,  3.1099,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]]),\n",
       " 'momentum': tensor([[[ 6.3097e+00, -6.0218e-01, -2.8924e-01,  9.5726e-01,  7.5050e+00],\n",
       "          [ 2.4933e+01,  1.7781e-01, -2.5812e-01, -9.6611e-01,  2.5329e+01],\n",
       "          [ 1.0572e+01, -6.5972e-01, -4.7193e-01,  8.8163e-01,  1.2959e+01],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 9.5393e+00, -1.6976e-01, -5.0844e-01,  8.6110e-01,  9.6781e+00],\n",
       "          [ 5.8424e+00,  5.4885e-01, -9.9995e-01, -9.5800e-03,  6.7462e+00],\n",
       "          [ 1.4071e+01,  9.2309e-01,  7.9855e-01, -6.0193e-01,  2.0504e+01],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 7.2962e+01, -4.3977e-01,  6.9137e-01,  7.2250e-01,  8.0132e+01],\n",
       "          [ 1.6543e+01,  9.0563e-01, -4.7894e-01, -8.7785e-01,  2.3803e+01],\n",
       "          [ 7.8199e+00, -3.5700e-01, -9.7673e-01, -2.1448e-01,  8.3762e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 2.3593e+00, -2.6480e-02, -9.5982e-01,  2.8060e-01,  2.3642e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 6.1855e+00,  5.2444e-01, -4.5881e-01,  8.8853e-01,  7.0566e+00],\n",
       "          [ 2.2390e-01, -3.9961e-01, -9.9953e-01, -3.0763e-02,  2.7937e-01],\n",
       "          [ 2.2696e+00, -1.1387e-01, -9.7832e-01,  2.0708e-01,  2.4695e+00],\n",
       "          ...,\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]]),\n",
       " 'p4': tensor([[[ 6.3097e+00,  2.4933e+01,  1.0572e+01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-6.0218e-01,  1.7781e-01, -6.5972e-01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-2.9344e-01, -2.8805e+00, -4.9148e-01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 7.5050e+00,  2.5329e+01,  1.2959e+01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 9.5393e+00,  5.8424e+00,  1.4071e+01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-1.6976e-01,  5.4885e-01,  9.2309e-01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-5.3337e-01, -1.5804e+00,  2.2167e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 9.6781e+00,  6.7462e+00,  2.0504e+01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 7.2962e+01,  1.6543e+01,  7.8199e+00,  ...,  0.0000e+00,\n",
       "            2.3593e+00,  0.0000e+00],\n",
       "          [-4.3977e-01,  9.0563e-01, -3.5700e-01,  ...,  0.0000e+00,\n",
       "           -2.6480e-02,  0.0000e+00],\n",
       "          [ 7.6339e-01, -2.6422e+00, -1.7870e+00,  ...,  0.0000e+00,\n",
       "           -1.2864e+00,  0.0000e+00],\n",
       "          [ 8.0132e+01,  2.3803e+01,  8.3762e+00,  ...,  0.0000e+00,\n",
       "            2.3642e+00,  0.0000e+00]],\n",
       " \n",
       "         [[ 6.1855e+00,  2.2390e-01,  2.2696e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 5.2444e-01, -3.9961e-01, -1.1387e-01,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [-4.7666e-01, -1.6016e+00, -1.3622e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00],\n",
       "          [ 7.0566e+00,  2.7937e-01,  2.4695e+00,  ...,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets_unpacked)\n",
    "targets_unpacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e193e773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds_unpacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "802a663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "px = targets_unpacked[\"pt\"] * targets_unpacked[\"cos_phi\"] * msk_true_particles\n",
    "py = targets_unpacked[\"pt\"] * targets_unpacked[\"sin_phi\"] * msk_true_particles\n",
    "pz = targets_unpacked[\"pt\"] * np.sinh(targets_unpacked[\"eta\"]) * msk_true_particles\n",
    "# phi = np.arctan2(targets_unpacked[\"sin_phi\"], targets_unpacked[\"cos_phi\"]) * msk_true_particles\n",
    "\n",
    "px_np = px.detach().cpu().numpy()\n",
    "py_np = py.detach().cpu().numpy()\n",
    "pz_np = pz.detach().cpu().numpy()\n",
    "# phi_np = phi.detach().cpu().numpy()\n",
    "\n",
    "# print(\"px_np\", px_np)\n",
    "# print(\"py_np\", py_np)\n",
    "# print(\"pz_np\", pz_np)\n",
    "# print(\"phi_np\", phi_np)\n",
    "\n",
    "true_mom = np.sqrt(np.sum(px_np, axis=1)**2 + np.sum(py_np, axis=1)**2 + np.sum(pz_np, axis=1)**2)\n",
    "\n",
    "E = np.sqrt(px_np**2 + py_np**2 + pz_np**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bc3a2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E Shape (4, 171)\n",
      "px Shape torch.Size([4, 171])\n",
      "py Shape torch.Size([4, 171])\n",
      "pz Shape torch.Size([4, 171])\n"
     ]
    }
   ],
   "source": [
    "# four-vectors\n",
    "# px_py_pz_E = np.column_stack((px_np, py_np, pz_np, E)) \n",
    "print(\"E Shape\", E.shape)\n",
    "print(\"px Shape\", px.shape)\n",
    "print(\"py Shape\", py.shape)\n",
    "print(\"pz Shape\", pz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6404372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastjet as fj\n",
    "import numpy as np\n",
    "import awkward\n",
    "\n",
    "# Four momentum \n",
    "px_np = targets_unpacked[\"pt\"] * targets_unpacked[\"cos_phi\"] * msk_true_particles\n",
    "py_np = targets_unpacked[\"pt\"] * targets_unpacked[\"sin_phi\"] * msk_true_particles\n",
    "pz_np = targets_unpacked[\"pt\"] * np.sinh(targets_unpacked[\"eta\"]) * msk_true_particles\n",
    "E_np = np.sqrt(px_np**2 + py_np**2 + pz_np**2)\n",
    "\n",
    "\n",
    "particles = []\n",
    "for ip in range(E.shape[0]):\n",
    "    for ix in range(E.shape[1]):\n",
    "        px_value = float(px[ip, ix])\n",
    "        py_value = float(py[ip, ix])\n",
    "        pz_value = float(pz[ip, ix])\n",
    "        E_value = float(E[ip, ix])\n",
    "        particle = fj.PseudoJet(px_value, py_value, pz_value, E_value)\n",
    "        particles.append(particle)\n",
    "\n",
    "# print(particles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43801c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--------------------------------------------------------------------------\n",
      "#                         FastJet release 3.4.1\n",
      "#                 M. Cacciari, G.P. Salam and G. Soyez                  \n",
      "#     A software package for jet finding and analysis at colliders      \n",
      "#                           http://fastjet.fr                           \n",
      "#\t                                                                      \n",
      "# Please cite EPJC72(2012)1896 [arXiv:1111.6097] if you use this package\n",
      "# for scientific work and optionally PLB641(2006)57 [hep-ph/0512210].   \n",
      "#                                                                       \n",
      "# FastJet is provided without warranty under the GNU GPL v2 or higher.  \n",
      "# It uses T. Chan's closest pair algorithm, S. Fortune's Voronoi code,\n",
      "# CGAL and 3rd party plugin jet algorithms. See COPYING file for details.\n",
      "#--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "num_threads = 2 \n",
    "\n",
    "def cluster_jets(particles):\n",
    "    jetdef = fj.JetDefinition(fj.antikt_algorithm, 0.4)\n",
    "    jet_ptcut = 20\n",
    "    \n",
    "    cluster = fj.ClusterSequence(particles, jetdef)\n",
    "    jets = cluster.inclusive_jets()\n",
    "    \n",
    "    return jets\n",
    "\n",
    "chunks = [particles[i::num_threads] for i in range(num_threads)]\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    futures = [executor.submit(cluster_jets, chunk) for chunk in chunks]\n",
    "\n",
    "gen_jets = []\n",
    "for future in futures:\n",
    "    gen_jets.extend(future.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e8e812f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jet 1 : 0.0 100000.0 0.0 0.0\n",
      "Jet 2 : 0.04207113714907656 -2.294446773446033 3.8637198096925522 0.21077144145965576\n",
      "Jet 3 : 0.05501835700795849 1.7015463977073388 2.9580240122352164 0.15583454072475433\n",
      "Jet 4 : 0.06359913036312283 -1.2333173096636556 4.73913413874164 0.1184191107749939\n",
      "Jet 5 : 0.08707761535144456 -0.7762948729314197 3.513933736499252 0.11465997993946075\n",
      "Jet 6 : 0.16184824428645617 2.105999764182302 4.020031974279469 0.6746672987937927\n",
      "Jet 7 : 0.31891328440639516 -0.11105964370541344 3.513577082753678 0.3253082036972046\n",
      "Jet 8 : 0.21458386583894404 1.4332742869180173 4.1522520120423305 0.4754031002521515\n",
      "Jet 9 : 0.2217507032288144 -1.7443558813760374 2.985267149371785 0.6538288593292236\n",
      "Jet 10 : 0.2321124434233074 -0.9203590660341586 1.598646164573508 0.3375573456287384\n",
      "Jet 11 : 0.24835511081562286 0.7036822729619935 1.9210313695238448 0.3124234676361084\n",
      "Jet 12 : 0.2577485722077771 -1.163860106491377 5.2386921087772995 0.45293498039245605\n",
      "Jet 13 : 0.2781750949943036 1.1923423068704357 0.9968781489718737 0.5004786252975464\n",
      "Jet 14 : 0.30034530069743154 0.43431702312299647 5.236915624367942 0.3291206657886505\n",
      "Jet 15 : 0.3089837736699326 -1.4247517107813972 1.822702642840171 0.6793616414070129\n",
      "Jet 16 : 0.3353711286793776 0.023850757581209435 0.11260699130040927 0.33546653389930725\n",
      "Jet 17 : 0.3995614617469292 0.22682844176267988 4.13358407624167 0.4098845422267914\n",
      "Jet 18 : 0.4345145482722507 1.638011463013049 0.6295636789053403 1.1600005626678467\n",
      "Jet 19 : 0.5199144456169271 -1.2436861114287652 3.754694995825206 0.9765797853469849\n",
      "Jet 20 : 0.5551752070625503 -1.4943951298275489 6.0747350667355855 1.2993943691253662\n",
      "Jet 21 : 0.6170006276064371 -0.717301060072308 4.2516043334345 0.7826946079730988\n",
      "Jet 22 : 0.7028880223287185 0.1163704046995785 0.6611376835793746 0.7076526880264282\n",
      "Jet 23 : 1.02303948428892 0.21836322558080104 2.4183918756723 1.0609202980995178\n",
      "Jet 24 : 1.2743358768496909 -1.4484442191171827 5.625274969054597 2.8675225973129272\n",
      "Jet 25 : 0.9849795569429793 0.04865605950386703 5.816330055952669 0.9861456751823425\n",
      "Jet 26 : 1.042928911640276 1.1566694092176293 3.094838401461697 1.8251819163560867\n",
      "Jet 27 : 1.097895344521749 0.5880709377798199 3.8975730241016255 1.2932710647583008\n",
      "Jet 28 : 1.1373251200406427 -1.3153945543440824 1.43660970227516 2.2751007080078125\n",
      "Jet 29 : 1.2575065811574506 0.9030496493164013 1.5128324769406942 1.8090975731611252\n",
      "Jet 30 : 1.2892537099244488 -1.9945056019771892 2.4626810897891827 4.824806213378906\n",
      "Jet 31 : 1.4712527286515582 0.5764819082390292 2.6944536095804854 1.7248022556304932\n",
      "Jet 32 : 1.5083499575846988 1.2772844607542753 5.250303103209799 2.9159750938415527\n",
      "Jet 33 : 1.5591281126836538 0.15857561090880007 1.1539064618437656 1.5787723064422607\n",
      "Jet 34 : 1.7215108178147989 2.0742648093383873 4.84315742800573 6.959949970245361\n",
      "Jet 35 : 2.2581005608572973 2.472010488687421 5.054454846008232 13.47400712966919\n",
      "Jet 36 : 2.908402170965488 0.6836963471664088 0.1355388214087042 3.6527363806962967\n",
      "Jet 37 : 3.3687982395224023 -0.6895810427864131 5.074450641194458 4.259250685572624\n",
      "Jet 38 : 3.5359164891737693 -0.5361944577762896 0.21897021230577915 4.090039372444153\n",
      "Jet 39 : 3.416043876290439 1.0916008540918216 4.685554212556332 5.661611080169678\n",
      "Jet 40 : 3.8825381712271994 -1.237336640456044 0.33935693388217064 7.271487474441528\n",
      "Jet 41 : 7.299106915744186 -0.40242517292399066 1.6353208045610668 7.9235256016254425\n",
      "Jet 42 : 8.476609192278369 -1.4134477695507293 2.2420718413413483 18.512095786631107\n",
      "Jet 43 : 18.788187735636516 0.043254206964596094 5.133040631428727 19.063022553920746\n",
      "Jet 44 : 18.780285676203 -1.2690709533890752 0.9746570296017495 36.14975321292877\n",
      "Jet 45 : 19.744924007813818 0.9444385678554738 3.556008852947669 29.40788620710373\n",
      "Jet 46 : 19.745749378053734 0.90910127231791 2.2784425613422883 28.533161997795105\n",
      "Jet 47 : 19.839232890687864 0.6477512438964848 5.700999229539193 24.25202701240778\n",
      "Jet 48 : 28.537747711204815 0.14789784625473612 1.6582169355748688 29.253798633813858\n",
      "Jet 49 : 45.01124918238766 -0.2332565785099703 4.402228747683926 47.36897757649422\n",
      "Jet 50 : 53.37940105293927 -0.4875592136962245 5.790761951145805 61.12583269178867\n",
      "Jet 51 : 76.76856715752227 0.45132233606240324 4.660197890174574 85.78119450807571\n",
      "Jet 52 : 101.83066676815662 -0.38362088818035145 0.8546048300171027 110.90552079677582\n",
      "Jet 53 : 85.92697527385364 0.15684496967386383 3.0977491866249824 87.37486910820007\n",
      "Jet 54 : 0.0 100000.0 0.0 0.0\n",
      "Jet 55 : 0.10169744764220608 1.845153211482335 5.659895689944242 0.32985925674438477\n",
      "Jet 56 : 0.12637612120965167 0.5268643786936352 0.15184136026634282 0.14432576298713684\n",
      "Jet 57 : 0.16451782030977624 -0.0028474336298882886 2.737240324091807 0.16451849043369293\n",
      "Jet 58 : 0.1783999868223402 2.1320979590328517 0.42604380043211065 0.7627605199813843\n",
      "Jet 59 : 0.18805895562169306 0.1209897482172013 3.7488659109470235 0.18943709135055542\n",
      "Jet 60 : 0.19011709394837692 0.11523289479179176 5.427516767729845 0.19138073921203613\n",
      "Jet 61 : 0.2733828959002636 -2.5533223269613976 3.3739061884624144 1.767085075378418\n",
      "Jet 62 : 0.27485640361008296 -1.1265336915117692 6.110523242429346 0.46850642561912537\n",
      "Jet 63 : 0.2835367888789636 -0.30953295148655185 0.7006866553053699 0.29722851514816284\n",
      "Jet 64 : 0.28872305670347825 -2.612906437373648 5.94760380407917 1.9794785976409912\n",
      "Jet 65 : 0.3388791125671199 -1.8327871651467533 6.050461713228733 1.0863198041915894\n",
      "Jet 66 : 0.7480805000309932 0.28291777114782585 2.3850991378727646 0.7944678366184235\n",
      "Jet 67 : 0.5336181601108777 2.129053021829971 0.9899397732222517 2.2747747898101807\n",
      "Jet 68 : 0.5432599842404727 -1.152100841241974 0.3348668125937018 0.9454915523529053\n",
      "Jet 69 : 0.6087743700865954 0.15587666178297238 5.9304566712903055 0.6161852478981018\n",
      "Jet 70 : 0.9548356780540413 0.5045095095914336 4.115121690011202 1.094080239534378\n",
      "Jet 71 : 0.7380580166235223 -0.7420893491529477 4.1451228341458854 0.9507801532745361\n",
      "Jet 72 : 0.9574316416352082 -1.1346606298729942 1.469888330428182 1.6478147506713867\n",
      "Jet 73 : 0.8903904929637404 1.1614091427143225 3.9212744874899363 1.561511754989624\n",
      "Jet 74 : 0.8994682024446952 -0.5346489052095236 3.4969117326842194 1.03128120303154\n",
      "Jet 75 : 0.9134892153240592 -1.4064674602042633 2.1197814971888778 1.976114273071289\n",
      "Jet 76 : 0.9589454414830244 -1.6250497952634884 5.377351859186035 2.530020050704479\n",
      "Jet 77 : 1.2831932985722951 -1.0083335992318794 5.613723900169586 2.0018266439437866\n",
      "Jet 78 : 1.2677771322541058 -1.232230255305446 2.774637716672085 2.3583884239196777\n",
      "Jet 79 : 2.064056179244706 -0.2923754971229614 0.2727115654858343 2.185307800769806\n",
      "Jet 80 : 1.5710387702907795 -1.1414348159110401 5.129191204439874 2.7150624990463257\n",
      "Jet 81 : 1.5885174105478397 -2.0594630124355673 2.596902390579283 6.329669952392578\n",
      "Jet 82 : 2.1792553514646014 1.309030121493734 1.785987184393621 4.34156596660614\n",
      "Jet 83 : 1.7767068131487591 0.0145773943314323 4.521917278449312 1.7768956422805786\n",
      "Jet 84 : 1.9260147470722389 1.2858796558736378 3.1511034309008648 3.7592684626579285\n",
      "Jet 85 : 1.9463034120077898 -1.9012868477056022 4.3329938322154256 6.660138130187988\n",
      "Jet 86 : 2.3659895473173798 0.6046391062951781 3.1817825491847045 2.811816453933716\n",
      "Jet 87 : 3.1950151799613393 -0.5708015756329099 4.739691698130938 3.7359575629234314\n",
      "Jet 88 : 3.215509807052886 -1.0314046072238954 2.389262826030728 5.093975067138672\n",
      "Jet 89 : 3.2871838533413524 1.0944130536088723 5.187844628135733 5.4653690457344055\n",
      "Jet 90 : 3.552894062635054 0.6426619413216196 1.6883097933440636 4.321450769901276\n",
      "Jet 91 : 7.222314270715945 1.8748902311679603 5.117773648336489 24.140403747558594\n",
      "Jet 92 : 4.2818745475295446 1.4031954199902137 0.7799355398915089 9.25709444284439\n",
      "Jet 93 : 9.286003052872795 -1.5785380002273595 2.5073844074076015 23.467744946479797\n",
      "Jet 94 : 12.368439791326269 0.5920491424888761 5.739485358117592 14.764762103557587\n",
      "Jet 95 : 14.09038555423043 -1.2387272783078902 0.8693557823079305 26.428491920232773\n",
      "Jet 96 : 16.18462738188143 -0.6806735727060572 5.899145661486473 20.183139711618423\n",
      "Jet 97 : 16.058404518472518 -1.597612046656773 0.2599924868335913 41.32641136646271\n",
      "Jet 98 : 27.92339907518513 0.9239243850951402 2.2439666134014007 40.77158997952938\n",
      "Jet 99 : 33.03304301836021 -0.10594402594130976 5.021030228983534 33.96005191653967\n",
      "Jet 100 : 31.45727196319845 0.9143085504200839 3.576630390674567 45.696461379528046\n",
      "Jet 101 : 32.819254166143814 -0.4985346347900442 1.4497223033745938 36.985801696777344\n",
      "Jet 102 : 42.83204449113894 -0.331546086749176 4.264629240789468 46.10479111969471\n",
      "Jet 103 : 51.661615323385796 0.19136284921399987 3.2740407395062143 53.20751166343689\n",
      "Jet 104 : 40.18225223164555 0.500580335278864 4.717396094073167 45.62491166591644\n",
      "Jet 105 : 46.788335784418834 -0.21831971195994884 5.757639307089132 48.147045865654945\n",
      "Jet 106 : 48.822975916059356 -0.20080270352947288 1.085529003598926 49.94383490085602\n",
      "Jet 107 : 51.22031082148975 0.09174905804508235 1.5447662354199896 51.92986208200455\n"
     ]
    }
   ],
   "source": [
    "for i, jet in enumerate(gen_jets):\n",
    "    print(\"Jet\", i+1, \":\", jet.pt(), jet.eta(), jet.phi(), jet.e())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d04dad18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Jets: 107\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Jets:\", len(gen_jets))\n",
    "\n",
    "\n",
    "gen_jet_pt = [jet.pt() for jet in gen_jets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fc77189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jet pT values in bin 1: [18.788187735636516, 18.780285676203, 19.744924007813818, 19.745749378053734, 19.839232890687864, 12.368439791326269, 14.09038555423043, 16.18462738188143, 16.058404518472518]\n",
      "Jet pT values in bin 2: [28.537747711204815, 27.92339907518513]\n",
      "Jet pT values in bin 3: [33.03304301836021, 31.45727196319845, 32.819254166143814]\n",
      "Jet pT values in bin 4: [45.01124918238766, 53.37940105293927, 42.83204449113894, 51.661615323385796, 40.18225223164555, 46.788335784418834, 48.822975916059356, 51.22031082148975]\n",
      "Jet pT values in bin 5: [76.76856715752227]\n",
      "Jet pT values in bin 6: [85.92697527385364]\n",
      "Jet pT values in bin 7: [101.83066676815662]\n"
     ]
    }
   ],
   "source": [
    "import ROOT\n",
    "import numpy as np\n",
    "\n",
    "bins = [10, 20, 30, 40, 60, 80, 100, 200]\n",
    "\n",
    "x_vals = []\n",
    "ratio_iqr_median = []\n",
    "\n",
    "# Iterating over bins\n",
    "for i in range(len(bins) - 1):\n",
    "    lim_low = bins[i]\n",
    "    lim_hi = bins[i + 1]\n",
    "    x_vals.append(np.mean([lim_low, lim_hi]))\n",
    "\n",
    "    # Filtering out jet pT values within each bin\n",
    "    jet_values_in_bin = [pt for pt in gen_jet_pt if lim_low < pt <= lim_hi]\n",
    "    print(\"Jet pT values in bin {}: {}\".format(i+1, jet_values_in_bin))  # Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10b51263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ROOT\n",
    "import numpy as np\n",
    "\n",
    "# Define bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100, 200]\n",
    "\n",
    "# Initialize lists to store values\n",
    "x_vals = []\n",
    "ratio_iqr_median = []\n",
    "\n",
    "# Iterate over bins\n",
    "for i in range(len(bins) - 1):\n",
    "    lim_low = bins[i]\n",
    "    lim_hi = bins[i + 1]\n",
    "    x_vals.append(np.mean([lim_low, lim_hi]))\n",
    "\n",
    "    # Filter jet pT values within the bin\n",
    "    jet_values_in_bin = [pt for pt in gen_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Check if there are any jet pT values in the bin\n",
    "    if len(jet_values_in_bin) == 0:\n",
    "        # Append NaN values\n",
    "        ratio_iqr_median.append(np.nan)\n",
    "        continue  # Skip calculation for this bin\n",
    "\n",
    "    #  IQR and median for the jet pT values\n",
    "    jet_iqr = np.percentile(jet_values_in_bin, 75) - np.percentile(jet_values_in_bin, 25)\n",
    "    jet_median = np.median(jet_values_in_bin)\n",
    "    ratio_iqr_median_ratio = jet_iqr / jet_median\n",
    "    ratio_iqr_median.append(ratio_iqr_median_ratio)\n",
    "\n",
    "#  We need to filter out NaN values,ROOT unable to print it\n",
    "x_vals_filtered = [x for x, y in zip(x_vals, ratio_iqr_median) if not np.isnan(y)]\n",
    "ratio_iqr_median_filtered = [y for y in ratio_iqr_median if not np.isnan(y)]\n",
    "\n",
    "# Create a TGraph with filtered values\n",
    "gr_ratio = ROOT.TGraph(len(x_vals_filtered), np.array(x_vals_filtered), np.array(ratio_iqr_median_filtered))\n",
    "# Set the titles to empty strings\n",
    "gr_ratio.SetTitle(\"\")\n",
    "\n",
    "# Create a canvas\n",
    "canvas = ROOT.TCanvas(\"canvas\", \"Ratio IQR/Median vs Jet pT\", 800, 600)\n",
    "\n",
    "# Draw the graph with connecting lines\n",
    "gr_ratio.SetMarkerStyle(20)\n",
    "gr_ratio.SetMarkerColor(ROOT.kBlue)\n",
    "gr_ratio.SetLineColor(ROOT.kBlue)\n",
    "gr_ratio.GetXaxis().SetTitle(\"Jet pT (GeV)\")\n",
    "gr_ratio.GetYaxis().SetTitle(\"Response IQR/Median\")\n",
    "gr_ratio.Draw(\"APL\")\n",
    "\n",
    "# Add legend\n",
    "legend = ROOT.TLegend(0.75, 0.75, 0.9, 0.9)\n",
    "legend.AddEntry(gr_ratio, \"Jet pT\", \"p\")\n",
    "legend.Draw()\n",
    "\n",
    "# Show the canvas\n",
    "canvas.Draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59f5f143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.19629730627647413,\n",
       " 0.010880909634087955,\n",
       " 0.02400680782056452,\n",
       " 0.14358528955008465,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_iqr_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0468d700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reconstructed particles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0be43ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_px = preds_unpacked[\"pt\"] * preds_unpacked[\"cos_phi\"] * msk_pred_particles\n",
    "reco_py = preds_unpacked[\"pt\"] * preds_unpacked[\"sin_phi\"] * msk_pred_particles\n",
    "reco_pz = preds_unpacked[\"pt\"] * np.sinh(preds_unpacked[\"eta\"]) * msk_pred_particles\n",
    "reco_phi = np.arctan2(preds_unpacked[\"sin_phi\"], preds_unpacked[\"cos_phi\"]) * msk_pred_particles\n",
    "\n",
    "reco_px_np = reco_px.detach().cpu().numpy()\n",
    "reco_py_np = reco_py.detach().cpu().numpy()\n",
    "reco_pz_np = reco_pz.detach().cpu().numpy()\n",
    "reco_phi_np = reco_phi.detach().cpu().numpy()\n",
    "\n",
    "# print(\"reco_px_np\", reco_px_np)\n",
    "# print(\"reco_py_np\", reco_py_np)\n",
    "# print(\"reco_pz_np\", reco_pz_np)\n",
    "# print(\"reco_phi_np\", reco_phi_np)\n",
    "\n",
    "reco_pred_mom = np.sqrt(np.sum(reco_px_np, axis=1)**2 + np.sum(reco_py_np, axis=1)**2 + np.sum(reco_pz_np, axis=1)**2)\n",
    "\n",
    "reco_E_np = np.sqrt(reco_px_np**2 + reco_py_np**2 + reco_pz_np**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4726630d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E Shape torch.Size([4, 171])\n",
      "px Shape (4, 171)\n",
      "py Shape (4, 171)\n",
      "pz Shape (4, 171)\n"
     ]
    }
   ],
   "source": [
    "# four-vectors\n",
    "# px_py_pz_E = np.column_stack((px_np, py_np, pz_np, E)) \n",
    "print(\"E Shape\", E_np.shape)\n",
    "print(\"px Shape\", reco_px_np.shape)\n",
    "print(\"py Shape\", reco_py_np.shape)\n",
    "print(\"pz Shape\", reco_pz_np.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da2c45e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reco_particles = []   # TODO:Change this to reco_jets\n",
    "for ip in range(E_np.shape[0]):\n",
    "    for ix in range(E_np.shape[1]):\n",
    "        px_value = float(reco_px_np[ip, ix])\n",
    "        py_value = float(reco_py_np[ip, ix])\n",
    "        pz_value = float(reco_pz_np[ip, ix])\n",
    "        E_value = float(E_np[ip, ix])\n",
    "        particle = fj.PseudoJet(px_value, py_value, pz_value, E_value)\n",
    "        reco_particles.append(particle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7dc7e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reco_particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91fa6295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "num_threads = 2 \n",
    "\n",
    "# Function to perform jet clustering\n",
    "def cluster_jets(particles):\n",
    "    jetdef = fj.JetDefinition(fj.antikt_algorithm, 0.4)\n",
    "#     jet_ptcut = 20\n",
    "    \n",
    "    cluster = fj.ClusterSequence(reco_particles, jetdef)\n",
    "    jets = cluster.inclusive_jets()\n",
    "    \n",
    "    return jets\n",
    "\n",
    "# Spliting particles list into chunks to process in parallel\n",
    "chunks = [particles[i::num_threads] for i in range(num_threads)]\n",
    "\n",
    "# jet clustering in parallel\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    futures = [executor.submit(cluster_jets, chunk) for chunk in chunks]\n",
    "\n",
    "reco_jets = []\n",
    "for future in futures:\n",
    "    reco_jets.extend(future.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44dfe7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jet 1 : 0.0 100000.0 0.0 0.0\n",
      "Jet 2 : 0.0950697083959137 0.3628628347542296 2.9998606131379293 0.0\n",
      "Jet 3 : 0.10285790940421541 -1.2253477644889066 4.659921053400183 0.1184191107749939\n",
      "Jet 4 : 0.10850048722920232 -1.7060135924649558 2.968018956793744 0.6538288593292236\n",
      "Jet 5 : 0.16564941071427414 -0.02048923845243078 2.7183313278200614 0.16451849043369293\n",
      "Jet 6 : 0.17374009172510282 0.045118893388370426 2.3754815765812816 0.0\n",
      "Jet 7 : 0.2096423854829563 0.048009243700695425 5.601971087643939 0.0\n",
      "Jet 8 : 0.21192121915126003 0.9788329916149997 0.2262165387184454 76.81983505934477\n",
      "Jet 9 : 0.24111857686791913 1.4425063528149098 4.154812435301968 0.4754031002521515\n",
      "Jet 10 : 0.2941525094939013 -2.5831827399116403 3.367723864574657 1.767085075378418\n",
      "Jet 11 : 0.31061133690114884 0.4266132803348739 5.192565008105359 0.3291206657886505\n",
      "Jet 12 : 0.31391794486045227 -2.6036389911371534 5.9459254631223395 1.9794785976409912\n",
      "Jet 13 : 0.36429328967724917 -1.006166464796966 2.5356097425256023 0.9648559093475342\n",
      "Jet 14 : 0.37226246517661943 -1.1549577413185128 6.120138338836423 0.46850642561912537\n",
      "Jet 15 : 0.3787415923436054 0.5755902036240023 0.48230767792138535 0.0\n",
      "Jet 16 : 0.38653281626638053 2.253711112246054 5.411071961825065 0.0\n",
      "Jet 17 : 0.5367629008419899 -1.2452382237216593 3.7502384754583176 0.9765797853469849\n",
      "Jet 18 : 0.5444627040999427 2.138595365490534 0.9860134343475969 2.2747747898101807\n",
      "Jet 19 : 0.5467809015067274 0.048895802227414215 4.563121529906132 0.0\n",
      "Jet 20 : 0.6165422332785244 2.229216686454657 5.385199615781754 2.548323154449463\n",
      "Jet 21 : 0.70914800559446 -0.011246064602160074 3.26550237520587 0.0\n",
      "Jet 22 : 0.7283174415377948 0.12464634470953297 0.6555006730264432 0.8401409089565277\n",
      "Jet 23 : 1.5826021353344748 -1.6118978218031021 5.900730135983317 4.008252501487732\n",
      "Jet 24 : 0.7534065763060088 -0.7418842866788282 4.147667172512414 0.9507801532745361\n",
      "Jet 25 : 0.8644376091055408 0.795896912847082 3.654326153170259 0.0\n",
      "Jet 26 : 0.8757275896581077 -0.5333044258921966 3.4978611926456082 1.0134529620409012\n",
      "Jet 27 : 0.9778147611059227 -1.5391333797964553 2.16385408772488 0.5862389802932739\n",
      "Jet 28 : 1.262312363776102 1.1026118338150512 1.0415808008307295 0.0\n",
      "Jet 29 : 1.3195243213712193 1.1323768859350023 1.7487981767151246 2.1205899715423584\n",
      "Jet 30 : 1.5491310873951498 0.5775208096733784 2.6965336053584124 1.7248022556304932\n",
      "Jet 31 : 1.5696192610928885 -1.4826382638986888 1.899715673733497 3.617835581302643\n",
      "Jet 32 : 1.7544283283248618 -0.40151656941510494 4.671267438969489 0.0\n",
      "Jet 33 : 1.9576307120340888 -1.9078212375096606 4.332115596026477 6.660138130187988\n",
      "Jet 34 : 2.1639090766019557 -0.017575739041513164 4.114671241802958 1.9358714520931244\n",
      "Jet 35 : 2.6210870879857002 -1.2432001063154485 5.217580314798085 4.825150370597839\n",
      "Jet 36 : 3.363929867961552 -0.35016099238881165 5.49725862140699 3.1421546638011932\n",
      "Jet 37 : 2.4541140233956846 -1.2098694633099052 1.4640122851507889 4.260472804307938\n",
      "Jet 38 : 2.2802089005215547 -0.7357908182633698 4.94190128223317 3.015023946762085\n",
      "Jet 39 : 2.29738567125774 -0.046922634912452944 5.098882952736782 0.0\n",
      "Jet 40 : 2.5952059342826885 0.6409164533982803 3.1606945070941634 3.032889202237129\n",
      "Jet 41 : 2.6062343602532043 1.248689113665604 3.1066167545545245 4.556050524115562\n",
      "Jet 42 : 2.8102734754951926 0.4932074007678712 2.3154468697359825 8.437624484300613\n",
      "Jet 43 : 2.868245711996053 -2.0370643742279806 2.5338544437704167 11.154476165771484\n",
      "Jet 44 : 3.590086248345355 0.7783907280920345 0.1418371212662965 3.7970621436834335\n",
      "Jet 45 : 4.7555630039767145 -0.4302844736914952 0.23173068971564034 5.540181279182434\n",
      "Jet 46 : 5.420220826371249 1.4224723490236402 0.7798778725452411 10.917573630809784\n",
      "Jet 47 : 3.612056420608446 -1.1754405646184483 0.2960596409124434 5.9182288646698\n",
      "Jet 48 : 5.323209498149437 -1.2566244201225187 2.4702046727405005 9.656011939048767\n",
      "Jet 49 : 5.882076911781738 0.5382491064596694 4.755193296191322 3.7376763820648193\n",
      "Jet 50 : 8.100362232510864 0.9519587210634195 5.320857093012835 11.845819532871246\n",
      "Jet 51 : 8.299736479819058 1.9015290621424903 5.074493391306501 27.362677335739136\n",
      "Jet 52 : 8.288225160293347 -0.14650035207738127 4.03848659397935 2.3723796606063843\n",
      "Jet 53 : 16.44199043441555 -1.49919590262884 2.3998825298490103 37.13849449157715\n",
      "Jet 54 : 11.215973453137154 -1.5669896046944805 0.2920734943390946 27.625317692756653\n",
      "Jet 55 : 11.898837893248976 0.0484260181276766 1.5981832591492808 6.929943382740021\n",
      "Jet 56 : 33.51194713095605 -1.2587022113148498 0.9274460629723875 62.578245133161545\n",
      "Jet 57 : 32.206104949377405 -0.6961944852797455 5.849526839478399 40.19378834962845\n",
      "Jet 58 : 32.671482925394635 0.3894377209106362 5.748788647134587 33.29080734401941\n",
      "Jet 59 : 35.36958958155319 -0.49811230554986136 1.4764827997500205 44.909327298402786\n",
      "Jet 60 : 53.135797132826525 -0.026509403352639384 5.027058333819016 54.447786048054695\n",
      "Jet 61 : 45.386632014041396 0.9280275192162147 2.2344245677424803 62.375822365283966\n",
      "Jet 62 : 47.63143495724089 0.9280714910256463 3.587946310194276 70.5451865196228\n",
      "Jet 63 : 92.59220638422879 -0.29621660558453566 4.345535718529247 97.84115090966225\n",
      "Jet 64 : 63.89674343726053 -0.24904092852616935 5.759866215420725 81.1835349202156\n",
      "Jet 65 : 152.04766692261956 -0.3396634290588445 0.9261228854970209 161.03184062242508\n",
      "Jet 66 : 81.17721818065007 0.12167412281031394 1.5559589545036574 81.34599679708481\n",
      "Jet 67 : 119.25305288645963 0.17323404408971235 3.176325390367497 123.42352955043316\n",
      "Jet 68 : 107.12458535528967 0.5209672266271842 4.656786742251118 132.06375461816788\n",
      "Jet 69 : 0.0 100000.0 0.0 0.0\n",
      "Jet 70 : 0.0950697083959137 0.3628628347542296 2.9998606131379293 0.0\n",
      "Jet 71 : 0.10285790940421541 -1.2253477644889066 4.659921053400183 0.1184191107749939\n",
      "Jet 72 : 0.10850048722920232 -1.7060135924649558 2.968018956793744 0.6538288593292236\n",
      "Jet 73 : 0.16564941071427414 -0.02048923845243078 2.7183313278200614 0.16451849043369293\n",
      "Jet 74 : 0.17374009172510282 0.045118893388370426 2.3754815765812816 0.0\n",
      "Jet 75 : 0.2096423854829563 0.048009243700695425 5.601971087643939 0.0\n",
      "Jet 76 : 0.21192121915126003 0.9788329916149997 0.2262165387184454 76.81983505934477\n",
      "Jet 77 : 0.24111857686791913 1.4425063528149098 4.154812435301968 0.4754031002521515\n",
      "Jet 78 : 0.2941525094939013 -2.5831827399116403 3.367723864574657 1.767085075378418\n",
      "Jet 79 : 0.31061133690114884 0.4266132803348739 5.192565008105359 0.3291206657886505\n",
      "Jet 80 : 0.31391794486045227 -2.6036389911371534 5.9459254631223395 1.9794785976409912\n",
      "Jet 81 : 0.36429328967724917 -1.006166464796966 2.5356097425256023 0.9648559093475342\n",
      "Jet 82 : 0.37226246517661943 -1.1549577413185128 6.120138338836423 0.46850642561912537\n",
      "Jet 83 : 0.3787415923436054 0.5755902036240023 0.48230767792138535 0.0\n",
      "Jet 84 : 0.38653281626638053 2.253711112246054 5.411071961825065 0.0\n",
      "Jet 85 : 0.5367629008419899 -1.2452382237216593 3.7502384754583176 0.9765797853469849\n",
      "Jet 86 : 0.5444627040999427 2.138595365490534 0.9860134343475969 2.2747747898101807\n",
      "Jet 87 : 0.5467809015067274 0.048895802227414215 4.563121529906132 0.0\n",
      "Jet 88 : 0.6165422332785244 2.229216686454657 5.385199615781754 2.548323154449463\n",
      "Jet 89 : 0.70914800559446 -0.011246064602160074 3.26550237520587 0.0\n",
      "Jet 90 : 0.7283174415377948 0.12464634470953297 0.6555006730264432 0.8401409089565277\n",
      "Jet 91 : 1.5826021353344748 -1.6118978218031021 5.900730135983317 4.008252501487732\n",
      "Jet 92 : 0.7534065763060088 -0.7418842866788282 4.147667172512414 0.9507801532745361\n",
      "Jet 93 : 0.8644376091055408 0.795896912847082 3.654326153170259 0.0\n",
      "Jet 94 : 0.8757275896581077 -0.5333044258921966 3.4978611926456082 1.0134529620409012\n",
      "Jet 95 : 0.9778147611059227 -1.5391333797964553 2.16385408772488 0.5862389802932739\n",
      "Jet 96 : 1.262312363776102 1.1026118338150512 1.0415808008307295 0.0\n",
      "Jet 97 : 1.3195243213712193 1.1323768859350023 1.7487981767151246 2.1205899715423584\n",
      "Jet 98 : 1.5491310873951498 0.5775208096733784 2.6965336053584124 1.7248022556304932\n",
      "Jet 99 : 1.5696192610928885 -1.4826382638986888 1.899715673733497 3.617835581302643\n",
      "Jet 100 : 1.7544283283248618 -0.40151656941510494 4.671267438969489 0.0\n",
      "Jet 101 : 1.9576307120340888 -1.9078212375096606 4.332115596026477 6.660138130187988\n",
      "Jet 102 : 2.1639090766019557 -0.017575739041513164 4.114671241802958 1.9358714520931244\n",
      "Jet 103 : 2.6210870879857002 -1.2432001063154485 5.217580314798085 4.825150370597839\n",
      "Jet 104 : 3.363929867961552 -0.35016099238881165 5.49725862140699 3.1421546638011932\n",
      "Jet 105 : 2.4541140233956846 -1.2098694633099052 1.4640122851507889 4.260472804307938\n",
      "Jet 106 : 2.2802089005215547 -0.7357908182633698 4.94190128223317 3.015023946762085\n",
      "Jet 107 : 2.29738567125774 -0.046922634912452944 5.098882952736782 0.0\n",
      "Jet 108 : 2.5952059342826885 0.6409164533982803 3.1606945070941634 3.032889202237129\n",
      "Jet 109 : 2.6062343602532043 1.248689113665604 3.1066167545545245 4.556050524115562\n",
      "Jet 110 : 2.8102734754951926 0.4932074007678712 2.3154468697359825 8.437624484300613\n",
      "Jet 111 : 2.868245711996053 -2.0370643742279806 2.5338544437704167 11.154476165771484\n",
      "Jet 112 : 3.590086248345355 0.7783907280920345 0.1418371212662965 3.7970621436834335\n",
      "Jet 113 : 4.7555630039767145 -0.4302844736914952 0.23173068971564034 5.540181279182434\n",
      "Jet 114 : 5.420220826371249 1.4224723490236402 0.7798778725452411 10.917573630809784\n",
      "Jet 115 : 3.612056420608446 -1.1754405646184483 0.2960596409124434 5.9182288646698\n",
      "Jet 116 : 5.323209498149437 -1.2566244201225187 2.4702046727405005 9.656011939048767\n",
      "Jet 117 : 5.882076911781738 0.5382491064596694 4.755193296191322 3.7376763820648193\n",
      "Jet 118 : 8.100362232510864 0.9519587210634195 5.320857093012835 11.845819532871246\n",
      "Jet 119 : 8.299736479819058 1.9015290621424903 5.074493391306501 27.362677335739136\n",
      "Jet 120 : 8.288225160293347 -0.14650035207738127 4.03848659397935 2.3723796606063843\n",
      "Jet 121 : 16.44199043441555 -1.49919590262884 2.3998825298490103 37.13849449157715\n",
      "Jet 122 : 11.215973453137154 -1.5669896046944805 0.2920734943390946 27.625317692756653\n",
      "Jet 123 : 11.898837893248976 0.0484260181276766 1.5981832591492808 6.929943382740021\n",
      "Jet 124 : 33.51194713095605 -1.2587022113148498 0.9274460629723875 62.578245133161545\n",
      "Jet 125 : 32.206104949377405 -0.6961944852797455 5.849526839478399 40.19378834962845\n",
      "Jet 126 : 32.671482925394635 0.3894377209106362 5.748788647134587 33.29080734401941\n",
      "Jet 127 : 35.36958958155319 -0.49811230554986136 1.4764827997500205 44.909327298402786\n",
      "Jet 128 : 53.135797132826525 -0.026509403352639384 5.027058333819016 54.447786048054695\n",
      "Jet 129 : 45.386632014041396 0.9280275192162147 2.2344245677424803 62.375822365283966\n",
      "Jet 130 : 47.63143495724089 0.9280714910256463 3.587946310194276 70.5451865196228\n",
      "Jet 131 : 92.59220638422879 -0.29621660558453566 4.345535718529247 97.84115090966225\n",
      "Jet 132 : 63.89674343726053 -0.24904092852616935 5.759866215420725 81.1835349202156\n",
      "Jet 133 : 152.04766692261956 -0.3396634290588445 0.9261228854970209 161.03184062242508\n",
      "Jet 134 : 81.17721818065007 0.12167412281031394 1.5559589545036574 81.34599679708481\n",
      "Jet 135 : 119.25305288645963 0.17323404408971235 3.176325390367497 123.42352955043316\n",
      "Jet 136 : 107.12458535528967 0.5209672266271842 4.656786742251118 132.06375461816788\n"
     ]
    }
   ],
   "source": [
    "for i, jet in enumerate(reco_jets):\n",
    "    print(\"Jet\", i+1, \":\", jet.pt(), jet.eta(), jet.phi(), jet.e())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84b009c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Jets: 136\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Jets:\", len(reco_jets))\n",
    "\n",
    "\n",
    "reco_jet_pt = [jet.pt() for jet in reco_jets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49736597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TCanvas::Constructor>: Deleting canvas with same name: canvas\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxwAAAI8CAIAAAD0vjrdAAAABmJLR0QAAAAAAAD5Q7t/AAAgAElEQVR4nO3dTZLbxpou4ERHTx0lzX0c3sgNgMPuhdiytIF7zgTgyKcXYMuhzQDYiMMDzV26G8AdZAuGQRJksZIkfp5n4GDxN5OkiVeZiS+zrusCAACv8x+PbgAAwBoIVQAACQhVAAAJCFUAAAkIVQAACQhVAAAJCFUAAAkIVQAACQhVAAAJCFUAAAkIVQAACQhVAAAJCFUAAAkIVQAACQhVAAAJCFUAAAkIVQAACQhVAAAJCFUAAAkIVQAACQhVAAAJCFUAAAkIVQAACQhVAAAJCFUAAAkIVQAACQhVAAAJCFUAAAkIVQAACQhVAAAJCFUAAAkIVQAACQhVAAAJCFUAAAkIVQAACQhVAAAJCFUAAAkIVQAACQhVAAAJCFUAAAkIVQAACQhVAAAJCFUAAAkIVQAACQhVAAAJCFUAAAkIVQAACQhVAAAJCFUAAAkIVQAACQhVAAAJCFUAAAkIVQAACQhVAAAJCFUAAAkIVQAACfznoxtwV1mWPboJALAhXdc9ugn3s61QFTb26QJwSpZljgi3trWxjFlP/1VVVRRFURRVVV3+qJfeHwDg9eY7UlUURdu28XLbtk3TNE1z4aOKorhp2wAARmY6UtU0Tdu2ZVl2Xdd1XVmWMVdd8qi7NBAA4G9mOqMcZ2GHbcuyLM/z6VwV7xPT2NEZQDPoAESOCHewtTd5piNVIYQ8z0d/To9CxSm/S6YIAQCSm2+oetG6qKqq2rbdVBwGAGZljqHq6GjTRMZqmma/39d1fcmTZ9e6tjcAwCbM8ey/l567t9vt8jy/8FFGswCAWzgTqqqq2u/3R2+6czo5tVgqLkgf1aZqmqavcXWHtgEATIWqOK0WDtaM38eLlpyPkl/btqpVAQD3NHWuYyyk+ZD5ssOXzrLsVKGEkYl7bu3cTgBOcUS4g629yWcWqj9kjCoMJvXin/FCn5OapsmyzF40AMB8TIWqWKfgbk0ZKooiVlGPZ961bXt4cp+SVADAfJwZl3t4Rc340qlWR21tHBKAUxwR7mBrb/JUb5um2e12p25d4tu0tU8XgFMcEe5ga2/ymZIKj1pTBQCwLNuKkFuLzACc4ohwB1t7k6/cpqaqqoWuE7cXDQBwC+e3qTmsXNA0zXJLa24qMgMAd3MmVJ0avLl8rz0AgC04U6cqhFDXdRzdKcuy67p4WaICABiaClXDGlFlWfaLqOq6PrXLMgDANl26TU3cjK+/HBQ0BwAYmApVghQAwIXOFJCIC9Xrui6KIl6O84Bt2y7xNLqtFcwA4BRHhDvY2pt8Zvqvrut+BjBuabzf79u2Lcvy5k0DAFiOF0fIpmmWe+rf1iIzAKc4ItzB1t7kjfV2Y58uAKc4ItzB1t7kI9N/WZb1y9Lt6AIAcIkjFdWH1dL7BVWrMR0HNxWoAYCEtjUut7VxSABOcUS4g629yWfO/gMA4BJHpv8uXC+1qewJADDtSKga1qCKdT77K0d/AgAQTU12Nk2z2+3Ksqyqanh93L5miSNVM5/cbdsuz51WCXAPMz8irMPW3uQjI1W9uNPfKFGFr6UWFl0FdFY+f+5+/TV8/Bien8ObN927d+HDh/Dtt9IVwCI1TTPaKrcoioRHzPjkR5/w8KWjeCi/dcM4H6q4qc+fux9/DN99F56fsxDC83P48qX74Yfw6VMnVwHMwUunEZqm2e/3w2vin3Ej3de3Jz7/0RGg3W536lFVVd26YUyd/deXAB1dHwOvDyCJX38N330Xfvvtr/9dP37Mvv8+/PLLAxsFQPj8ufvXv7q3b7uiCG/fdv/8Z/f58wtmsrqBuHnuROJJKM/z7sBw0ulRDduCqVAVP4PdbhfjbVQUxX6/X19R0Ef5+PFvierrldnHjw9pDgAhfJ1G+PPPOI2QPT9nX76EH34IL8pVvaIo+vO9Ejf0dWbbsIU6U6eqrus8z/f7/e6rtm3LsvTuJ/HlS/f8fOqm8Py8ocV9ALOSfBphOL0TVzLFBcr99VVVndoLbrhrXPJpIvNOCU2tqQohxE89DDLs0t/9WW1T8/SUvXlzPFc9PYU3b6ypAniMjx//d6nr36/Mnp66n3++5glHK2fato3n1/crbfb7ff/nbrfrz5uLZ+KHEPqpuljbKBVLehI6E6pGVvCmz+3cznfvwpcv3cePf/tf9/377ptvHtUigK07O41wyT96h0fMGINGK2eGy8N3u12e5/3Kp67r+rPs45X9wSveNPG6bduO7pDn+XB+6WzDuNr5UBWrUsXL8bMcfTy8xocP4Ycfwk8//ZWr3r/vfv89fPr02HYBbFfyaYQ8z/t41BuNUxwOW8QFzW3bjkJPnufTg1VxTOvqhnG1M6Eqpt24iKqvpb7f7/tpQV7p22+zT5+6X34JT0/dly8hhPDNN+HTp/CPf5j7A3iY108jXH6UjPfc7/ejege9w/g1EapiTkrSMF7q/Nl/dV1XVTVcSVeWZdu2PpVUvv02+/nn7Pk5q+sQQvbvf2cSFcBjffgQ/vgj/PTTXytG4jTChw+3esW6rkd1EE4dZx1/Z2sqVJ2q2Wqc8EaKQpYCmIVvv80+fQpPT+HpqQuhe3rqbjeNcLQq5HBGaDSClXahOgmdKakAANs0nEZ4fr7tNEJcWjM80b5t2xi24gKpfoBjBWeMrdj56b/DYcb4ifpcb6HrwuRZHQDc2x2mEaqqyvM8VlLIsqxt236xeazPGc/pizfFcp3M0Jnto2NYjucdxA8yDkKWZbnEScBFbJedZWH2bQRYvHkeEaY3Sz5102zN802+nfO9rapqNJu73J0XF/HpClUAd7CII8LSbe1N3lhvl/DpClUAd7CII8LSbe1NfllF9RWY1TY1AMBqHAlVF07tLbROxvxjU1yrPvtmAgB/c2Rcbnospzf/dHJoKeOQQhXArS3liLBoW3uTp0oq5Hl+WOC1d7cmAgDM35FQFTNTrIoRa2bY6Q8AYNrJkaqqqm6druKWghfujz1x5+zAEmtoAQCL9oLJzmHBqjzPX5muRptsTzxh0zS73S7eJ3zd82jY7MNFYKdqky5octeyKoCbWtARYbm29ia/YO+/OHYVq+O/cjfHpmliffbhYNipUBUTVdyvu2maWLm/z0zxUaO1X0aqAIA7uzRUNU1TFEWWZf02Na951ZiT+ugTL0wkoeHLjXbzXmLZfgBgfc4U/2yapqqqflwq4ZZ/cS5v+Oep0a9+U8mhPkVJVwDAHBwPVbfLUr3L088oP8WWjB4+XFb1+vVeAAAvdSRU9QGlLMt4wl3alzyaeEbr1o+Kk4Z9w+Ll+Kg+9sXV9BNnKV5Y2vTQnZfaqasOAMsyNf233+/70/0OXR0yrk5p/Vr12KqYouq6Hj5hVVVxFfzEk1z36gAAE46EqtFqp/u4cMIujpzFXHV0HjB8HfSKK+vTNhIA4JQjoeo+C5IufJW4uitW/uyvvGSuUKICAO7pSEmFO+xLc3iuXyxbdXjPfthpeOXwjL/D+ulWqQMAD3B0s+S6ruMkYJ7nfYnOhGKVhDzP45/xtUa39q8b29mX94zZq781PvbUrSOn+jtbS2svwGIs7oiwRFt7k8/0tk9XE0nlOqNxqWFJ9FHk6g6Wlg9vOrx1op2L+3SX1l6AxVjcEWGJtvYmX7opz/C0u4SlFi6v2NlP6h29c2ze2VYtbhMiVRUAbmRxR4Ql2tqbfE1vYy2oUS2DRVjcpytUAdzI4o4IS7S1N3ljvV3apytUAdzI4o4IS7S1N/nkNjX9hFp/bl28Jl7e7/ebepseRV11AFiKIxGyaZp+Q5io67rD3V2WGKqWGJmFKoBbWOIRYXG29iYfqVMVE1U8HS+eiBcTVVmWdV33p+nduaGpZJMe3ToAYKmOT//1OxYXRVHX9W6363csXrrlxkEAYM6OjFSN9Onq1k0BAFiu46FKhAIAeJHzI1U8VjwBEACYOaEKACCBk3WqRpeH10SmCAEAekcKSFxYWWCJp9EttGCGUlUAyS30iLAsW3uTj4xUxdpUAABcblsRcrmR2WAVQFrLPSIsyNbe5ONrqob6TQAtogJgTWykQVonI+ThDoBRXdcxYLVtu7j4efb/n9n2yEgVAItjpCqEEKqq2u/3IYQ8z2OEiuNVbdv2SWuhS6829ekCAHdzJELGRJXn+WEZhRBCHKM6devMLTcyG6kCYHGWe9i9zsmSCqfehX4GbYlv06I/XbkKgGVZ9GH3CuOK6nH8qSzLo/eOt8aJvyWOVAEA3MjxNVWnTvQrimJTkRMA4ELX7P1njAoAYGQcquIY1XRsEqoAAEZevFD9kjvM1tJXzFmrDsCCLP2w+1JHpv/iOvQsyw5HpJqmiYlqoUWqAABu5HiEHBb/DCHE4p8hhLZtQwhlWVZVdc9WprL0yGykCoAFWfph96WmehvrfA6vWWjNz95yt6mJhCoAFkSoOiJuqHz7xtzc0j9doQqABVn6YfelNtbb5X+6chUAS7GCw+6LHCn+eeF6qYUuqwIAuIWTJRXOWmL2XEFkNlIFwFKs4LD7IkdGqjbVfwCAJK7ZpgYAgBGhamG6Llw2PQsA3JVQBQCQgFAFAJCAUAUAkMCRs//WbbpghDMfAYDrXBqq4pZ/K9isZgWxKa5VX34/AGBVzoeq4bbKXddlWbb0bZUBAJI7s6Yqy7K2bcuyzPM8XpPnedu2KxiyAgBIaCpUxd396rquqqpPUU3TlGXZj10BABCmQ9WpdVQxbJkBBADozbqkQhwhK4oixriEd146ddUBYG7OT/8djkjFsatbL6sqimK/37dt27btfr+feLmmabIs2+/38c/9fj9dNwEAILmpUFUURZ7nu92uKIoYraqq6peu37RZTdPEV+m6ruu6uIrr1ITjbrcLIXRd1zRN0zR1XYeviRAA4D6ys3WbqqrqB4Giuq5vPUwVh5qGbZso5ZBlWVmWwxR16s5Zdr6/S6FUFQAzt6bD7iXO16mqquohoz59EYf+z1OnHMahqRFFHwCAe3rZNjX3rKt++asMyz2ErxN/qw9V6qoDwKycOfsvLqLqw8put9vtdlmW3XTs6ugc3yUhKTYvLsY6df/sWq/qEgCwdmfqVMXVVDGgxMt1Xed5PlplldbVg0xd19V1XZblfr8/Ffu6a13dHQBgC86XVIh5Io4exSXqDyn+eeHLxebdOvYBAIycmf7rV4sPV1PF/946VF34/E3T9BUfeqtfUAUAzM2ZOlX9CXf7/f5owLqRw3P9ThXHio0chaqNbKGjrjoAzMeZUBX/OxqgisU2bxqqRmfwxQv9MqlYQn24amq/3/dBqqqqO5QnBQAYOlOVq6/82dfSjOfB3aH+56jo6PAVY7AblvccnZ03USZ0ZUvOVVUAYLbWd9idNvfeXj7V2KeoiTuv79MVqgCYrfUddqdtrLer+3SFKgBma32H3Wlnzv6L59Yphjlb1qoDwExMbVPTr0kfbcMHAMDIVKgaFv9cjekxtpV1FgC4mzMbKq9vjEpsAgBu4dLinwAATDizLP8+O9LczVpPQ3AOIAAztNbD7ilnFqqHENq2PboOaVNvEwDAtDNrqsIal1UBACS3rXG5tY5Dmv4DYIbWetg95Uzxz17TNKtZWQUAkNz5UBUrqu92u91uF2upx/pVzIe66gDwcGfWVMUl6nmeF0VRFEUcr9rv9+FraVAAAML0mqqqqvb7fV3XsbDC6PolzpKueHLXsioA5mbFh92jpnobi38evUOWZYdha/7O7gO93M9eqAJgbrYWqs6XVFiZTX26AMDdnNmmJhwrpx6vX9wwFQDA7ZwZlxsuVI/XxFXqZVkucaH6uschzQACMCvrPuweOt/buCx9eM1CE1VY+6crVAEwK+s+7B56QW+bpln6lN+6P12hCoBZWfdh99BFC9VH5dSXHq0AAJI7EyGbptntdqMr8zxf6JY1647MRqoAmJV1H3YPndmmJiaqsizruu66rq7rPM/btjVYNUM2qwGAB5oKVXE1el3XVVX1ZRSapinLsm3bhQ5WAQDcwlSoirHpcFBqoaf+AQDczvnin4cWPUaVTXp06wCApTpf/POwKlWWZQtdq776FXPWqgMwH6s/7I5M9bZpmqqq2rYNIeR5Hq8c/RlCKIpiKROCW/h05SoAZmILh92hqd4e1lI/ZSlv2RY+XaEKgJnYwmF3aGO93cCnK1QBMBNbOOwOnalTdWiJS6kAAG7tTKiqqirLshik4uXdbtdfAwBAdGaheqyoHu8TKw7EWqBt2y5xQG8j45BmAAGYg40cdnvnK6rHtyMOTdV13Z/rZ7AKAKB3ZvqvL50wrK4e/ytUAQD0zoSqWJUqhDCsrXBq+xoAgM36z4nbqqra7XZ9eCrLMgwWWi00VE3vRbOpqV8AIKEzK8j6+p/9vjQxlBzuXbMI21kxZ606AA+3ncNu9OLeNk2z0DGqsKVPV6gC4OG2c9iNXlD801IqAIBTzoeqoiiyLItlP0MIWZYtceIPAOCmzoSqLMvati3Lsq+tkOf5fr83XgUAMHS++Gcsod6nqKZpyrLsSy28Rnzavpro1XfODhhL67oweZojAJDYVKg6tYgqSUX1oij2+33btm3bnh36yrKsL5S13+9tPggAzM0LFqon1DRNnFXsuq7rujj0dSonxbxV13XTNE3TxPMI4gKvMNg/pxswUgUA3NlUqDq1HU2MLK9ZVhUjUR994oVTSaht2zzPhy8Xy5BGzkkEAObgTEX1WD+9X6Uer4mDTK984f45+z9PrdMaJaqRPvNJVwDAA02FqhBC0zR9UfXwdQfAJOXUL08/h0Nlw40Io+HmM33x942La9W3VHQNAB7p/JqqqqriQqV+3dIrE9XRxHNhxmqaJuanuq7jNXF8a7Q8a+LZDk8VvNCL+wkAbMmZkaqhVDNrVz9PURQxQtV13T/J8HIYTFCeepJN1csHAO7m5EhVrAs1GpSK599VVZV85GZ6wi4OUPUnDA5T1GFEO7W+HgDgdo6MVMXF6fFyLCIVc0ySgp/DV7n8nnGx/ItykhXrAMA9HRmpiomqLMu6ruPSpX6UqP7qlZNoh+f6TZxRGNtzNFHFEazD4bTXtG1N1FUHgLs5vqZqeH5fXde73S7JGX+9qqp2u11RFMM6CP3zx6Gp+Ip9Qjp89ThB2e9F2D9JkooPAAAvcn6hegwraWfTiqIoyzJuOBOv6c/m640GnA7LKPS75WRZ1s9XhkQVHwAAXiQ7nMjLsmx0St3hNamkqtgZV9D341WnZNmR/q6bUlUAPMrWDrsvKKlwCwnLNFiZDgA80GM2VOZurFUHgPs4PlJ1eALd4TVGhgAAesfXVF3yyCXOkp7t2hI7dZZlVQA8hDVVYd31CDb16QIAd7OtCLm1yBwZqQLgIbZ22LVQff2sVQeAOxCqAAASEKoAABIQqgAAEhCqAAASEKo2wVp1ALg1oQoAIAGhCgAggeN7/63Y9E41m6pRBgAktLlQJTYBALdg+m8rrFUHgJsSqgAAEhCqAAASEKoAABIQqgAAEhCqNsRadQC4HaEKACABoQoAIAGhCgAggc1VVLdNDQBwC5sLVWITAHALpv+2xQmAAHAjQhUAQAJCFQBAAkIVAEACQhUAQAJC1eZYqw4AtyBUAQAkIFQBACQgVAEAJCBUAQAksLltauz9F76uVd9GXwHgTjYXqjYSmwCAOzP9BwCQgFAFAJCAUAUAkMCsQ1VVVUVRFEVRVVXaO6OuOgCklc124XZRFG3b9n/med40zak7x3P68jwPIcRH1XVdFMXh3Wbb3/tzAiAAN7W1w+5MR6qapmnbtizLruu6rivLsm3bU6Eqhqe6rpumaZomfn673e6O7QUAtm6mETKOPA3blmXZqcGqw5uqqtrv94dd21pknmakCoCb2tphd6YjVeHrXN7wz+Fs4Oimw5k+AIB7mm/xz8tz0uHw1X6/T9uYVVJXHQASmuNI1dE5vgszVtM0ceqwruujd8iudX1/AIANmONI1dVzef0Jg0dP/Ys2NbkLANzNHEeqjpqopxC+DlD1JwxaYgUA3NkcR6qi6RQ1uudut5suZAUAcFMzPdcxTuSNSiqUZXm0Wvph/YVTtnZu5yWsVQfgRrZ22J1pb0eDT6OMFW+NGSteDiGUZTl6ksMEtrVP9xJCFQA3srXD7kyn/4qiKMtyv9/3p90dns03muw7LKNgE0AA4G7mHiH7kaokz7a1yHwJI1UA3MjWDrsb6+3GPt1LCFUA3MjWDruLKanAjcS66gDAKwlVAAAJzHSh+u1MbzizqVFKACChzYUqsQkAuAXTfwAACQhVWKsOAAkIVQAACQhVAAAJCFUAAAkIVQAACQhVhGCtOgC8mlAFAJCAUAUAkMDmKqrbpgYAuIXNhSqxCQC4BdN//C9r1QHgNYQqAIAEhCoAgASEKgCABIQqAIAEhCr+Yq06AFxNqAIASECoAgBIQKgCAEhgcxXVbVMDANzC5kKV2DQtrlX3JgHAS5n+AwBIQKgCAEhAqAIASECoAgBIQKhiTF11ALiCUAUAkIBQBQCQgFAFAJCAUAUAkIBQBQCQwOa2qbH33yVsVgMAL7W5UCU2AQC3YPoPACABoQoAIAGhCgAggUeGqqqqiqIoiqKqqgsfUhRF0zSjK7MDlz8hp9isBgBe5GEL1YuiaNs2Xm7btmmaw7Q00jRN/xAAgFl5zEhVjEdlWXZd13VdWZYxV03cv6qq3W539KYQQl3X3YCRKgDgzrKHlBiIxaKGL51lWZ7np3LVsLhUXddFUfR/VlW13+8v7EWWPaa/C6VUFQCvsbXD7sPWVOV5PvpzYmovjj/VdX14U5/DLplABAC4kUeuqUr4bMOhrIkRL15EXXUAuNwDRqqOJp6rM1Yc3xotz5p4tsNTBS90XfMAgI14wEhV2jGqwyVW0ycJbmpyFwC4m7kU/7x6wu4wosVrzAACAPf0sFB169CTdjwMAGDaY0LV4bl+sWzVS5+naZrD+unGqBJSVx0ALvSYUBVjUD+YFC/02ehoVDqqKIo8z/f7fR+kqqq6Lp8BALzGY0oqFEVRluV+v+/PqjusQXXhgFNMYMNi62VZqqgOANzZg0udxuT0+vVPsfJn3J554m5bK+2ahFJVAFxna4fdjfV2Y59uEkIVANfZ2mF3LiUVmC1r1QHgEkIVAEACD9v771GmN5zZ1CglAJDQ5kKV2AQA3ILpPwCABIQqzrNWHQDOEqoAABIQqgAAEhCqAAASEKoAABIQqriIteoAME2oAgBIQKgCAEhgcxXVbVMDANzC5kKV2AQA3ILpPy5lrToATBCqAAASEKoAABIQqgAAEhCqAAASEKp4AWvVAeAUoQoAIAGhCgAgAaEKACABoQoAIIHNbVNj779XimvVvU8AMLK5UCU2AQC3YPoPACABoQoAIAGhCgAgAaGKF1NXHQAOCVUAAAkIVQAACQhVAAAJCFUAAAkIVVzDWnUAGNlcRXXb1AAAt7C5UCU2AQC3YPoPACABoQoAIAGhiitZqw4AQ8sIVVVVFUVRFEVVVRc+pCiKpmlu2CYAgIEFLFQviqJt23i5bdumac6mpaZp+ocAANzB3EeqYjwqy7Lruq7ryrKMuWri/lVV7Xa7O7YRACBkMy8xEMtKDRuZZVme56dy1bAMVV3XRVGMbp15f5cly4K3E4BTtnbYnftIVQghz/PRnxNTe3FAq67r27eLbWnbDf0uAHCFBYSq0WgT87GFEwA/f+7+9a/u7duuKMLbt90//9l9/ixdAXDErEPV0Tm+V2as7FqveVEW6vPn7scfw59/hufnLITs+Tn78iX88EOQqwA4NOtQdYsxqu5ayVvC/P36a/juu/Dbb39F6o8fs++/D7/88sBGATBTCyipMKL6FHfz8WMcoxpdmT09dT///JAWATBfsx6piqQoHuLLl+75+dRN4fnZ4CUAfzP3UHV4rl8sW/Wo9jCy4rXqT0/Zmzenbgpv3qy02wBca+6hKu5L0y+uihf6zWqapsmy7PK9a+BF3r0LP/00HpF6/7579+4hzQFg1uYeqoqiiFXU4yl4bdse1qAyP8iNfPgQ/vgjhDDMVd3vv4cPHx7UIABmbDGlTmNyen09haX0d0HWXVc9y7r/+3/Dx4/hy5fw9BTevQv/8z+h68z9AZy3tcPuxnq7sU/3PtYeqv63d03TFUU2uhKACVs77G6stxv7dO9mrSHjVL/W2l+AtLZ22J37miqYoRWf8wjA1ZZX/POVpjec2VSgZtr0cFTMVb4vAPQ2F6rEJi4hMAHwUqb/4EomAQEYEqpIYGXx4vJhqpV1HIDXEKrgb1468SdXARAJVQAACQhV8Jfr1qcbrAIgCFWQhFwFgFBFGitIFcooAPAaQhWEkCJRrSBWAvAaQhUkI1cBbNnmKqrbpoZDCSf+bF8DsFmbC1ViEyMyEABJmP4jGZNfkfcBYJuEKjbtRsNUchXABglVAAAJCFVs101XUxmsAtgaoYqNusP6dLkKYFOEKlISI0a8IQDbIVSxRcooAJCcUMXm3DlRGawC2AihCm5OrgLYAqGKbTHxB8CNbG6bGnv/3ZrN747ytgCs3uZCldi0ZY+NNXIVwLqZ/mMr5hBoLK4CWDGhCgAgAaGKTZjDMFVksApgrYQq0ptbbphPoorm9v4AkIRQBQCQgFDFys1tmCoyWAWwPkIVPIZcBbAyQhVrNs9hqp5cBbAmQhU3MYe4MPNEBcDKbK6ium1qmBVl1gFWY3OhSmzaiAUlFbkKYB1M/7FCMgoA9ydUwePNYQkaAK8kVHErjwoKCx2mkqsAlu6RoaqqqqIoiqKoquo1d84OXPKErNJCE1UkVwEs2sMWqhdF0bZtvNy2bdM0TdMkuTMAwP09ZqSqaZq2bcuy7Lqu67qyLGNUuuLO8UJd192AkaptWvQwVWSwCmC5soeUGIjFooYvnWVZnudHc9X0nauq2u/3F/Yiyx7T3826c8pZQaiKVlCj5A4AAA5kSURBVNMRYOO2dth92JqqPM9Hf/YTfC+683DIypzg3Nxz3EUQAeCxHrmmKuGdh3XST414sWIrS1TKgQIs0QNGqo4mnqszVhyyGq24mni2w1MFL3R58+D1LK4CWJwHjFS9KD+dVdf18AmrqooL20/df1OTuxux1kEd41UAyzKX4p8vmrAb3vkwosVrzABuhNgBwEw8LFRdnaIulHY8jKuZxnoN7x7AgjwmVB2e6xcrUb30zk3THNZPN0a1HVsYppKrAJbiMaEqxqB+MCle6LPRKCpN3LkoijzP9/t9H6SqqprIZ7BEchXAIjympEJRFGVZ7vf7/qy6uq5H9+lzUlEUdV3vdrujd44JbLfb9deUZami+hZsYZgKgAV5cKnTmJwuXP80cedY+TPuuDzxDFsr7ToTt0g/G0xUG+wysHRbO+xurLcb+3RnQqhKZZu9BpZra4fduZRUYMWSLwmSLQCYIaGKhdlyorJiHWDOHrb336NMbzizqVFKlkiZdYDZ2lyoEpsWTZ4IchXAXJn+AwBIQKjiHpIsBjI807O4CmCGhCqWQaIakasA5kaoAgBIQKhiAQxTHWWwCmBWhCrmTqKaIFcBzIdQxZ04/N+INxZgJoQqZs0wFQBLIVTB4hmsApgDoYr5Mkx1ObkK4OE2t02Nvf+WQqICYFk2F6rEpgeyad1NeXsBHsv0H3MkHFzHJCDAAwlVzI5E9RpyFcCjCFUAAAkIVcyLYarXM1gF8BBCFXfleH8f3meA+xOqmBHDVAAsl1DFXEhUaRmsArgzoQpWS64CuCehilkwTHUjchXA3Wyuorptah7usPC3RAXACmwuVIlNbI3tawDuw/Qfj9G2/3uQd7y/A5OAAHewuZEqHuvz5+7XX0MIoSjCmzfdu3fxagd8ABbPSBX38/lz9+OP4c8/QwhZCNnzc/bvf4f/+q/w+bOhqpszWAVwa0IV9/Prr+G778Jvvw2P7dn334dffnlUi7ZFrgK4qWxTC7ezbFv9nZu3b7vn5yNH9aen49dzCxaxAXeztcOukSru5MuX7vn51E3h+XlD/9cBsEpCFXfy9JS9eXPqpvDmjZGqOzEJCHAjzv5biUUMsb57F7586T5+/Nsh/f377ptvXvAki+hpKjfq7DwrV23nk91OT4POrtR2evpSRqq4nw8fwh9/hJ9++ut/xffvu99/Dx8+PLBRAJDG5kJVNunRrVu5b7/NPn0KT0/h6akLoXt66r75Jnz6FP7xD+/8vZkEBEhuWyN4Kx6xXFzXmqYrimuO6ovr6WvcurOzmgTczie7nZ4GnV2py3u6nfck2lhv1/vprrhrI9vpabhLZ+eTq7bzyW6np0FnV0qoOmVz038AALcgVB33mvVVVz/2IYu6HtLTVz72IS+6rI/18tc9XFzlY73DYx/yomv9Did84Csf+5AXXdzHum6rClVVVRVFURRFVVWPbgsshkXrAEmsp05VURRt28bLbds2TdM0zUNbBAv1fx7dgHvaVGdZJd/hGVnJSFXTNG3blmXZdV3XdWVZxlz16HbBMnRdyLLuX//q3r7tQmjevu3++c/u8+fVLi/9/HlDnWWVfIfnaSWharfbhRD6Wb94wSQgXOjz5+6//zv8/HN4fs5C+I/n5+zLl/DDD2GVP9OfP3c//hj+/HMTnWWVfIdnayWhKoSQ5/noz342EJj266/hu+9CCH8trfr4Mfv++/DLL49r083Ezv722yY6yyr5Ds/WSgpIZFlWluVwaCousRr17j6lNa5+7HZe9DWP1eBbPPbt2+75+chi9aen49cnedFUj33pAx/e2UV8JR7+oq957OobnOQ7fJ+ebq1O1RoWqh9dOzVctz50+UmkizvHdVkv+prHanDqx34TwvNwmKr35UvIsu7oTSd0r2jv1Y990QNP/r7fsbMPeZde81gNvvVjU32Huyx7E8L/u/CJFld7Yv7WEKqKorjwnpvKy3C5t2+75+cj1z89hRcN3ixBtqXOskoT3+Hs+fnL3dvDX9YQqo5y6h9c7t278OVL9/Hj3yLF+/fdN988qkU3tKnOskq+w7O1noXqUhRc7cOH8Mcf4aef/hrKff+++/338OHDAxt1K5vqLKvkOzxbKwlVh+f6xbJVj2oPLMu332afPoWnp/D01IXQPT1133wTPn0K//jHCqfDNtVZVsl3eLZWsiy/aZrdbpfneRyvOnrqH3CJpumKYis/zZvqLKvkOzwrKxmpKooiVlHPsizLsrZt67p+dKNuYnp/w1XuflgUxeHc7sp62jTNfD7WW/9Az+o7fIvOHv3Ghpl1PBWdHd3hsEe37uw9v8Oz+qWaqW5d6rqu6/rRrbiV+JHled5XOh129rD86cMamk7sVL8B0fDK1fS0n6fu+zW8dWWdPfwOD29dQWfjP+cOf4Wmu7bQjp/q7Cp/qU51treaH6tTPd3UL9XV1haqVix+ZYdf9OHXOv5vMNz9cPr//0XohxuHv1Mr6+moO/HP/vdoZZ0dfYeP9n25na3ruj/qjJo93bUldnyis+v7pZro7PA+K/ixuvw7vO5fqtcQqhbjMPvHL25/6+jfDSv4t0L/T6Lh79TKeho7OLymLMu+vyvr7HR3lt7Z4T/TR4eT6a4dfgfm3/Hpzk78Uq2ss8P7HP5YLa6zEz2d/qVaXE9vR6hajDzPJ77lh9/gw2/5svQ9OgxVa+rp9E/Pyjp79Jd3mCBX0NmjUyfTXVtux492Ns/z0RTY6J9/a+ps1HdhHT9WF36Hp29dRE9vYSUL1bcgrhAcXjOqInF5Zfn5i+scT60JXVNPw9fFrUVRZFl2uD50TZ2NXcuyrKqqqqriNhejLTsf07Lbm+7a6NZFvw9N04zWKe/3++Gfa+psCKGqqomTzdfU2Rf9Ui26p68hVC1S0zTxgDRxkuNyv9NN0+z3+8vP31x0T0MI+/0+HnXiGay73W7iDKPldjaK/34ddnnizkvv7IQVd23okl+qRXvpj9VCXfFLtVmr3aZmxYqve0XXdb3Kn+ZYcmyVXTul/2duHL/Z7Xan/uG7aPGrW5ZlHMmoqir+Rm/6BOz1Wv0vVdjej9VGfqlew0jVksR/9sXDUtd10/8nL/TfEPH4GseZo/B1QuFUjxba0/B1uGJ0KvL04M1yO9s0zTBRhRCqqsrzfDQ3NHrIfdp2f9NdW3rHt/BLFbb0Y7WpX6pXMlK1GKOq8UfvcNcG3dLoQNu2bdu2/U/zmnoazs0ErbuzcTCjXy+4ss4OrTtFDW3qlyps7Mfq0Bb+532Zhy2R54WmP6/p86oWLVxwlvJye3rYneE1K+vsYePX19lTJ8RNdG25p6OfOk3spb9Uy+3syCU/VvPv7IXf4bP/886/p7cgVC3DsLLcyPAOE5Vvliscq6e3mp6OuhNH1Nf6sY4K+cTOjuoHLr2zRw9I0107eusiCicedva6X6qFdvbQJT9W8+/sJd/hS36p5t/TW1jeb9Y2TZxdcuo+q/lCHw5XrKyno6UJo3/erayzo2UZ6+vsqUPvdNcW2vGJULW+X6orQlW3zM6e6ummfqmulnWW7q9LnNjewtkoK+vpdHd0dh022/FDOrtQvsPThCoAgASUVAAASECoAgBIQKgCAEhAqAKAxJqmmUM9zJk0YzuEKgBmKsuyJKeSxb3qDvWbJhVFcfQO0eVtiFv0xH3xdrvd6FVuJ25cnaoZ8eFH7x/fyXjT6EUJtqkBYAXi9jjT57OXZTncU2W/3/dbeg8DRNM0+/0+z/OXhqHYhtELDV/lRc92uaIohkWkDpsRQtjtdpc3Iz5qv98f3jm+dfH6+PxGwv7mwXWyAOCEcPFuJ7H45KlbY+Y4rEh59Dh4SZ3Po04dVW+6PcBhx6ebcWG/Jt6x4Sdy3Ru1Yqb/AFiMuINvURSjsaX4Z1VVjxo4iQ04WlO+b9vwyqMdCYNVUFVVxTtM96iqquFGBWebMXq2U82Ig1WjKw87csV43so9OtUBwHHh7+Mio22O+kPYcPLr6CbcdxipetEh9VRH4k39rf2FicaMbk3VjKNPdTjkNj1AuEFGqgBYgKqq2rbtA0Q8nPcDKv3R/cKBkzhCEwbB5fUufKqJjkRt28Yo2TRN13Xh9EKoo9vCpGpGTKLDka3YsOGTDNeoEYKACcBchcFIVThYXxWP+vHyJWuqDh1dsHX1SNVokOzUONB0Rw5HgyaWZA0feOrJr2vG4X1OvS2HHd8yZ/8BsBijpVQveuwoWsW1RAnadKI9wwVe8cy70a2nHnj1Kx710mb0Y3ghhDzP27Yd3vPoO2akqidUATAXwyP66PoQQtu2/TH+7EMOJU9RI6O2DV+uTzNJOnJ1M5qmibdONGP02LZtY8MO5/44ZE0VALNQFEUssHT0pnBimummOelyMXCcHbNJ25HDh0w3o49QFzZjdE7lqaVdM/kI5kCoAmAWzq56Ht0Uy6Dftk0Xi207GgoPM8ftOvKiZoxmA482I84ATsz9MSRUATALo9pIoz/LsuyP7uHryWujZVI3Xdwz3KHlqNiY4X3ifi+jKbZLOnKhozH0aDMO5+/i3fqcFJtxOMEXn+TU3N/R0w83LfHCdwC41ihbjOanRrcOT17ry10ePaHvVJ2qo06d5hafZPpMt6NVN7uDE+smOvKis/+6EyffHW1GXdejcyQnmjF6iVPv3uEJgxuXdZM7JQHAnU2Pf0zc2lchv1HDjtZGPxSrop9tSZJhnrjg6egQXd+M6Vd5TTOOVmPfMqEKAC4SA8TcZruy7GGH8ge+9DxZUwUA510y+PQQZVk+pFWjbQcJRqoAYOmKc/sur+ZFZ06oAgBIwPQfAEACQhUAQAJCFQBAAkIVAEACQhUAQAJCFQBAAkIVAEACQhUAQAJCFQBAAkIVAEACQhUAQAJCFQBAAkIVAEACQhUAQAJCFQBAAv8fLLKdl/+2B7wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ROOT\n",
    "import numpy as np\n",
    "\n",
    "# Define bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100, 200]\n",
    "\n",
    "# Initialize lists to store values\n",
    "x_vals = []\n",
    "ratio_iqr_median = []\n",
    "\n",
    "# Iterate over bins\n",
    "for i in range(len(bins) - 1):\n",
    "    lim_low = bins[i]\n",
    "    lim_hi = bins[i + 1]\n",
    "    x_vals.append(np.mean([lim_low, lim_hi]))\n",
    "\n",
    "    # Filter gen_jet pT values within the bin\n",
    "    gen_jet_values_in_bin = [pt for pt in gen_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Filter reco_jet pT values within the bin\n",
    "    reco_jet_values_in_bin = [pt for pt in reco_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Check if there are any values in both gen_jet_pt and reco_jet_pt\n",
    "    if len(gen_jet_values_in_bin) == 0 or len(reco_jet_values_in_bin) == 0:\n",
    "        # Append NaN values\n",
    "        ratio_iqr_median.append(np.nan)\n",
    "        continue\n",
    "\n",
    "    # Calculate IQR and median for reco_jet_pt / gen_jet_pt values\n",
    "    ratio_values_in_bin = [reco / gen for reco, gen in zip(reco_jet_values_in_bin, gen_jet_values_in_bin)]\n",
    "    ratio_iqr = np.percentile(ratio_values_in_bin, 75) - np.percentile(ratio_values_in_bin, 25)\n",
    "    ratio_median = np.median(ratio_values_in_bin)\n",
    "    ratio_iqr_median_ratio = ratio_iqr / ratio_median\n",
    "    ratio_iqr_median.append(ratio_iqr_median_ratio)\n",
    "\n",
    "# Filter out NaN values\n",
    "x_vals_filtered = [x for x, y in zip(x_vals, ratio_iqr_median) if not np.isnan(y)]\n",
    "ratio_iqr_median_filtered = [y for y in ratio_iqr_median if not np.isnan(y)]\n",
    "\n",
    "# Create a TGraph with filtered values\n",
    "gr_ratio = ROOT.TGraph(len(x_vals_filtered), np.array(x_vals_filtered), np.array(ratio_iqr_median_filtered))\n",
    "\n",
    "# Set the titles to empty strings\n",
    "gr_ratio.SetTitle(\"\")\n",
    "\n",
    "# Create a canvas\n",
    "canvas = ROOT.TCanvas(\"canvas\", \"Ratio IQR/Median vs Jet pT\", 800, 600)\n",
    "\n",
    "# Draw the graph with connecting lines\n",
    "gr_ratio.SetMarkerStyle(20)\n",
    "gr_ratio.SetMarkerColor(ROOT.kBlue)\n",
    "gr_ratio.SetLineColor(ROOT.kBlue)\n",
    "gr_ratio.GetXaxis().SetTitle(\"Jet PT, Gen (GeV)\")\n",
    "gr_ratio.GetYaxis().SetTitle(\"Response IQR/Median\")\n",
    "gr_ratio.Draw(\"APL\")\n",
    "\n",
    "# Get the X and Y axes\n",
    "xaxis = gr_ratio.GetXaxis()\n",
    "yaxis = gr_ratio.GetYaxis()\n",
    "\n",
    "# Set tick length for all four axes\n",
    "xaxis.SetTickLength(0.03)\n",
    "yaxis.SetTickLength(0.03)\n",
    "\n",
    "\n",
    "# Add legend\n",
    "legend = ROOT.TLegend(0.75, 0.75, 0.9, 0.9)\n",
    "legend.AddEntry(gr_ratio, \"Pred FP\", \"p\")\n",
    "legend.Draw()\n",
    "\n",
    "# Show the canvas\n",
    "canvas.Draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b5b2558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning in <TCanvas::Constructor>: Deleting canvas with same name: canvas\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ROOT\n",
    "import numpy as np\n",
    "\n",
    "# Define bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100, 200]\n",
    "\n",
    "# Initialize lists to store values\n",
    "x_vals = []\n",
    "ratio_iqr_median = []\n",
    "\n",
    "# Iterate over bins\n",
    "for i in range(len(bins) - 1):\n",
    "    lim_low = bins[i]\n",
    "    lim_hi = bins[i + 1]\n",
    "    x_vals.append(np.mean([lim_low, lim_hi]))\n",
    "\n",
    "    # Apply selection to gen_jet indices within the bin\n",
    "    selected_indices = [idx for idx, pt in enumerate(gen_jet_pt) if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Match selected gen_jet indices to reco_jet values\n",
    "    matched_reco_jet_values = [reco_jet_pt[idx] for idx in selected_indices]\n",
    "\n",
    "    # Check if there are any values in both selected gen_jet_pt and matched reco_jet_pt\n",
    "    if len(selected_indices) == 0 or len(matched_reco_jet_values) == 0:\n",
    "        # Append NaN values\n",
    "        ratio_iqr_median.append(np.nan)\n",
    "        continue\n",
    "\n",
    "    # Calculate IQR and median for matched reco_jet_pt / gen_jet_pt values\n",
    "    ratio_values_in_bin = [reco / gen_jet_pt[idx] for reco, idx in zip(matched_reco_jet_values, selected_indices)]\n",
    "    ratio_iqr = np.percentile(ratio_values_in_bin, 75) - np.percentile(ratio_values_in_bin, 25)\n",
    "    ratio_median = np.median(ratio_values_in_bin)\n",
    "    ratio_iqr_median_ratio = ratio_iqr / ratio_median\n",
    "    ratio_iqr_median.append(ratio_iqr_median_ratio)\n",
    "\n",
    "# Filter out NaN values\n",
    "x_vals_filtered = [x for x, y in zip(x_vals, ratio_iqr_median) if not np.isnan(y)]\n",
    "ratio_iqr_median_filtered = [y for y in ratio_iqr_median if not np.isnan(y)]\n",
    "\n",
    "# Create a TGraph with filtered values\n",
    "gr_ratio = ROOT.TGraph(len(x_vals_filtered), np.array(x_vals_filtered), np.array(ratio_iqr_median_filtered))\n",
    "\n",
    "# Set the titles to empty strings\n",
    "gr_ratio.SetTitle(\"\")\n",
    "\n",
    "# Create a canvas\n",
    "canvas = ROOT.TCanvas(\"canvas\", \"Ratio IQR/Median vs Jet pT\", 800, 600)\n",
    "\n",
    "\n",
    "# Draw the graph with connecting lines\n",
    "gr_ratio.SetMarkerStyle(20)\n",
    "gr_ratio.SetMarkerColor(ROOT.kBlue)\n",
    "gr_ratio.SetLineColor(ROOT.kBlue)\n",
    "gr_ratio.GetXaxis().SetTitle(\"Jet PT, Gen (GeV)\")\n",
    "gr_ratio.GetYaxis().SetTitle(\"Response IQR/Median\")\n",
    "gr_ratio.Draw(\"APL\")\n",
    "\n",
    "\n",
    "# Get the X and Y axes\n",
    "xaxis = gr_ratio.GetXaxis()\n",
    "yaxis = gr_ratio.GetYaxis()\n",
    "\n",
    "# Set tick length for all four axes\n",
    "xaxis.SetTickLength(0.03)\n",
    "yaxis.SetTickLength(0.03)\n",
    "\n",
    "# Add legend\n",
    "legend = ROOT.TLegend(0.75, 0.75, 0.9, 0.9)\n",
    "legend.AddEntry(gr_ratio, \"Pred FP\", \"p\")\n",
    "legend.Draw()\n",
    "\n",
    "# Show the canvas\n",
    "canvas.Draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b80ef895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin 1: Number of events (Gen Jet): 9, Number of events (Reco Jet): 9\n",
      "Bin 2: Number of events (Gen Jet): 2, Number of events (Reco Jet): 2\n",
      "Bin 3: Number of events (Gen Jet): 3, Number of events (Reco Jet): 3\n",
      "Bin 4: Number of events (Gen Jet): 8, Number of events (Reco Jet): 8\n",
      "Bin 5: Number of events (Gen Jet): 1, Number of events (Reco Jet): 1\n",
      "Bin 6: Number of events (Gen Jet): 1, Number of events (Reco Jet): 1\n",
      "Bin 7: Number of events (Gen Jet): 1, Number of events (Reco Jet): 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100, 200]\n",
    "\n",
    "# Iterate over bins\n",
    "for i in range(len(bins) - 1):\n",
    "    lim_low = bins[i]\n",
    "    lim_hi = bins[i + 1]\n",
    "\n",
    "    # Apply selection to gen_jet indices within the bin\n",
    "    selected_indices_gen_jet = [idx for idx, pt in enumerate(gen_jet_pt) if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Match selected gen_jet indices to reco_jet values\n",
    "    matched_reco_jet_values = [reco_jet_pt[idx] for idx in selected_indices_gen_jet]\n",
    "\n",
    "    # Calculate the number of events in the bin\n",
    "    num_events_gen_jet = len(selected_indices_gen_jet)\n",
    "    num_events_reco_jet = len(matched_reco_jet_values)\n",
    "\n",
    "    print(f\"Bin {i+1}: Number of events (Gen Jet): {num_events_gen_jet}, Number of events (Reco Jet): {num_events_reco_jet}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "856e8331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ROOT\n",
    "import numpy as np\n",
    "\n",
    "# Define bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100, 200]\n",
    "\n",
    "# Iterate over bins\n",
    "for bin_index in range(len(bins) - 1):\n",
    "    lim_low = bins[bin_index]\n",
    "    lim_hi = bins[bin_index + 1]\n",
    "\n",
    "    # Apply selection to gen_jet indices within the bin\n",
    "    selected_indices_gen_jet = [idx for idx, pt in enumerate(gen_jet_pt) if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Match selected gen_jet indices to reco_jet values\n",
    "    matched_reco_jet_values = [reco_jet_pt[idx] for idx in selected_indices_gen_jet]\n",
    "\n",
    "    # Check if there are any values in both selected gen_jet_pt and matched reco_jet_pt\n",
    "    if len(selected_indices_gen_jet) == 0 or len(matched_reco_jet_values) == 0:\n",
    "        print(f\"No events in Bin {bin_index + 1}.\")\n",
    "        continue\n",
    "\n",
    "    # Calculate IQR and median for matched reco_jet_pt / gen_jet_pt values\n",
    "    ratio_values_in_bin = [reco / gen for reco, gen in zip(matched_reco_jet_values, selected_indices_gen_jet)]\n",
    "    ratio_iqr = np.percentile(ratio_values_in_bin, 75) - np.percentile(ratio_values_in_bin, 25)\n",
    "    ratio_median = np.median(ratio_values_in_bin)\n",
    "    ratio_iqr_median_ratio = ratio_iqr / ratio_median\n",
    "\n",
    "    # Create a canvas for the bin\n",
    "    canvas = ROOT.TCanvas(f\"canvas_{bin_index}\", f\"Response Distribution in Bin {bin_index + 1}\", 800, 600)\n",
    "\n",
    "    # Create histogram for the response distribution in the bin\n",
    "    hist = ROOT.TH1F(f\"hist_{bin_index}\", f\"  Bin {bin_index + 1}\", 20, 0, 2)  # Adjust binning as needed\n",
    "    for val in ratio_values_in_bin:\n",
    "        hist.Fill(val)\n",
    "\n",
    "    # Draw the histogram\n",
    "    hist.SetLineColor(ROOT.kBlue)\n",
    "    hist.Draw()\n",
    "\n",
    "    # Add legend\n",
    "    legend = ROOT.TLegend(0.75, 0.75, 0.9, 0.9)\n",
    "    legend.AddEntry(hist, f\"Bin {bin_index + 1}\", \"l\")\n",
    "    legend.Draw()\n",
    "\n",
    "    # Show the canvas\n",
    "    canvas.Draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed681ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "203a0d91",
   "metadata": {},
   "source": [
    "# Quantization INT8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d2e0d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/ao/quantization/observer.py:1207: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.qconfig = torch.ao.quantization.get_default_qconfig('onednn')\n",
    "custom_module_config = {\n",
    "        \"float_to_observed_custom_module_class\": {torch.nn.MultiheadAttention: QuantizeableMultiheadAttention},\n",
    "        \"observed_to_quantized_custom_module_class\": {QuantizeableMultiheadAttention: QuantizedMultiheadAttention},\n",
    "}\n",
    "\n",
    "model_prepared = torch.ao.quantization.prepare(model, prepare_custom_config_dict=custom_module_config)\n",
    "\n",
    "#calibrate on data\n",
    "num_events_to_calibrate = 100\n",
    "for ind in range(max_events_train,max_events_train+num_events_to_calibrate):\n",
    "    _X = torch.unsqueeze(torch.tensor(ds_train[ind][\"X\"]).to(torch.float32), 0)\n",
    "    _mask = _X[:, :, 0]!=0\n",
    "    model_prepared(_X, _mask)\n",
    "\n",
    "model_int8 = torch.ao.quantization.convert(model_prepared,convert_custom_config_dict=custom_module_config,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12a705b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizeFeaturesStub(\n",
       "  (quants): ModuleList(\n",
       "    (0): Quantize(scale=tensor([0.0078]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "    (1): Quantize(scale=tensor([0.0396]), zero_point=tensor([138]), dtype=torch.quint8)\n",
       "    (2): Quantize(scale=tensor([0.0342]), zero_point=tensor([129]), dtype=torch.quint8)\n",
       "    (3): Quantize(scale=tensor([0.0078]), zero_point=tensor([127]), dtype=torch.quint8)\n",
       "    (4): Quantize(scale=tensor([0.0078]), zero_point=tensor([128]), dtype=torch.quint8)\n",
       "    (5): Quantize(scale=tensor([0.0339]), zero_point=tensor([117]), dtype=torch.quint8)\n",
       "    (6): Quantize(scale=tensor([49.7997]), zero_point=tensor([62]), dtype=torch.quint8)\n",
       "    (7): Quantize(scale=tensor([22.5108]), zero_point=tensor([128]), dtype=torch.quint8)\n",
       "    (8): Quantize(scale=tensor([30.9407]), zero_point=tensor([130]), dtype=torch.quint8)\n",
       "    (9): Quantize(scale=tensor([0.0122]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "    (10): Quantize(scale=tensor([3.2249]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "    (11): Quantize(scale=tensor([0.3067]), zero_point=tensor([25]), dtype=torch.quint8)\n",
       "    (12): Quantize(scale=tensor([0.4297]), zero_point=tensor([130]), dtype=torch.quint8)\n",
       "    (13): Quantize(scale=tensor([8.0287]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "    (14): Quantize(scale=tensor([9.5126]), zero_point=tensor([151]), dtype=torch.quint8)\n",
       "    (15): Quantize(scale=tensor([4.2339]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "    (16): Quantize(scale=tensor([3.6354]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "    (17-19): 3 x Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_int8.quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "999e620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_quantized = torch.quantize_per_tensor((X_features_padded[:, :, 0]!=0).to(torch.float32), 1, 0, torch.quint8)\n",
    "preds = model_int8(X_features_padded, mask_quantized)\n",
    "preds = preds[0].detach(), preds[1].detach()\n",
    "preds_unpacked_int8 = unpack_predictions(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "272d479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_int8 = mlpf_loss(targets_unpacked, preds_unpacked_int8, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d010742f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Final total loss')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(2), [loss[\"Total\"].detach().numpy(), loss_int8[\"Total\"].detach().numpy()])\n",
    "plt.xticks(range(2), [\"fp32\", \"int8\"])\n",
    "plt.ylabel(\"Final total loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2214072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_pred_int8 = preds_unpacked_int8[\"pt\"][msk_true_particles].numpy()\n",
    "eta_pred_int8 = preds_unpacked_int8[\"eta\"][msk_true_particles].numpy()\n",
    "sphi_pred_int8 = preds_unpacked_int8[\"sin_phi\"][msk_true_particles].numpy()\n",
    "cphi_pred_int8 = preds_unpacked_int8[\"cos_phi\"][msk_true_particles].numpy()\n",
    "energy_pred_int8 = preds_unpacked_int8[\"energy\"][msk_true_particles].numpy()\n",
    "\n",
    "px = preds_unpacked_int8[\"pt\"] * preds_unpacked_int8[\"cos_phi\"] * msk_true_particles\n",
    "py = preds_unpacked_int8[\"pt\"] * preds_unpacked_int8[\"sin_phi\"] * msk_true_particles\n",
    "pred_met_int8 = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07044dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fda66df6dc0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(pred_met/true_met, bins=np.linspace(0,5,61), histtype=\"step\", lw=2, label=\"fp32\");\n",
    "plt.hist(pred_met_int8/true_met, bins=np.linspace(0,5,61), histtype=\"step\", lw=2, label=\"int8\");\n",
    "plt.xlabel(\"reco_met / gen_met\")\n",
    "plt.ylabel(\"number of events / bin\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b3255af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the 3-momentum for the quantized particles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7fe474c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "px_int8 = preds_unpacked_int8[\"pt\"] * preds_unpacked_int8[\"cos_phi\"] * msk_true_particles\n",
    "py_int8 = preds_unpacked_int8[\"pt\"] * preds_unpacked_int8[\"sin_phi\"] * msk_true_particles\n",
    "pz_int8 = preds_unpacked_int8[\"pt\"] * np.sinh(preds_unpacked_int8[\"eta\"]) * msk_true_particles\n",
    "phi_int8 = np.arctan2(preds_unpacked_int8[\"sin_phi\"], preds_unpacked_int8[\"cos_phi\"]) * msk_true_particles\n",
    "\n",
    "px_np_int8 = px_int8.detach().cpu().numpy()\n",
    "py_np_int8 = py_int8.detach().cpu().numpy()\n",
    "pz_np_int8 = pz_int8.detach().cpu().numpy()\n",
    "phi_np_int8 = phi_int8.detach().cpu().numpy()\n",
    "\n",
    "# print(\"px_np\", px_np)\n",
    "# print(\"py_np\", py_np)\n",
    "# print(\"pz_np\", pz_np)\n",
    "# print(\"phi_np\", phi_np)\n",
    "\n",
    "quantized_mom = np.sqrt(np.sum(px_np_int8, axis=1)**2 + np.sum(py_np_int8, axis=1)**2 + np.sum(pz_np_int8, axis=1)**2)\n",
    "int8_E_np = np.sqrt(px_np_int8**2 + py_np_int8**2 + pz_np_int8**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f59b8cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int8_E Shape (16, 200)\n",
      "px Shape (16, 200)\n",
      "py Shape (16, 200)\n",
      "pz Shape (16, 200)\n"
     ]
    }
   ],
   "source": [
    "# four-vectors\n",
    "# px_py_pz_E = np.column_stack((px_np, py_np, pz_np, E)) \n",
    "print(\"int8_E Shape\", int8_E_np.shape)\n",
    "print(\"px Shape\", px_np_int8.shape)\n",
    "print(\"py Shape\", py_np_int8.shape)\n",
    "print(\"pz Shape\", pz_np_int8.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cbb0d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INT8_Jets_particles = []   # TODO:Change this to reco_jets\n",
    "for ip in range(int8_E_np.shape[0]):\n",
    "    for ix in range(int8_E_np.shape[1]):\n",
    "        px_value = float(px_np_int8[ip, ix])\n",
    "        py_value = float(py_np_int8[ip, ix])\n",
    "        pz_value = float(pz_np_int8[ip, ix])\n",
    "        E_value = float(int8_E_np[ip, ix])\n",
    "        particle = fj.PseudoJet(px_value, py_value, pz_value, E_value)\n",
    "        INT8_Jets_particles.append(particle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "983511e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INT8_Jets_particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1602ea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "num_threads = 2 \n",
    "\n",
    "def cluster_jets(particles):\n",
    "    jetdef = fj.JetDefinition(fj.antikt_algorithm, 0.4)\n",
    "#     jet_ptcut = 20\n",
    "    \n",
    "    cluster = fj.ClusterSequence(INT8_Jets_particles, jetdef)\n",
    "    jets = cluster.inclusive_jets()\n",
    "    \n",
    "    return jets\n",
    "\n",
    "chunks = [particles[i::num_threads] for i in range(num_threads)]\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "    futures = [executor.submit(cluster_jets, chunk) for chunk in chunks]\n",
    "\n",
    "INT8_reco_jets = []\n",
    "for future in futures:\n",
    "    INT8_reco_jets.extend(future.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "39c90542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jet 1 : 0.0 100000.0 0.0 0.0\n",
      "Jet 2 : 0.042896021468405335 0.28781828540681575 5.077070008271944 0.044685062021017075\n",
      "Jet 3 : 0.05048450439488717 -2.9105019810120414 2.950329933305678 0.46497398614883423\n",
      "Jet 4 : 0.2623754022198784 -4.026813416016047 0.4607782343929308 7.3595967292785645\n",
      "Jet 5 : 0.30600831951709684 -4.362343938649678 5.250558280556705 12.003716468811035\n",
      "Jet 6 : 0.5067151736707779 0.7875412244189958 3.5862781296310144 0.67214435338974\n",
      "Jet 7 : 0.6010212862610597 -3.5409234801126344 0.22132971003957175 10.375956535339355\n",
      "Jet 8 : 0.6487825550245411 -3.947957004875568 1.6607264212054387 16.819273471832275\n",
      "Jet 9 : 0.6661709856550436 -3.158910005574564 1.9913328398502137 7.856617450714111\n",
      "Jet 10 : 0.8367933342235239 2.581819108752742 3.542584139366838 5.563338279724121\n",
      "Jet 11 : 1.1025917268919003 1.4188344006907518 3.379453132508136 2.411531925201416\n",
      "Jet 12 : 1.1628402659739723 -3.3834859336681262 4.48114216899547 17.156118392944336\n",
      "Jet 13 : 1.1899790561526709 -3.2631542284810373 6.085463640796483 15.57093620300293\n",
      "Jet 14 : 1.2174030729120064 -3.5938702834818814 1.3183388131951626 22.15799903869629\n",
      "Jet 15 : 1.4164392717987537 4.705041366541018 6.1996511138931005 78.2667007446289\n",
      "Jet 16 : 1.4708324817722227 0.7332973975900844 4.61586810748538 1.8883703351020813\n",
      "Jet 17 : 1.916148822003898 2.79920540290472 2.1569480035843545 15.801888465881348\n",
      "Jet 18 : 1.9430896343238657 -2.230717532780804 0.9833434441388534 9.146116256713867\n",
      "Jet 19 : 1.9736286594023333 3.2221295925745923 3.0979933329525937 24.79010581970215\n",
      "Jet 20 : 2.2682807791300883 -1.0702547959119784 4.208827259150869 3.696424186229706\n",
      "Jet 21 : 2.305333832721721 -1.2404409289674057 1.7666987887347574 4.318344593048096\n",
      "Jet 22 : 2.4900192928899614 3.4128515228757887 2.3320864687048624 37.830748558044434\n",
      "Jet 23 : 2.4973866218879133 -1.8683804178065104 4.3241090801043685 8.281546592712402\n",
      "Jet 24 : 2.513680151301318 2.192302300479524 3.7207000023678787 11.396380126476288\n",
      "Jet 25 : 2.6407794555221527 -2.57051191248577 5.383510646610109 17.361820220947266\n",
      "Jet 26 : 2.729243214219466 0.9391108170024977 0.30670324016722594 4.027019485831261\n",
      "Jet 27 : 3.188651806023038 -2.7838690979580565 1.8607502522778727 25.89987850189209\n",
      "Jet 28 : 2.9149981865223644 -0.8211253959259233 2.0672740701600025 3.9541842937469482\n",
      "Jet 29 : 3.2691386531111912 1.1156484205159694 4.948394963839672 5.523616790771484\n",
      "Jet 30 : 3.341370582725543 -2.0070678368040804 2.447564308655465 12.65685749053955\n",
      "Jet 31 : 3.5233209531659524 -0.32302306851807117 5.474180176845253 3.7087433338165283\n",
      "Jet 32 : 3.5599732174864163 2.56231423486316 1.7596318532501054 23.220162391662598\n",
      "Jet 33 : 3.5594432195377768 -1.7197532401090458 6.130858101431677 10.255220413208008\n",
      "Jet 34 : 3.80200183292518 1.072169304282596 1.7795285000472787 6.2048211097717285\n",
      "Jet 35 : 4.126240641864167 2.615797863829362 5.450787411463872 28.372966766357422\n",
      "Jet 36 : 4.49444148977964 1.4501348201286655 5.253667599697199 10.108510971069336\n",
      "Jet 37 : 4.996653944396501 -1.8158621544175737 4.7514397075087516 15.777231931686401\n",
      "Jet 38 : 4.728357019467551 3.2991401177370943 5.799525751853987 64.13144874572754\n",
      "Jet 39 : 5.302811740163217 -3.040303760273555 5.430006983972476 55.57622468471527\n",
      "Jet 40 : 6.912091287251132 -2.7817939156443074 5.823646589174096 56.036927700042725\n",
      "Jet 41 : 5.809367317645437 2.0235245720849475 2.816993658220651 22.35781915485859\n",
      "Jet 42 : 7.900171980529625 -2.5611957377888497 4.698027834506109 51.48495864868164\n",
      "Jet 43 : 7.525714532317134 2.7906563706039837 5.9657581368967705 61.54724597930908\n",
      "Jet 44 : 8.626037176837329 2.3793116173785394 5.924832839710763 46.993045806884766\n",
      "Jet 45 : 14.713553117688738 1.6203665141852748 5.99488954545105 38.79111671447754\n",
      "Jet 46 : 7.552752007172549 -0.06968296194018551 4.074102794989107 7.571096420288086\n",
      "Jet 47 : 10.084970402147231 -3.266197031588419 0.8274047813242396 132.37844944000244\n",
      "Jet 48 : 8.51582312015258 -2.5917120748336973 3.9141906426900044 57.1734414100647\n",
      "Jet 49 : 8.54762720623228 0.716919946727014 5.34625017933427 10.83996868133545\n",
      "Jet 50 : 8.91367899996295 1.6767279453503177 4.623519698025664 24.694725275039673\n",
      "Jet 51 : 10.283425324695246 -2.3092178157203755 1.9917088295515286 52.293808937072754\n",
      "Jet 52 : 9.232058392519752 0.03953093447860013 0.49436774266898453 9.292068183422089\n",
      "Jet 53 : 9.650795749723292 0.3465865779410999 1.2574741539006427 10.288460731506348\n",
      "Jet 54 : 20.277636837565495 0.8101137373503537 5.952864690780622 27.75037518143654\n",
      "Jet 55 : 10.286716946733874 0.08448402713909203 6.053359892981841 10.368640661239624\n",
      "Jet 56 : 10.572684841706783 -0.16841793512051753 1.4640216190749054 10.825427293777466\n",
      "Jet 57 : 25.151786326622947 -2.8048339904139628 0.3596878526160879 208.66809034347534\n",
      "Jet 58 : 14.51722245827294 -3.013914990500147 1.50650750327557 148.2211742401123\n",
      "Jet 59 : 18.288428636412505 0.48900706066391536 0.18537596876291546 20.820199951529503\n",
      "Jet 60 : 21.795734981367158 -2.6747789178337196 1.0193045289510756 158.9570164680481\n",
      "Jet 61 : 23.817895795421567 -0.6776085186550909 5.753112628565197 30.05847518146038\n",
      "Jet 62 : 25.391724711409505 -0.7317425468444969 1.401929750542586 33.01580882072449\n",
      "Jet 63 : 12.684827872484215 3.8305146084392074 0.7086885508219669 292.4389762878418\n",
      "Jet 64 : 16.55672919342653 -1.3219933134760125 0.017953028546234505 33.415549993515015\n",
      "Jet 65 : 14.4638069042947 2.7099283870508493 0.7552964373502129 109.16851758956909\n",
      "Jet 66 : 15.728722558403085 -2.072442387647789 5.7147514806034225 63.49968719482422\n",
      "Jet 67 : 15.90174182278087 -2.261717669548425 4.1548224488423555 77.19127631187439\n",
      "Jet 68 : 21.69155706013458 -1.4510998924257144 5.635380045039704 49.040611267089844\n",
      "Jet 69 : 18.39699823987733 -0.7949865677183744 0.2817435591477374 24.69789233803749\n",
      "Jet 70 : 20.13255968451716 -0.3203406646934932 0.25204641089818314 21.493557691574097\n",
      "Jet 71 : 17.59927833267218 3.2127922552620234 0.9664560349570868 219.0189208984375\n",
      "Jet 72 : 23.7707264482665 0.2769314348999508 1.7105439332428618 24.97911310195923\n",
      "Jet 73 : 34.20551716617585 -1.7323196572043509 1.7365053197816698 99.97253847122192\n",
      "Jet 74 : 24.29522559832774 -2.148538536629023 5.26330470557027 105.64615440368652\n",
      "Jet 75 : 24.207217949930932 2.7148601597081123 3.083887699357279 183.64757752418518\n",
      "Jet 76 : 40.90516587518888 -2.1890976775389097 6.14032036339049 185.12443709373474\n",
      "Jet 77 : 23.088545966498135 2.1431598378688803 4.810600227261097 99.85838508605957\n",
      "Jet 78 : 28.37725504154881 2.324639649700366 2.427634719304411 146.5762640624307\n",
      "Jet 79 : 22.240153097058574 2.5002563883267874 1.2797412958853924 136.46600818634033\n",
      "Jet 80 : 21.918482775979935 2.5457091376577177 4.55217474577254 140.63394737243652\n",
      "Jet 81 : 35.749310184062374 1.9653352828396975 0.1780651705454512 130.26742953062057\n",
      "Jet 82 : 27.672172974215705 1.9801735763627188 5.553879771742505 102.25956058502197\n",
      "Jet 83 : 26.398829926668117 1.309122549467527 2.19937778673369 52.545250713825226\n",
      "Jet 84 : 44.08885170063238 -1.6610793475361616 0.34214411182486254 120.6432728767395\n",
      "Jet 85 : 70.34953230416305 1.247112819213975 4.049978529105964 133.7806363105774\n",
      "Jet 86 : 42.33179751952645 2.0817937869960144 1.7725697061054382 172.5635757446289\n",
      "Jet 87 : 31.051925974923194 -1.2143323603316405 2.7744285744627977 57.043797969818115\n",
      "Jet 88 : 58.02071457346972 3.140512087547765 0.42151187868845086 671.9808106422424\n",
      "Jet 89 : 74.11715520111586 -2.1901721111433994 0.514678370800596 335.8001526594162\n",
      "Jet 90 : 40.49206590700664 -2.459585565674423 3.3850506612442834 238.6330542564392\n",
      "Jet 91 : 48.39438494821236 -1.2869306378529661 1.3616198762978513 94.76544988155365\n",
      "Jet 92 : 130.46202287571265 -0.3794429636892167 0.8440402307917519 143.26753736659884\n",
      "Jet 93 : 55.466947990391574 1.0454239382655404 1.3769918437462034 89.19150367379189\n",
      "Jet 94 : 95.9250365139796 -2.351368523862515 1.4593980351416902 508.6838208436966\n",
      "Jet 95 : 45.21475826561299 2.6524270044318414 4.125509508246989 322.38049364089966\n",
      "Jet 96 : 70.99004523565577 2.1011969166864564 1.1717295894813655 294.8598771095276\n",
      "Jet 97 : 58.246357840989866 0.5084527898094648 4.078281055822133 66.70750117301941\n",
      "Jet 98 : 91.5515329713781 2.523576036619783 0.24859314931883753 575.049186706543\n",
      "Jet 99 : 52.70311220665374 -0.4581533425352106 3.456053080222644 58.49049687385559\n",
      "Jet 100 : 84.96641795260331 -2.1394445678400915 3.7434128625582925 366.30638743937016\n",
      "Jet 101 : 56.35405286020733 -2.2438549484621624 2.813792796571015 268.716402053833\n",
      "Jet 102 : 127.26661048265208 0.36685174645219853 0.8264625408306502 139.526719879359\n",
      "Jet 103 : 188.6454375550947 -1.8521343593291588 0.9946168336228198 618.5612887740135\n",
      "Jet 104 : 76.73648352285163 -1.6516685238243627 3.2074002447355667 208.12503588199615\n",
      "Jet 105 : 135.32197874122113 1.606383576427109 1.3160488245342716 352.38024113141\n",
      "Jet 106 : 126.51804909450472 1.4234931367415231 0.249150429445947 279.70590978860855\n",
      "Jet 107 : 230.82392881350393 -1.2081386553569784 0.8111647402702008 425.3538479208946\n",
      "Jet 108 : 85.81219505848104 -0.9984878604338469 3.434031910992161 133.1160652935505\n",
      "Jet 109 : 88.87134882636673 2.1726481684758445 3.320146811434594 395.52654671669006\n",
      "Jet 110 : 133.7025587448014 -0.6225100794640183 4.354651675322445 161.9413719177246\n",
      "Jet 111 : 431.0923296597571 2.279194837459981 0.6986149800736863 2129.86682677269\n",
      "Jet 112 : 349.2249438791375 2.020566314677983 4.205146350955067 1343.0333963632584\n",
      "Jet 113 : 467.93817637735395 1.0306461180463897 0.783955001271642 747.3268901705742\n",
      "Jet 114 : 931.9578889545319 1.6714258274750053 0.7297199115069526 2576.893737182021\n",
      "Jet 115 : 424.9736487899823 -1.597013839718099 3.925014243988704 1097.0097351968288\n",
      "Jet 116 : 500.5258491970993 -0.6861336717591854 3.935347290188496 635.0578958876431\n",
      "Jet 117 : 0.0 100000.0 0.0 0.0\n",
      "Jet 118 : 0.042896021468405335 0.28781828540681575 5.077070008271944 0.044685062021017075\n",
      "Jet 119 : 0.05048450439488717 -2.9105019810120414 2.950329933305678 0.46497398614883423\n",
      "Jet 120 : 0.2623754022198784 -4.026813416016047 0.4607782343929308 7.3595967292785645\n",
      "Jet 121 : 0.30600831951709684 -4.362343938649678 5.250558280556705 12.003716468811035\n",
      "Jet 122 : 0.5067151736707779 0.7875412244189958 3.5862781296310144 0.67214435338974\n",
      "Jet 123 : 0.6010212862610597 -3.5409234801126344 0.22132971003957175 10.375956535339355\n",
      "Jet 124 : 0.6487825550245411 -3.947957004875568 1.6607264212054387 16.819273471832275\n",
      "Jet 125 : 0.6661709856550436 -3.158910005574564 1.9913328398502137 7.856617450714111\n",
      "Jet 126 : 0.8367933342235239 2.581819108752742 3.542584139366838 5.563338279724121\n",
      "Jet 127 : 1.1025917268919003 1.4188344006907518 3.379453132508136 2.411531925201416\n",
      "Jet 128 : 1.1628402659739723 -3.3834859336681262 4.48114216899547 17.156118392944336\n",
      "Jet 129 : 1.1899790561526709 -3.2631542284810373 6.085463640796483 15.57093620300293\n",
      "Jet 130 : 1.2174030729120064 -3.5938702834818814 1.3183388131951626 22.15799903869629\n",
      "Jet 131 : 1.4164392717987537 4.705041366541018 6.1996511138931005 78.2667007446289\n",
      "Jet 132 : 1.4708324817722227 0.7332973975900844 4.61586810748538 1.8883703351020813\n",
      "Jet 133 : 1.916148822003898 2.79920540290472 2.1569480035843545 15.801888465881348\n",
      "Jet 134 : 1.9430896343238657 -2.230717532780804 0.9833434441388534 9.146116256713867\n",
      "Jet 135 : 1.9736286594023333 3.2221295925745923 3.0979933329525937 24.79010581970215\n",
      "Jet 136 : 2.2682807791300883 -1.0702547959119784 4.208827259150869 3.696424186229706\n",
      "Jet 137 : 2.305333832721721 -1.2404409289674057 1.7666987887347574 4.318344593048096\n",
      "Jet 138 : 2.4900192928899614 3.4128515228757887 2.3320864687048624 37.830748558044434\n",
      "Jet 139 : 2.4973866218879133 -1.8683804178065104 4.3241090801043685 8.281546592712402\n",
      "Jet 140 : 2.513680151301318 2.192302300479524 3.7207000023678787 11.396380126476288\n",
      "Jet 141 : 2.6407794555221527 -2.57051191248577 5.383510646610109 17.361820220947266\n",
      "Jet 142 : 2.729243214219466 0.9391108170024977 0.30670324016722594 4.027019485831261\n",
      "Jet 143 : 3.188651806023038 -2.7838690979580565 1.8607502522778727 25.89987850189209\n",
      "Jet 144 : 2.9149981865223644 -0.8211253959259233 2.0672740701600025 3.9541842937469482\n",
      "Jet 145 : 3.2691386531111912 1.1156484205159694 4.948394963839672 5.523616790771484\n",
      "Jet 146 : 3.341370582725543 -2.0070678368040804 2.447564308655465 12.65685749053955\n",
      "Jet 147 : 3.5233209531659524 -0.32302306851807117 5.474180176845253 3.7087433338165283\n",
      "Jet 148 : 3.5599732174864163 2.56231423486316 1.7596318532501054 23.220162391662598\n",
      "Jet 149 : 3.5594432195377768 -1.7197532401090458 6.130858101431677 10.255220413208008\n",
      "Jet 150 : 3.80200183292518 1.072169304282596 1.7795285000472787 6.2048211097717285\n",
      "Jet 151 : 4.126240641864167 2.615797863829362 5.450787411463872 28.372966766357422\n",
      "Jet 152 : 4.49444148977964 1.4501348201286655 5.253667599697199 10.108510971069336\n",
      "Jet 153 : 4.996653944396501 -1.8158621544175737 4.7514397075087516 15.777231931686401\n",
      "Jet 154 : 4.728357019467551 3.2991401177370943 5.799525751853987 64.13144874572754\n",
      "Jet 155 : 5.302811740163217 -3.040303760273555 5.430006983972476 55.57622468471527\n",
      "Jet 156 : 6.912091287251132 -2.7817939156443074 5.823646589174096 56.036927700042725\n",
      "Jet 157 : 5.809367317645437 2.0235245720849475 2.816993658220651 22.35781915485859\n",
      "Jet 158 : 7.900171980529625 -2.5611957377888497 4.698027834506109 51.48495864868164\n",
      "Jet 159 : 7.525714532317134 2.7906563706039837 5.9657581368967705 61.54724597930908\n",
      "Jet 160 : 8.626037176837329 2.3793116173785394 5.924832839710763 46.993045806884766\n",
      "Jet 161 : 14.713553117688738 1.6203665141852748 5.99488954545105 38.79111671447754\n",
      "Jet 162 : 7.552752007172549 -0.06968296194018551 4.074102794989107 7.571096420288086\n",
      "Jet 163 : 10.084970402147231 -3.266197031588419 0.8274047813242396 132.37844944000244\n",
      "Jet 164 : 8.51582312015258 -2.5917120748336973 3.9141906426900044 57.1734414100647\n",
      "Jet 165 : 8.54762720623228 0.716919946727014 5.34625017933427 10.83996868133545\n",
      "Jet 166 : 8.91367899996295 1.6767279453503177 4.623519698025664 24.694725275039673\n",
      "Jet 167 : 10.283425324695246 -2.3092178157203755 1.9917088295515286 52.293808937072754\n",
      "Jet 168 : 9.232058392519752 0.03953093447860013 0.49436774266898453 9.292068183422089\n",
      "Jet 169 : 9.650795749723292 0.3465865779410999 1.2574741539006427 10.288460731506348\n",
      "Jet 170 : 20.277636837565495 0.8101137373503537 5.952864690780622 27.75037518143654\n",
      "Jet 171 : 10.286716946733874 0.08448402713909203 6.053359892981841 10.368640661239624\n",
      "Jet 172 : 10.572684841706783 -0.16841793512051753 1.4640216190749054 10.825427293777466\n",
      "Jet 173 : 25.151786326622947 -2.8048339904139628 0.3596878526160879 208.66809034347534\n",
      "Jet 174 : 14.51722245827294 -3.013914990500147 1.50650750327557 148.2211742401123\n",
      "Jet 175 : 18.288428636412505 0.48900706066391536 0.18537596876291546 20.820199951529503\n",
      "Jet 176 : 21.795734981367158 -2.6747789178337196 1.0193045289510756 158.9570164680481\n",
      "Jet 177 : 23.817895795421567 -0.6776085186550909 5.753112628565197 30.05847518146038\n",
      "Jet 178 : 25.391724711409505 -0.7317425468444969 1.401929750542586 33.01580882072449\n",
      "Jet 179 : 12.684827872484215 3.8305146084392074 0.7086885508219669 292.4389762878418\n",
      "Jet 180 : 16.55672919342653 -1.3219933134760125 0.017953028546234505 33.415549993515015\n",
      "Jet 181 : 14.4638069042947 2.7099283870508493 0.7552964373502129 109.16851758956909\n",
      "Jet 182 : 15.728722558403085 -2.072442387647789 5.7147514806034225 63.49968719482422\n",
      "Jet 183 : 15.90174182278087 -2.261717669548425 4.1548224488423555 77.19127631187439\n",
      "Jet 184 : 21.69155706013458 -1.4510998924257144 5.635380045039704 49.040611267089844\n",
      "Jet 185 : 18.39699823987733 -0.7949865677183744 0.2817435591477374 24.69789233803749\n",
      "Jet 186 : 20.13255968451716 -0.3203406646934932 0.25204641089818314 21.493557691574097\n",
      "Jet 187 : 17.59927833267218 3.2127922552620234 0.9664560349570868 219.0189208984375\n",
      "Jet 188 : 23.7707264482665 0.2769314348999508 1.7105439332428618 24.97911310195923\n",
      "Jet 189 : 34.20551716617585 -1.7323196572043509 1.7365053197816698 99.97253847122192\n",
      "Jet 190 : 24.29522559832774 -2.148538536629023 5.26330470557027 105.64615440368652\n",
      "Jet 191 : 24.207217949930932 2.7148601597081123 3.083887699357279 183.64757752418518\n",
      "Jet 192 : 40.90516587518888 -2.1890976775389097 6.14032036339049 185.12443709373474\n",
      "Jet 193 : 23.088545966498135 2.1431598378688803 4.810600227261097 99.85838508605957\n",
      "Jet 194 : 28.37725504154881 2.324639649700366 2.427634719304411 146.5762640624307\n",
      "Jet 195 : 22.240153097058574 2.5002563883267874 1.2797412958853924 136.46600818634033\n",
      "Jet 196 : 21.918482775979935 2.5457091376577177 4.55217474577254 140.63394737243652\n",
      "Jet 197 : 35.749310184062374 1.9653352828396975 0.1780651705454512 130.26742953062057\n",
      "Jet 198 : 27.672172974215705 1.9801735763627188 5.553879771742505 102.25956058502197\n",
      "Jet 199 : 26.398829926668117 1.309122549467527 2.19937778673369 52.545250713825226\n",
      "Jet 200 : 44.08885170063238 -1.6610793475361616 0.34214411182486254 120.6432728767395\n",
      "Jet 201 : 70.34953230416305 1.247112819213975 4.049978529105964 133.7806363105774\n",
      "Jet 202 : 42.33179751952645 2.0817937869960144 1.7725697061054382 172.5635757446289\n",
      "Jet 203 : 31.051925974923194 -1.2143323603316405 2.7744285744627977 57.043797969818115\n",
      "Jet 204 : 58.02071457346972 3.140512087547765 0.42151187868845086 671.9808106422424\n",
      "Jet 205 : 74.11715520111586 -2.1901721111433994 0.514678370800596 335.8001526594162\n",
      "Jet 206 : 40.49206590700664 -2.459585565674423 3.3850506612442834 238.6330542564392\n",
      "Jet 207 : 48.39438494821236 -1.2869306378529661 1.3616198762978513 94.76544988155365\n",
      "Jet 208 : 130.46202287571265 -0.3794429636892167 0.8440402307917519 143.26753736659884\n",
      "Jet 209 : 55.466947990391574 1.0454239382655404 1.3769918437462034 89.19150367379189\n",
      "Jet 210 : 95.9250365139796 -2.351368523862515 1.4593980351416902 508.6838208436966\n",
      "Jet 211 : 45.21475826561299 2.6524270044318414 4.125509508246989 322.38049364089966\n",
      "Jet 212 : 70.99004523565577 2.1011969166864564 1.1717295894813655 294.8598771095276\n",
      "Jet 213 : 58.246357840989866 0.5084527898094648 4.078281055822133 66.70750117301941\n",
      "Jet 214 : 91.5515329713781 2.523576036619783 0.24859314931883753 575.049186706543\n",
      "Jet 215 : 52.70311220665374 -0.4581533425352106 3.456053080222644 58.49049687385559\n",
      "Jet 216 : 84.96641795260331 -2.1394445678400915 3.7434128625582925 366.30638743937016\n",
      "Jet 217 : 56.35405286020733 -2.2438549484621624 2.813792796571015 268.716402053833\n",
      "Jet 218 : 127.26661048265208 0.36685174645219853 0.8264625408306502 139.526719879359\n",
      "Jet 219 : 188.6454375550947 -1.8521343593291588 0.9946168336228198 618.5612887740135\n",
      "Jet 220 : 76.73648352285163 -1.6516685238243627 3.2074002447355667 208.12503588199615\n",
      "Jet 221 : 135.32197874122113 1.606383576427109 1.3160488245342716 352.38024113141\n",
      "Jet 222 : 126.51804909450472 1.4234931367415231 0.249150429445947 279.70590978860855\n",
      "Jet 223 : 230.82392881350393 -1.2081386553569784 0.8111647402702008 425.3538479208946\n",
      "Jet 224 : 85.81219505848104 -0.9984878604338469 3.434031910992161 133.1160652935505\n",
      "Jet 225 : 88.87134882636673 2.1726481684758445 3.320146811434594 395.52654671669006\n",
      "Jet 226 : 133.7025587448014 -0.6225100794640183 4.354651675322445 161.9413719177246\n",
      "Jet 227 : 431.0923296597571 2.279194837459981 0.6986149800736863 2129.86682677269\n",
      "Jet 228 : 349.2249438791375 2.020566314677983 4.205146350955067 1343.0333963632584\n",
      "Jet 229 : 467.93817637735395 1.0306461180463897 0.783955001271642 747.3268901705742\n",
      "Jet 230 : 931.9578889545319 1.6714258274750053 0.7297199115069526 2576.893737182021\n",
      "Jet 231 : 424.9736487899823 -1.597013839718099 3.925014243988704 1097.0097351968288\n",
      "Jet 232 : 500.5258491970993 -0.6861336717591854 3.935347290188496 635.0578958876431\n"
     ]
    }
   ],
   "source": [
    "for i, jet in enumerate(INT8_reco_jets):\n",
    "    print(\"Jet\", i+1, \":\", jet.pt(), jet.eta(), jet.phi(), jet.e())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "74cd0cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Jets: 232\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Jets:\", len(INT8_reco_jets))\n",
    "\n",
    "\n",
    "INT8_jet_pt = [jet.pt() for jet in INT8_reco_jets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "125547ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ROOT\n",
    "import numpy as np\n",
    "\n",
    "# Define bins\n",
    "bins = [10, 20, 30, 40, 60, 80, 100, 200]\n",
    "\n",
    "# Initialize lists to store values\n",
    "x_vals = []\n",
    "reco_ratio_iqr_median = []\n",
    "int8_ratio_iqr_median = []\n",
    "\n",
    "# Iterate over bins\n",
    "for i in range(len(bins) - 1):\n",
    "    lim_low = bins[i]\n",
    "    lim_hi = bins[i + 1]\n",
    "    x_vals.append(np.mean([lim_low, lim_hi]))\n",
    "\n",
    "    # Filter gen_jet_pt values within the bin\n",
    "    gen_jet_values_in_bin = [pt for pt in gen_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Filter reco_jet_pt values within the bin\n",
    "    reco_jet_values_in_bin = [pt for pt in reco_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Filter INT8_jet_pt values within the bin\n",
    "    INT8_jet_values_in_bin = [pt for pt in INT8_jet_pt if lim_low < pt <= lim_hi]\n",
    "\n",
    "    # Check if there are any values in both gen_jet_pt and reco_jet_pt\n",
    "    if len(gen_jet_values_in_bin) == 0 or len(reco_jet_values_in_bin) == 0 or len(INT8_jet_values_in_bin) == 0:\n",
    "        # Append NaN values\n",
    "        reco_ratio_iqr_median.append(np.nan)\n",
    "        int8_ratio_iqr_median.append(np.nan)\n",
    "        continue\n",
    "\n",
    "    # Calculate IQR and median for reco_jet_pt / gen_jet_pt values\n",
    "    reco_ratio_values_in_bin = [reco / gen for reco, gen in zip(reco_jet_values_in_bin, gen_jet_values_in_bin)]\n",
    "    reco_ratio_iqr = np.percentile(reco_ratio_values_in_bin, 75) - np.percentile(reco_ratio_values_in_bin, 25)\n",
    "    reco_ratio_median = np.median(reco_ratio_values_in_bin)\n",
    "    reco_ratio_iqr_median_ratio = reco_ratio_iqr / reco_ratio_median\n",
    "    reco_ratio_iqr_median.append(reco_ratio_iqr_median_ratio)\n",
    "\n",
    "    # Calculate IQR and median for INT8_jet_pt / gen_jet_pt values\n",
    "    int8_ratio_values_in_bin = [int8 / gen for int8, gen in zip(INT8_jet_values_in_bin, gen_jet_values_in_bin)]\n",
    "    int8_ratio_iqr = np.percentile(int8_ratio_values_in_bin, 75) - np.percentile(int8_ratio_values_in_bin, 25)\n",
    "    int8_ratio_median = np.median(int8_ratio_values_in_bin)\n",
    "    int8_ratio_iqr_median_ratio = int8_ratio_iqr / int8_ratio_median\n",
    "    int8_ratio_iqr_median.append(int8_ratio_iqr_median_ratio)\n",
    "\n",
    "# Filter out NaN values\n",
    "x_vals_filtered_reco = [x for x, y in zip(x_vals, reco_ratio_iqr_median) if not np.isnan(y)]\n",
    "reco_ratio_iqr_median_filtered = [y for y in reco_ratio_iqr_median if not np.isnan(y)]\n",
    "\n",
    "x_vals_filtered_int8 = [x for x, y in zip(x_vals, int8_ratio_iqr_median) if not np.isnan(y)]\n",
    "int8_ratio_iqr_median_filtered = [y for y in int8_ratio_iqr_median if not np.isnan(y)]\n",
    "\n",
    "# Create TGraphs with filtered values\n",
    "gr_reco_ratio = ROOT.TGraph(len(x_vals_filtered_reco), np.array(x_vals_filtered_reco), np.array(reco_ratio_iqr_median_filtered))\n",
    "gr_int8_ratio = ROOT.TGraph(len(x_vals_filtered_int8), np.array(x_vals_filtered_int8), np.array(int8_ratio_iqr_median_filtered))\n",
    "\n",
    "# Set the titles to empty strings\n",
    "gr_reco_ratio.SetTitle(\"\")\n",
    "gr_int8_ratio.SetTitle(\"\")\n",
    "\n",
    "# Create a canvas\n",
    "canvas = ROOT.TCanvas(\"canvas\", \"Ratio IQR/Median vs Jet pT\", 800, 600)\n",
    "\n",
    "# Draw the first graph (Reco_jet_pt/gen_jet_pt)\n",
    "gr_reco_ratio.SetMarkerStyle(20)\n",
    "gr_reco_ratio.SetMarkerColor(ROOT.kBlue)\n",
    "gr_reco_ratio.SetLineColor(ROOT.kBlue)\n",
    "gr_reco_ratio.GetXaxis().SetTitle(\"Jet P_{T,gen} (GeV)\")\n",
    "gr_reco_ratio.GetYaxis().SetTitle(\"Response IQR / Median\")\n",
    "gr_reco_ratio.Draw(\"APL\")\n",
    "\n",
    "# Draw the second graph (INT8_jet_pt/gen_jet_pt) on the same canvas\n",
    "gr_int8_ratio.SetMarkerStyle(20)\n",
    "gr_int8_ratio.SetMarkerColor(ROOT.kRed)\n",
    "gr_int8_ratio.SetLineColor(ROOT.kRed)\n",
    "gr_int8_ratio.Draw(\"PL same\")\n",
    "\n",
    "# Get the X and Y axes\n",
    "xaxis = gr_reco_ratio.GetXaxis()\n",
    "yaxis = gr_reco_ratio.GetYaxis()\n",
    "\n",
    "# Set tick length for all four axes\n",
    "xaxis.SetTickLength(0.03)\n",
    "yaxis.SetTickLength(0.03)\n",
    "\n",
    "# Set y-axis range to show both plots\n",
    "min_y = min(min(reco_ratio_iqr_median_filtered), min(int8_ratio_iqr_median_filtered))\n",
    "max_y = max(max(reco_ratio_iqr_median_filtered), max(int8_ratio_iqr_median_filtered))\n",
    "yaxis.SetRangeUser(min_y * 0.9, max_y * 1.1)\n",
    "\n",
    "# Add legend\n",
    "legend = ROOT.TLegend(0.75, 0.75, 0.9, 0.9)\n",
    "legend.AddEntry(gr_reco_ratio, \"Pred_FP32\", \"lp\")\n",
    "legend.AddEntry(gr_int8_ratio, \"Pred_INT8\", \"lp\")\n",
    "legend.Draw()\n",
    "\n",
    "# Show the canvas\n",
    "canvas.Draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6726f15b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf9e55e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e07f45f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
