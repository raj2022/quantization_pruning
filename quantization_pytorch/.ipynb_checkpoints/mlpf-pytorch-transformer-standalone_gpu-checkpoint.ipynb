{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b53ad0ae",
   "metadata": {},
   "source": [
    "## TO DO:\n",
    "\n",
    "* Working on the CPU with a reseaonable numbr of statistics\n",
    "try to plot the figure 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c0a1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import time\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39607f7d-cab2-469b-874e-c835e0c78865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import tensorflow_datasets as tfds\n",
    "import torch_geometric\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f57959f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 1\n",
      "Current GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {gpu_count}\")\n",
    "\n",
    "    # Get the name of the current GPU\n",
    "    current_gpu = torch.cuda.get_device_name(0)\n",
    "    print(f\"Current GPU: {current_gpu}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch is running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb7fa6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2460cf92-75be-4622-bfb2-8392978e2ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../mlpf/tensorflow_datasets/\"\n",
    "dataset = \"clic_edm_ttbar_pf\"\n",
    "\n",
    "#Load dataset\n",
    "builder = tfds.builder(dataset, data_dir=data_dir)\n",
    "ds_train = builder.as_data_source(split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86bd3286-c7a9-4bcb-a945-162cad36eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_FEATURES_TRK = [\n",
    "    \"elemtype\",\n",
    "    \"pt\",\n",
    "    \"eta\",\n",
    "    \"sin_phi\",\n",
    "    \"cos_phi\",\n",
    "    \"p\",\n",
    "    \"chi2\",\n",
    "    \"ndf\",\n",
    "    \"dEdx\",\n",
    "    \"dEdxError\",\n",
    "    \"radiusOfInnermostHit\",\n",
    "    \"tanLambda\",\n",
    "    \"D0\",\n",
    "    \"omega\",\n",
    "    \"Z0\",\n",
    "    \"time\",\n",
    "]\n",
    "X_FEATURES_CL = [\n",
    "    \"elemtype\",\n",
    "    \"et\",\n",
    "    \"eta\",\n",
    "    \"sin_phi\",\n",
    "    \"cos_phi\",\n",
    "    \"energy\",\n",
    "    \"position.x\",\n",
    "    \"position.y\",\n",
    "    \"position.z\",\n",
    "    \"iTheta\",\n",
    "    \"energy_ecal\",\n",
    "    \"energy_hcal\",\n",
    "    \"energy_other\",\n",
    "    \"num_hits\",\n",
    "    \"sigma_x\",\n",
    "    \"sigma_y\",\n",
    "    \"sigma_z\",\n",
    "]\n",
    "Y_FEATURES = [\"cls_id\", \"charge\", \"pt\", \"eta\", \"sin_phi\", \"cos_phi\", \"energy\"]\n",
    "Y_CLASSES = [0, 211, 130, 22, 11, 13]\n",
    "\n",
    "INPUT_DIM = max(len(X_FEATURES_TRK), len(X_FEATURES_CL))\n",
    "NUM_CLASSES = len(Y_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409952b1-db7a-47f6-b57c-8d0ce37e0872",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d581d65-1794-4ac5-b65f-96461a4b9c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as nnF\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class QuantizeableMultiheadAttention(nn.MultiheadAttention):\n",
    "    _FLOAT_MODULE = nn.MultiheadAttention\n",
    "\n",
    "    r\"\"\"Quantizable implementation of the MultiheadAttention.\n",
    "\n",
    "    Note::\n",
    "        Please, refer to :class:`~torch.nn.MultiheadAttention` for more\n",
    "        information\n",
    "\n",
    "    Allows the model to jointly attend to information from different\n",
    "    representation subspaces.\n",
    "    See reference: Attention Is All You Need\n",
    "\n",
    "    The original MHA module is not quantizable.\n",
    "    This reimplements it by explicitly instantiating the linear layers.\n",
    "\n",
    "    .. math::\n",
    "        \\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O\n",
    "        \\text{where} head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "\n",
    "    Args:\n",
    "        embed_dim: total dimension of the model.\n",
    "        num_heads: parallel attention heads.\n",
    "        dropout: a Dropout layer on attn_output_weights. Default: 0.0.\n",
    "        bias: add bias as module parameter. Default: True.\n",
    "        add_bias_kv: add bias to the key and value sequences at dim=0.\n",
    "        add_zero_attn: add a new batch of zeros to the key and\n",
    "                       value sequences at dim=1.\n",
    "        kdim: total number of features in key. Default: None.\n",
    "        vdim: total number of features in value. Default: None.\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
    "\n",
    "    Note that if :attr:`kdim` and :attr:`vdim` are None, they will be set\n",
    "    to :attr:`embed_dim` such that query, key, and value have the same\n",
    "    number of features.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> import torch.ao.nn.quantizable as nnqa\n",
    "        >>> multihead_attn = nnqa.MultiheadAttention(embed_dim, num_heads)\n",
    "        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)\n",
    "\n",
    "    Note::\n",
    "        Please, follow the quantization flow to convert the quantizable MHA.\n",
    "    \"\"\"\n",
    "    __constants__ = ['batch_first']\n",
    "\n",
    "    def __init__(self, embed_dim: int, num_heads: int,\n",
    "                 dropout: float = 0., bias: bool = True,\n",
    "                 add_bias_kv: bool = False, add_zero_attn: bool = False,\n",
    "                 kdim: Optional[int] = None, vdim: Optional[int] = None, batch_first: bool = False,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__(embed_dim, num_heads, dropout,\n",
    "                         bias, add_bias_kv,\n",
    "                         add_zero_attn, kdim, vdim, batch_first,\n",
    "                         **factory_kwargs)\n",
    "        self.linear_Q = nn.Linear(self.embed_dim, self.embed_dim, bias=bias, **factory_kwargs)\n",
    "        self.linear_K = nn.Linear(self.kdim, self.embed_dim, bias=bias, **factory_kwargs)\n",
    "        self.linear_V = nn.Linear(self.vdim, self.embed_dim, bias=bias, **factory_kwargs)\n",
    "        # for the type: ignore, see https://github.com/pytorch/pytorch/issues/58969\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=bias, **factory_kwargs)  # type: ignore[assignment]\n",
    "\n",
    "        # Functionals\n",
    "        # self.q_scaling_product = torch.ao.nn.quantized.FloatFunctional()\n",
    "        # note: importing torch.ao.nn.quantized at top creates a circular import\n",
    "\n",
    "        # Quant/Dequant\n",
    "        self.quant_attn_output = torch.ao.quantization.QuantStub()\n",
    "        self.quant_attn_output_weights = torch.ao.quantization.QuantStub()\n",
    "        self.dequant_q = torch.ao.quantization.DeQuantStub()\n",
    "        self.dequant_k = torch.ao.quantization.DeQuantStub()\n",
    "        self.dequant_v = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def _get_name(self):\n",
    "        return 'QuantizableMultiheadAttention'\n",
    "\n",
    "    @classmethod\n",
    "    def from_float(cls, other):\n",
    "        assert type(other) == cls._FLOAT_MODULE\n",
    "        assert hasattr(other, 'qconfig'), \"The float module must have 'qconfig'\"\n",
    "        # Setting the dropout to 0.0!\n",
    "        observed = cls(other.embed_dim, other.num_heads, other.dropout,\n",
    "                       (other.in_proj_bias is not None),\n",
    "                       (other.bias_k is not None),\n",
    "                       other.add_zero_attn, other.kdim, other.vdim,\n",
    "                       other.batch_first)\n",
    "        observed.bias_k = other.bias_k\n",
    "        observed.bias_v = other.bias_v\n",
    "        observed.qconfig = other.qconfig\n",
    "\n",
    "        # Set the linear weights\n",
    "        # for the type: ignores, see https://github.com/pytorch/pytorch/issues/58969\n",
    "        observed.out_proj.weight = other.out_proj.weight  # type: ignore[has-type]\n",
    "        observed.out_proj.bias = other.out_proj.bias  # type: ignore[has-type]\n",
    "        if other._qkv_same_embed_dim:\n",
    "            # Use separate params\n",
    "            bias = other.in_proj_bias\n",
    "            _start = 0\n",
    "            _end = _start + other.embed_dim\n",
    "            weight = other.in_proj_weight[_start:_end, :]\n",
    "            if bias is not None:\n",
    "                bias = torch.nn.Parameter(bias[_start:_end], bias.requires_grad)\n",
    "            observed.linear_Q.weight = torch.nn.Parameter(weight,\n",
    "                                                          weight.requires_grad)\n",
    "            observed.linear_Q.bias = bias\n",
    "\n",
    "            bias = other.in_proj_bias\n",
    "            _start = _end\n",
    "            _end = _start + other.embed_dim\n",
    "            weight = other.in_proj_weight[_start:_end, :]\n",
    "            if bias is not None:\n",
    "                bias = torch.nn.Parameter(bias[_start:_end], bias.requires_grad)\n",
    "            observed.linear_K.weight = torch.nn.Parameter(weight,\n",
    "                                                          weight.requires_grad)\n",
    "            observed.linear_K.bias = bias\n",
    "\n",
    "            bias = other.in_proj_bias\n",
    "            _start = _end\n",
    "            weight = other.in_proj_weight[_start:, :]\n",
    "            if bias is not None:\n",
    "                bias = torch.nn.Parameter(bias[_start:], bias.requires_grad)\n",
    "            observed.linear_V.weight = torch.nn.Parameter(weight,\n",
    "                                                          weight.requires_grad)\n",
    "            observed.linear_V.bias = bias\n",
    "        else:\n",
    "            observed.linear_Q.weight = nn.Parameter(other.q_proj_weight)\n",
    "            observed.linear_K.weight = nn.Parameter(other.k_proj_weight)\n",
    "            observed.linear_V.weight = nn.Parameter(other.v_proj_weight)\n",
    "            if other.in_proj_bias is None:\n",
    "                observed.linear_Q.bias = None  # type: ignore[assignment]\n",
    "                observed.linear_K.bias = None  # type: ignore[assignment]\n",
    "                observed.linear_V.bias = None  # type: ignore[assignment]\n",
    "            else:\n",
    "                observed.linear_Q.bias = nn.Parameter(other.in_proj_bias[0:other.embed_dim])\n",
    "                observed.linear_K.bias = nn.Parameter(other.in_proj_bias[other.embed_dim:(other.embed_dim * 2)])\n",
    "                observed.linear_V.bias = nn.Parameter(other.in_proj_bias[(other.embed_dim * 2):])\n",
    "        observed.eval()\n",
    "        # Explicit prepare\n",
    "        observed = torch.ao.quantization.prepare(observed, inplace=True)\n",
    "        return observed\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def dequantize(self):\n",
    "        r\"\"\"Utility to convert the quantized MHA back to float.\n",
    "\n",
    "        The motivation for this is that it is not trivial to conver the weights\n",
    "        from the format that is used in the quantized version back to the\n",
    "        float.\n",
    "        \"\"\"\n",
    "        fp = self._FLOAT_MODULE(self.embed_dim, self.num_heads, self.dropout,\n",
    "                                (self.linear_Q._weight_bias()[1] is not None),\n",
    "                                (self.bias_k is not None),\n",
    "                                self.add_zero_attn, self.kdim, self.vdim, self.batch_first)\n",
    "        assert fp._qkv_same_embed_dim == self._qkv_same_embed_dim\n",
    "        if self.bias_k is not None:\n",
    "            fp.bias_k = nn.Parameter(self.bias_k.dequantize())\n",
    "        if self.bias_v is not None:\n",
    "            fp.bias_v = nn.Parameter(self.bias_v.dequantize())\n",
    "\n",
    "        # Set the linear weights\n",
    "        # Note: Because the linear layers are quantized, mypy does not nkow how\n",
    "        # to deal with them -- might need to ignore the typing checks.\n",
    "        # for the type: ignore[has-type], see https://github.com/pytorch/pytorch/issues/58969\n",
    "        w, b = self.out_proj._weight_bias()  # type: ignore[operator, has-type]\n",
    "        fp.out_proj.weight = nn.Parameter(w.dequantize())\n",
    "        if b is not None:\n",
    "            fp.out_proj.bias = nn.Parameter(b)\n",
    "\n",
    "        wQ, bQ = self.linear_Q._weight_bias()  # type: ignore[operator]\n",
    "        wQ = wQ.dequantize()\n",
    "        wK, bK = self.linear_K._weight_bias()  # type: ignore[operator]\n",
    "        wK = wK.dequantize()\n",
    "        wV, bV = self.linear_V._weight_bias()  # type: ignore[operator]\n",
    "        wV = wV.dequantize()\n",
    "        if fp._qkv_same_embed_dim:\n",
    "            # Use separate params\n",
    "            _start = 0\n",
    "            _end = _start + fp.embed_dim\n",
    "            fp.in_proj_weight[_start:_end, :] = wQ\n",
    "            if fp.in_proj_bias is not None:\n",
    "                assert all(bQ == 0)\n",
    "                fp.in_proj_bias[_start:_end] = bQ\n",
    "\n",
    "            _start = _end\n",
    "            _end = _start + fp.embed_dim\n",
    "            fp.in_proj_weight[_start:_end, :] = wK\n",
    "            if fp.in_proj_bias is not None:\n",
    "                assert all(bK == 0)\n",
    "                fp.in_proj_bias[_start:_end] = bK\n",
    "\n",
    "            _start = _end\n",
    "            fp.in_proj_weight[_start:, :] = wV\n",
    "            if fp.in_proj_bias is not None:\n",
    "                assert all(bV == 0)\n",
    "                fp.in_proj_bias[_start:] = bV\n",
    "        else:\n",
    "            fp.q_proj_weight = nn.Parameter(wQ)\n",
    "            fp.k_proj_weight = nn.Parameter(wK)\n",
    "            fp.v_proj_weight = nn.Parameter(wV)\n",
    "            if fp.in_proj_bias is None:\n",
    "                self.linear_Q.bias = None\n",
    "                self.linear_K.bias = None\n",
    "                self.linear_V.bias = None\n",
    "            else:\n",
    "                fp.in_proj_bias[0:fp.embed_dim] = bQ\n",
    "                fp.in_proj_bias[fp.embed_dim:(fp.embed_dim * 2)] = bK\n",
    "                fp.in_proj_bias[(fp.embed_dim * 2):] = bV\n",
    "\n",
    "        return fp\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_observed(cls, other):\n",
    "        # The whole flow is float -> observed -> quantized\n",
    "        # This class does float -> observed only\n",
    "        # See nn.quantized.MultiheadAttention\n",
    "        raise NotImplementedError(\"It looks like you are trying to prepare an \"\n",
    "                                  \"MHA module. Please, see \"\n",
    "                                  \"the examples on quantizable MHAs.\")\n",
    "\n",
    "    def forward(self,\n",
    "                query: Tensor,\n",
    "                key: Tensor,\n",
    "                value: Tensor,\n",
    "                key_padding_mask: Optional[Tensor] = None,\n",
    "                need_weights: bool = True,\n",
    "                attn_mask: Optional[Tensor] = None,\n",
    "                average_attn_weights: bool = True,\n",
    "                is_causal: bool = False) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        r\"\"\"\n",
    "    Note::\n",
    "        Please, refer to :func:`~torch.nn.MultiheadAttention.forward` for more\n",
    "        information\n",
    "\n",
    "    Args:\n",
    "        query, key, value: map a query and a set of key-value pairs to an output.\n",
    "            See \"Attention Is All You Need\" for more details.\n",
    "        key_padding_mask: if provided, specified padding elements in the key will\n",
    "            be ignored by the attention. When given a binary mask and a value is True,\n",
    "            the corresponding value on the attention layer will be ignored.\n",
    "        need_weights: output attn_output_weights.\n",
    "        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all\n",
    "            the batches while a 3D mask allows to specify a different mask for the entries of each batch.\n",
    "\n",
    "    Shape:\n",
    "        - Inputs:\n",
    "        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is\n",
    "          the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n",
    "        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n",
    "        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is\n",
    "          the embedding dimension. :math:`(N, S, E)` if ``batch_first`` is ``True``.\n",
    "        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.\n",
    "          If a BoolTensor is provided, the positions with the\n",
    "          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n",
    "        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.\n",
    "          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked\n",
    "          positions. If a BoolTensor is provided, positions with ``True``\n",
    "          is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n",
    "          is provided, it will be added to the attention weight.\n",
    "        - is_causal: If specified, applies a causal mask as attention mask. Mutually exclusive with providing attn_mask.\n",
    "          Default: ``False``.\n",
    "        - average_attn_weights: If true, indicates that the returned ``attn_weights`` should be averaged across\n",
    "          heads. Otherwise, ``attn_weights`` are provided separately per head. Note that this flag only has an\n",
    "          effect when ``need_weights=True.``. Default: True (i.e. average weights across heads)\n",
    "\n",
    "        - Outputs:\n",
    "        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,\n",
    "          E is the embedding dimension. :math:`(N, L, E)` if ``batch_first`` is ``True``.\n",
    "        - attn_output_weights: If ``average_attn_weights=True``, returns attention weights averaged\n",
    "          across heads of shape :math:`(N, L, S)`, where N is the batch size, L is the target sequence length,\n",
    "          S is the source sequence length. If ``average_attn_weights=False``, returns attention weights per\n",
    "          head of shape :math:`(N, num_heads, L, S)`.\n",
    "        \"\"\"\n",
    "        return self._forward_impl(query, key, value, key_padding_mask,\n",
    "                                  need_weights, attn_mask, average_attn_weights,\n",
    "                                  is_causal)\n",
    "\n",
    "    def _forward_impl(self,\n",
    "                      query: Tensor,\n",
    "                      key: Tensor,\n",
    "                      value: Tensor,\n",
    "                      key_padding_mask: Optional[Tensor] = None,\n",
    "                      need_weights: bool = True,\n",
    "                      attn_mask: Optional[Tensor] = None,\n",
    "                      average_attn_weights: bool = True,\n",
    "                      is_causal: bool = False) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        # This version will not deal with the static key/value pairs.\n",
    "        # Keeping it here for future changes.\n",
    "        #\n",
    "        # TODO: This method has some duplicate lines with the\n",
    "        # `torch.nn.functional.multi_head_attention`. Will need to refactor.\n",
    "        static_k = None\n",
    "        static_v = None\n",
    "\n",
    "        if attn_mask is not None and is_causal:\n",
    "            raise AssertionError(\"Only allow causal mask or attn_mask\")\n",
    "\n",
    "        if is_causal:\n",
    "            raise AssertionError(\"causal mask not supported by AO MHA module\")\n",
    "\n",
    "        if self.batch_first:\n",
    "            query, key, value = (x.transpose(0, 1) for x in (query, key, value))\n",
    "\n",
    "        tgt_len, bsz, embed_dim_to_check = query.size()\n",
    "        assert self.embed_dim == embed_dim_to_check\n",
    "        # allow MHA to have different sizes for the feature dimension\n",
    "        assert key.size(0) == value.size(0) and key.size(1) == value.size(1)\n",
    "\n",
    "        head_dim = self.embed_dim // self.num_heads\n",
    "        assert head_dim * self.num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        scaling = float(head_dim) ** -0.5\n",
    "\n",
    "        q = self.linear_Q(query)\n",
    "        k = self.linear_K(key)\n",
    "        v = self.linear_V(value)\n",
    "\n",
    "        #JP fix here: disabled this\n",
    "        # q = self.q_scaling_product.mul_scalar(q, scaling)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.uint8:\n",
    "                warnings.warn(\"Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "                attn_mask = attn_mask.to(torch.bool)\n",
    "            assert attn_mask.is_floating_point() or attn_mask.dtype == torch.bool, \\\n",
    "                f'Only float and bool types are supported for attn_mask, not {attn_mask.dtype}'\n",
    "\n",
    "            if attn_mask.dim() == 2:\n",
    "                attn_mask = attn_mask.unsqueeze(0)\n",
    "                if list(attn_mask.size()) != [1, query.size(0), key.size(0)]:\n",
    "                    raise RuntimeError('The size of the 2D attn_mask is not correct.')\n",
    "            elif attn_mask.dim() == 3:\n",
    "                if list(attn_mask.size()) != [bsz * self.num_heads, query.size(0), key.size(0)]:\n",
    "                    raise RuntimeError('The size of the 3D attn_mask is not correct.')\n",
    "            else:\n",
    "                raise RuntimeError(f\"attn_mask's dimension {attn_mask.dim()} is not supported\")\n",
    "            # attn_mask's dim is 3 now.\n",
    "\n",
    "        # convert ByteTensor key_padding_mask to bool\n",
    "        if key_padding_mask is not None and key_padding_mask.dtype == torch.uint8:\n",
    "            warnings.warn(\"Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.\")\n",
    "            key_padding_mask = key_padding_mask.to(torch.bool)\n",
    "        if self.bias_k is not None and self.bias_v is not None:\n",
    "            if static_k is None and static_v is None:\n",
    "\n",
    "                # Explicitly assert that bias_k and bias_v are not None\n",
    "                # in a way that TorchScript can understand.\n",
    "                bias_k = self.bias_k\n",
    "                assert bias_k is not None\n",
    "                bias_v = self.bias_v\n",
    "                assert bias_v is not None\n",
    "\n",
    "                k = torch.cat([k, bias_k.repeat(1, bsz, 1)])\n",
    "                v = torch.cat([v, bias_v.repeat(1, bsz, 1)])\n",
    "                if attn_mask is not None:\n",
    "                    attn_mask = nnF.pad(attn_mask, (0, 1))\n",
    "                if key_padding_mask is not None:\n",
    "                    key_padding_mask = nnF.pad(key_padding_mask, (0, 1))\n",
    "            else:\n",
    "                assert static_k is None, \"bias cannot be added to static key.\"\n",
    "                assert static_v is None, \"bias cannot be added to static value.\"\n",
    "        else:\n",
    "            assert self.bias_k is None\n",
    "            assert self.bias_v is None\n",
    "\n",
    "        q = q.contiguous().view(tgt_len, bsz * self.num_heads, head_dim).transpose(0, 1)\n",
    "        if k is not None:\n",
    "            k = k.contiguous().view(-1, bsz * self.num_heads, head_dim).transpose(0, 1)\n",
    "        if v is not None:\n",
    "            v = v.contiguous().view(-1, bsz * self.num_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "        if static_k is not None:\n",
    "            assert static_k.size(0) == bsz * self.num_heads\n",
    "            assert static_k.size(2) == head_dim\n",
    "            k = static_k\n",
    "\n",
    "        if static_v is not None:\n",
    "            assert static_v.size(0) == bsz * self.num_heads\n",
    "            assert static_v.size(2) == head_dim\n",
    "            v = static_v\n",
    "\n",
    "        src_len = k.size(1)\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            assert key_padding_mask.size(0) == bsz\n",
    "            assert key_padding_mask.size(1) == src_len\n",
    "\n",
    "        if self.add_zero_attn:\n",
    "            src_len += 1\n",
    "            k_zeros = torch.zeros((k.size(0), 1) + k.size()[2:])\n",
    "            if k.is_quantized:\n",
    "                k_zeros = torch.quantize_per_tensor(k_zeros, k.q_scale(), k.q_zero_point(), k.dtype)\n",
    "            k = torch.cat([k, k_zeros], dim=1)\n",
    "            v_zeros = torch.zeros((v.size(0), 1) + k.size()[2:])\n",
    "            if v.is_quantized:\n",
    "                v_zeros = torch.quantize_per_tensor(v_zeros, v.q_scale(), v.q_zero_point(), v.dtype)\n",
    "            v = torch.cat([v, v_zeros], dim=1)\n",
    "\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = nnF.pad(attn_mask, (0, 1))\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = nnF.pad(key_padding_mask, (0, 1))\n",
    "\n",
    "        # Leaving the quantized zone here\n",
    "        q = self.dequant_q(q)\n",
    "        k = self.dequant_k(k)\n",
    "        v = self.dequant_v(v)\n",
    "        attn_output_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "        assert list(attn_output_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dtype == torch.bool:\n",
    "                attn_output_weights.masked_fill_(attn_mask, float('-inf'))\n",
    "            else:\n",
    "                attn_output_weights += attn_mask\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            attn_output_weights = attn_output_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_output_weights = attn_output_weights.masked_fill(\n",
    "                key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "                float('-inf'),\n",
    "            )\n",
    "            attn_output_weights = attn_output_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        attn_output_weights = nnF.softmax(\n",
    "            attn_output_weights, dim=-1)\n",
    "        attn_output_weights = nnF.dropout(attn_output_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.bmm(attn_output_weights, v)\n",
    "        assert list(attn_output.size()) == [bsz * self.num_heads, tgt_len, head_dim]\n",
    "        if self.batch_first:\n",
    "            attn_output = attn_output.view(bsz, tgt_len, self.embed_dim)\n",
    "        else:\n",
    "            attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, self.embed_dim)\n",
    "\n",
    "        # Reentering the quantized zone\n",
    "        attn_output = self.quant_attn_output(attn_output)\n",
    "        # for the type: ignore[has-type], see https://github.com/pytorch/pytorch/issues/58969\n",
    "        attn_output = self.out_proj(attn_output)  # type: ignore[has-type]\n",
    "\n",
    "        #JP fix: removed need_weights part from here, return attn_output instead of tuple\n",
    "        return attn_output\n",
    "\n",
    "class QuantizedMultiheadAttention(QuantizeableMultiheadAttention):\n",
    "    _FLOAT_MODULE = torch.ao.nn.quantizable.MultiheadAttention\n",
    "\n",
    "    def _get_name(self):\n",
    "        return \"QuantizedMultiheadAttention\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_float(cls, other):\n",
    "        # The whole flow is float -> observed -> quantized\n",
    "        # This class does observed -> quantized only\n",
    "        raise NotImplementedError(\"It looks like you are trying to convert a \"\n",
    "                                  \"non-observed MHA module. Please, see \"\n",
    "                                  \"the examples on quantizable MHAs.\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_observed(cls, other):\n",
    "        converted = torch.ao.quantization.convert(other, mapping=None,\n",
    "                                                  inplace=False,\n",
    "                                                  remove_qconfig=True,\n",
    "                                                  convert_custom_config_dict=None)\n",
    "        converted.__class__ = cls\n",
    "        # Remove the parameters for the bias_k and bias_v to quantize them\n",
    "        # TODO: This is a potential source of accuracy drop.\n",
    "        #       quantized cat takes the scale and zp of the first\n",
    "        #       element, which might lose the precision in the bias_k\n",
    "        #       and the bias_v (which are cat'ed with k/v being first).\n",
    "        if converted.bias_k is not None:\n",
    "            bias_k = converted._parameters.pop('bias_k')\n",
    "            sc, zp = torch._choose_qparams_per_tensor(bias_k,\n",
    "                                                      reduce_range=False)\n",
    "            bias_k = torch.quantize_per_tensor(bias_k, sc, zp, torch.quint8)\n",
    "            setattr(converted, 'bias_k', bias_k)  # noqa: B010\n",
    "\n",
    "        if converted.bias_v is not None:\n",
    "            bias_v = converted._parameters.pop('bias_v')\n",
    "            sc, zp = torch._choose_qparams_per_tensor(bias_k,  # type: ignore[possibly-undefined]\n",
    "                                                      reduce_range=False)\n",
    "            bias_v = torch.quantize_per_tensor(bias_v, sc, zp, torch.quint8)\n",
    "            setattr(converted, 'bias_v', bias_v)  # noqa: B010\n",
    "\n",
    "        del converted.in_proj_weight\n",
    "        del converted.in_proj_bias\n",
    "\n",
    "        return converted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e697d50c-8b91-42b5-ade2-9e9e23041001",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss, as described in https://arxiv.org/abs/1708.02002.\n",
    "    It is essentially an enhancement to cross entropy loss and is\n",
    "    useful for classification tasks when there is a large class imbalance.\n",
    "    x is expected to contain raw, unnormalized scores for each class.\n",
    "    y is expected to contain class labels.\n",
    "    Shape:\n",
    "        - x: (batch_size, C) or (batch_size, C, d1, d2, ..., dK), K > 0.\n",
    "        - y: (batch_size,) or (batch_size, d1, d2, ..., dK), K > 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, alpha = None, gamma = 0.0, reduction = \"mean\", ignore_index = -100\n",
    "    ):\n",
    "        \"\"\"Constructor.\n",
    "        Args:\n",
    "            alpha (Tensor, optional): Weights for each class. Defaults to None.\n",
    "            gamma (float, optional): A constant, as described in the paper.\n",
    "                Defaults to 0.\n",
    "            reduction (str, optional): 'mean', 'sum' or 'none'.\n",
    "                Defaults to 'mean'.\n",
    "            ignore_index (int, optional): class label to ignore.\n",
    "                Defaults to -100.\n",
    "        \"\"\"\n",
    "        if reduction not in (\"mean\", \"sum\", \"none\"):\n",
    "            raise ValueError('Reduction must be one of: \"mean\", \"sum\", \"none\".')\n",
    "\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "        self.nll_loss = nn.NLLLoss(weight=alpha, reduction=\"none\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        arg_keys = [\"alpha\", \"gamma\", \"reduction\"]\n",
    "        arg_vals = [self.__dict__[k] for k in arg_keys]\n",
    "        arg_strs = [f\"{k}={v!r}\" for k, v in zip(arg_keys, arg_vals)]\n",
    "        arg_str = \", \".join(arg_strs)\n",
    "        return f\"{type(self).__name__}({arg_str})\"\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        if x.ndim > 2:\n",
    "            # (N, C, d1, d2, ..., dK) --> (N * d1 * ... * dK, C)\n",
    "            c = x.shape[1]\n",
    "            x = x.permute(0, *range(2, x.ndim), 1).reshape(-1, c)\n",
    "            # (N, d1, d2, ..., dK) --> (N * d1 * ... * dK,)\n",
    "            y = y.view(-1)\n",
    "\n",
    "        # compute weighted cross entropy term: -alpha * log(pt)\n",
    "        # (alpha is already part of self.nll_loss)\n",
    "        log_p = F.log_softmax(x, dim=-1)\n",
    "        ce = self.nll_loss(log_p, y)\n",
    "\n",
    "        # get true class column from each row\n",
    "        # this is slow due to indexing\n",
    "        # all_rows = torch.arange(len(x))\n",
    "        # log_pt = log_p[all_rows, y]\n",
    "        log_pt = torch.gather(log_p, 1, y.unsqueeze(axis=-1)).squeeze(axis=-1)\n",
    "\n",
    "        # compute focal term: (1 - pt)^gamma\n",
    "        pt = log_pt.exp()\n",
    "        focal_term = (1 - pt) ** self.gamma\n",
    "\n",
    "        # the full loss: -alpha * ((1 - pt)^gamma) * log(pt)\n",
    "        loss = focal_term * ce\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            loss = loss.sum()\n",
    "\n",
    "        return loss\n",
    "\n",
    "class QuantizeFeaturesStub(torch.ao.quantization.QuantStub):\n",
    "    def __init__(self, num_feats):\n",
    "        super().__init__()\n",
    "        self.num_feats = num_feats\n",
    "        self.quants = torch.nn.ModuleList()\n",
    "        for ifeat in range(self.num_feats):\n",
    "            self.quants.append(torch.ao.quantization.QuantStub())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.quants[ifeat](x[..., ifeat:ifeat+1]) for ifeat in range(self.num_feats)], axis=-1)\n",
    "        \n",
    "def mlpf_loss(y, ypred, mask):\n",
    "    loss = {}\n",
    "    loss_obj_id = FocalLoss(gamma=2.0, reduction=\"none\")\n",
    "\n",
    "    msk_true_particle = torch.unsqueeze((y[\"cls_id\"] != 0).to(dtype=torch.float32), axis=-1)\n",
    "    nelem = torch.sum(mask)\n",
    "    npart = torch.sum(y[\"cls_id\"] != 0)\n",
    "    \n",
    "    ypred[\"momentum\"] = ypred[\"momentum\"] * msk_true_particle\n",
    "    y[\"momentum\"] = y[\"momentum\"] * msk_true_particle\n",
    "\n",
    "    ypred[\"cls_id_onehot\"] = ypred[\"cls_id_onehot\"].permute((0, 2, 1))\n",
    "\n",
    "    loss_classification = loss_obj_id(ypred[\"cls_id_onehot\"], y[\"cls_id\"]).reshape(y[\"cls_id\"].shape)\n",
    "    loss_regression = torch.nn.functional.huber_loss(ypred[\"momentum\"], y[\"momentum\"], reduction=\"none\")\n",
    "    \n",
    "    # average over all elements that were not padded\n",
    "    loss[\"Classification\"] = loss_classification.sum() / npart\n",
    "    \n",
    "    mom_normalizer = y[\"momentum\"][y[\"cls_id\"] != 0].std(axis=0)\n",
    "    reg_losses = loss_regression[y[\"cls_id\"] != 0]\n",
    "    # average over all true particles\n",
    "    loss[\"Regression\"] = (reg_losses / mom_normalizer).sum() / npart\n",
    "\n",
    "    px = ypred[\"momentum\"][..., 0:1] * ypred[\"momentum\"][..., 3:4] * msk_true_particle\n",
    "    py = ypred[\"momentum\"][..., 0:1] * ypred[\"momentum\"][..., 2:3] * msk_true_particle\n",
    "    pred_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)\n",
    "\n",
    "    px = y[\"momentum\"][..., 0:1] * y[\"momentum\"][..., 3:4] * msk_true_particle\n",
    "    py = y[\"momentum\"][..., 0:1] * y[\"momentum\"][..., 2:3] * msk_true_particle\n",
    "    true_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)\n",
    "    loss[\"MET\"] = torch.nn.functional.huber_loss(pred_met, true_met).mean()\n",
    "\n",
    "    loss[\"Total\"] = loss[\"Classification\"] + loss[\"Regression\"]\n",
    "    # loss[\"Total\"] += 0.1*loss[\"MET\"]\n",
    "    return loss\n",
    "    \n",
    "class SelfAttentionLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim=128,\n",
    "        num_heads=2,\n",
    "        width=128,\n",
    "        dropout_mha=0.1,\n",
    "        dropout_ff=0.1,\n",
    "        attention_type=\"efficient\",\n",
    "    ):\n",
    "        super(SelfAttentionLayer, self).__init__()\n",
    "\n",
    "        self.attention_type = attention_type\n",
    "        self.act = nn.ReLU\n",
    "        self.mha = torch.nn.MultiheadAttention(embedding_dim, num_heads, dropout=dropout_mha, batch_first=True)\n",
    "        self.norm0 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.norm1 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.seq = torch.nn.Sequential(\n",
    "            nn.Linear(embedding_dim, width), self.act(), nn.Linear(width, embedding_dim), self.act()\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(dropout_ff)\n",
    "\n",
    "        self.add0 = torch.ao.nn.quantized.FloatFunctional()\n",
    "        self.add1 = torch.ao.nn.quantized.FloatFunctional()\n",
    "        self.mul = torch.ao.nn.quantized.FloatFunctional()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        mha_out = self.mha(x, x, x, need_weights=False)[0]\n",
    "        x = self.add0.add(x, mha_out)\n",
    "        x = self.norm0(x)\n",
    "        x = self.add1.add(x, self.seq(x))\n",
    "        x = self.norm1(x)\n",
    "        x = self.dropout(x)\n",
    "        # x = self.mul.mul(x, mask.unsqueeze(-1))\n",
    "        return x\n",
    "\n",
    "class RegressionOutput(nn.Module):\n",
    "    def __init__(self, embed_dim, width, act, dropout):\n",
    "        super(RegressionOutput, self).__init__()\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "        self.nn = ffn(embed_dim, 1, width, act, dropout)\n",
    "\n",
    "    def forward(self, elems, x, orig_value):\n",
    "        nn_out = self.nn(x)\n",
    "        nn_out = self.dequant(nn_out)\n",
    "        return orig_value + nn_out\n",
    "\n",
    "def ffn(input_dim, output_dim, width, act, dropout):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, width),\n",
    "        act(),\n",
    "        torch.nn.LayerNorm(width),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(width, output_dim),\n",
    "    )\n",
    "\n",
    "def transform_batch(Xbatch):\n",
    "    Xbatch = Xbatch.clone()\n",
    "    Xbatch[..., 1] = torch.log(Xbatch[..., 1])\n",
    "    Xbatch[..., 5] = torch.log(Xbatch[..., 5])\n",
    "    Xbatch[torch.isnan(Xbatch)] = 0.0\n",
    "    Xbatch[torch.isinf(Xbatch)] = 0.0\n",
    "    return Xbatch\n",
    "    \n",
    "def unpack_target(y):\n",
    "    ret = {}\n",
    "    ret[\"cls_id\"] = y[..., 0].long()\n",
    "\n",
    "    for i, feat in enumerate(Y_FEATURES):\n",
    "        if i >= 2:  # skip the cls and charge as they are defined above\n",
    "            ret[feat] = y[..., i].to(dtype=torch.float32)\n",
    "    ret[\"phi\"] = torch.atan2(ret[\"sin_phi\"], ret[\"cos_phi\"])\n",
    "    \n",
    "    # note ~ momentum = [\"pt\", \"eta\", \"sin_phi\", \"cos_phi\", \"energy\"]\n",
    "    ret[\"momentum\"] = y[..., 2:7].to(dtype=torch.float32)\n",
    "    ret[\"p4\"] = torch.cat(\n",
    "        [ret[\"pt\"].unsqueeze(1), ret[\"eta\"].unsqueeze(1), ret[\"phi\"].unsqueeze(1), ret[\"energy\"].unsqueeze(1)], axis=1\n",
    "    )\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def unpack_predictions(preds):\n",
    "    ret = {}\n",
    "    ret[\"cls_id_onehot\"], ret[\"momentum\"] = preds\n",
    "\n",
    "    ret[\"pt\"] = ret[\"momentum\"][..., 0]\n",
    "    ret[\"eta\"] = ret[\"momentum\"][..., 1]\n",
    "    ret[\"sin_phi\"] = ret[\"momentum\"][..., 2]\n",
    "    ret[\"cos_phi\"] = ret[\"momentum\"][..., 3]\n",
    "    ret[\"energy\"] = ret[\"momentum\"][..., 4]\n",
    "\n",
    "    ret[\"cls_id\"] = torch.argmax(ret[\"cls_id_onehot\"], axis=-1)\n",
    "    ret[\"phi\"] = torch.atan2(ret[\"sin_phi\"], ret[\"cos_phi\"])\n",
    "    ret[\"p4\"] = torch.cat(\n",
    "        [\n",
    "            ret[\"pt\"].unsqueeze(axis=-1),\n",
    "            ret[\"eta\"].unsqueeze(axis=-1),\n",
    "            ret[\"phi\"].unsqueeze(axis=-1),\n",
    "            ret[\"energy\"].unsqueeze(axis=-1),\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "    return ret\n",
    "\n",
    "class MLPF(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=16,\n",
    "        num_classes=6,\n",
    "        num_convs=2,\n",
    "        dropout_ff=0.0,\n",
    "        dropout_conv_reg_mha=0.0,\n",
    "        dropout_conv_reg_ff=0.0,\n",
    "        dropout_conv_id_mha=0.0,\n",
    "        dropout_conv_id_ff=0.0,\n",
    "        num_heads=16,\n",
    "        head_dim=16,\n",
    "        elemtypes=[0,1,2],\n",
    "    ):\n",
    "        super(MLPF, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.act = nn.ReLU\n",
    "        self.elemtypes = elemtypes\n",
    "        self.num_elemtypes = len(self.elemtypes)\n",
    "\n",
    "        embedding_dim = num_heads * head_dim\n",
    "        width = num_heads * head_dim\n",
    "        \n",
    "        self.nn0_id = ffn(self.input_dim, embedding_dim, width, self.act, dropout_ff)\n",
    "        self.nn0_reg = ffn(self.input_dim, embedding_dim, width, self.act, dropout_ff)\n",
    "        \n",
    "        self.conv_id = nn.ModuleList()\n",
    "        self.conv_reg = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_convs):\n",
    "            self.conv_id.append(\n",
    "                SelfAttentionLayer(\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    width=width,\n",
    "                    dropout_mha=dropout_conv_id_mha,\n",
    "                    dropout_ff=dropout_conv_id_ff,\n",
    "                )\n",
    "            )\n",
    "            self.conv_reg.append(\n",
    "                SelfAttentionLayer(\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    width=width,\n",
    "                    dropout_mha=dropout_conv_reg_mha,\n",
    "                    dropout_ff=dropout_conv_reg_ff,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        decoding_dim = self.input_dim + embedding_dim\n",
    "\n",
    "        # DNN that acts on the node level to predict the PID\n",
    "        self.nn_id = ffn(decoding_dim, num_classes, width, self.act, dropout_ff)\n",
    "\n",
    "        # elementwise DNN for node momentum regression\n",
    "        embed_dim = decoding_dim + num_classes\n",
    "        self.nn_pt = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_eta = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_sin_phi = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_cos_phi = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        self.nn_energy = RegressionOutput(embed_dim, width, self.act, dropout_ff)\n",
    "        \n",
    "        self.quant = QuantizeFeaturesStub(self.input_dim + len(self.elemtypes))\n",
    "        self.dequant_id = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, X_features, mask):\n",
    "        Xfeat_transformed = transform_batch(X_features)\n",
    "        Xfeat_normed = self.quant(Xfeat_transformed)\n",
    "\n",
    "        embeddings_id, embeddings_reg = [], []\n",
    "        embedding_id = self.nn0_id(Xfeat_normed)\n",
    "        embedding_reg = self.nn0_reg(Xfeat_normed)\n",
    "        for num, conv in enumerate(self.conv_id):\n",
    "            conv_input = embedding_id if num == 0 else embeddings_id[-1]\n",
    "            out_padded = conv(conv_input, mask)\n",
    "            embeddings_id.append(out_padded)\n",
    "        for num, conv in enumerate(self.conv_reg):\n",
    "            conv_input = embedding_reg if num == 0 else embeddings_reg[-1]\n",
    "            out_padded = conv(conv_input, mask)\n",
    "            embeddings_reg.append(out_padded)\n",
    "\n",
    "        final_embedding_id = torch.cat([Xfeat_normed] + [embeddings_id[-1]], axis=-1)\n",
    "        preds_id = self.nn_id(final_embedding_id)\n",
    "\n",
    "        final_embedding_reg = torch.cat([Xfeat_normed] + [embeddings_reg[-1]] + [preds_id], axis=-1)\n",
    "        preds_pt = self.nn_pt(X_features, final_embedding_reg, X_features[..., 1:2])\n",
    "        preds_eta = self.nn_eta(X_features, final_embedding_reg, X_features[..., 2:3])\n",
    "        preds_sin_phi = self.nn_sin_phi(X_features, final_embedding_reg, X_features[..., 3:4])\n",
    "        preds_cos_phi = self.nn_cos_phi(X_features, final_embedding_reg, X_features[..., 4:5])\n",
    "        preds_energy = self.nn_energy(X_features, final_embedding_reg, X_features[..., 5:6])\n",
    "        preds_momentum = torch.cat([preds_pt, preds_eta, preds_sin_phi, preds_cos_phi, preds_energy], axis=-1)\n",
    "        \n",
    "        preds_id = self.dequant_id(preds_id)\n",
    "        return preds_id, preds_momentum\n",
    "\n",
    "model = MLPF(input_dim=INPUT_DIM, num_classes=NUM_CLASSES)\n",
    "optimizer = torch.optim.Adam(lr=1e-4, params=model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4721f1c7-b501-4757-982b-fa2194cf6176",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28fc87e0-5b18-4c44-a67e-ce2792b567de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 20/20 [00:08<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss=2.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss=1.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, loss=1.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, loss=1.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, loss=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, loss=0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, loss=0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, loss=0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss=0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 20/20 [00:03<00:00,  6.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, loss=0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_events_train = 1000\n",
    "max_events_eval = 1000\n",
    "events_per_batch = 50\n",
    "nepochs = 10\n",
    "\n",
    "model = MLPF(input_dim=INPUT_DIM, num_classes=NUM_CLASSES).to(device=device)\n",
    "optimizer = torch.optim.Adam(lr=1e-4, params=model.parameters())\n",
    "\n",
    "#Training loop\n",
    "loss_vals_epochs = []\n",
    "for epoch in range(nepochs):\n",
    "    loss_vals_steps = []\n",
    "    inds_train = range(0,max_events_train,events_per_batch)\n",
    "    for ind in tqdm.tqdm(inds_train):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #load the data for one batch\n",
    "        ds_elems = [ds_train[i] for i in range(ind,ind+events_per_batch)]\n",
    "        X_features = [torch.tensor(elem[\"X\"]).to(torch.float32) for elem in ds_elems]\n",
    "        y_targets = [torch.tensor(elem[\"ygen\"]).to(torch.float32) for elem in ds_elems]\n",
    "\n",
    "        #batch the data into [batch_size, num_elems, num_features]\n",
    "        X_features_padded = pad_sequence(X_features, batch_first=True).to(device=device)\n",
    "        y_targets_padded = pad_sequence(y_targets, batch_first=True).to(device=device)\n",
    "        mask = X_features_padded[:, :, 0]!=0\n",
    "\n",
    "        #run the model\n",
    "        preds = model(X_features_padded, mask)\n",
    "        preds_unpacked = unpack_predictions(preds)\n",
    "        targets_unpacked = unpack_target(y_targets_padded)\n",
    "\n",
    "        #compute loss, update model weights\n",
    "        loss = mlpf_loss(targets_unpacked, preds_unpacked, mask)\n",
    "        loss[\"Total\"].backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_vals_steps.append(loss[\"Total\"].detach().cpu().item())\n",
    "\n",
    "    loss_vals_epochs.append(np.mean(loss_vals_steps))\n",
    "    print(\"Epoch {}, loss={:.2f}\".format(epoch, loss_vals_epochs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd8787e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(losses)\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training Loss Curve')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b437405-87f1-409e-9ab9-ff79af3e1cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff2c4eb4e80>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_vals_epochs, label=\"training loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d11c05-a019-46b1-aff6-07d9e6565c5c",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4149d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the model back on CPU\n",
    "model = model.to(device=\"cpu\")\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "ds_elems = [ds_train[i] for i in range(max_events_train, max_events_train + max_events_eval)]\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32  \n",
    "for i in range(0, len(ds_elems), batch_size):\n",
    "    batch_elems = ds_elems[i:i + batch_size]\n",
    "\n",
    "    # input features\n",
    "    X_features = [torch.tensor(elem[\"X\"]).to(torch.float32) for elem in batch_elems]\n",
    "    X_features_padded = pad_sequence(X_features, batch_first=True)\n",
    "\n",
    "    #  target labels\n",
    "    y_targets = [torch.tensor(elem[\"ygen\"]).to(torch.float32) for elem in batch_elems]\n",
    "    y_targets_padded = pad_sequence(y_targets, batch_first=True)\n",
    "\n",
    "    #  mask for the batch\n",
    "    mask = X_features_padded[:, :, 0] != 0\n",
    "\n",
    "    #  model prediction, loss computation\n",
    "    preds = model(X_features_padded, mask)\n",
    "    preds = preds[0].detach(), preds[1].detach()\n",
    "\n",
    "    # Update mask for the batch\n",
    "    mask = X_features_padded[:, :, 0] != 0\n",
    "\n",
    "    # Unpack predictions and targets for the batch\n",
    "    preds_unpacked = unpack_predictions(preds)\n",
    "    targets_unpacked = unpack_target(y_targets_padded)\n",
    "    \n",
    "    # append to a list \n",
    "    all_preds.append(preds_unpacked)\n",
    "    all_targets.append(targets_unpacked)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Compute loss for the batch\n",
    "    loss = mlpf_loss(targets_unpacked, preds_unpacked, mask)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ab70010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Classification': tensor(0.5494),\n",
       " 'Regression': tensor(0.2855),\n",
       " 'MET': tensor(11.2636),\n",
       " 'Total': tensor(0.8349)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb31372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = [data['pt'] for data in all_preds]\n",
    "# pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "508ad615-a40e-49b5-a159-34fb564053dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_elems = [ds_train[i] for i in range(max_events_train,max_events_train+max_events_eval)]\n",
    "# X_features = [torch.tensor(elem[\"X\"]).to(torch.float32) for elem in ds_elems]\n",
    "# X_features_padded = pad_sequence(X_features, batch_first=True)\n",
    "# y_targets = [torch.tensor(elem[\"ygen\"]).to(torch.float32) for elem in ds_elems]\n",
    "# y_targets_padded = pad_sequence(y_targets, batch_first=True)\n",
    "# mask = X_features_padded[:, :, 0]!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f61b28d5-29d0-4b5d-9468-9a385b814faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = model(X_features_padded, mask)\n",
    "# preds = preds[0].detach(), preds[1].detach()\n",
    "# mask = X_features_padded[:, :, 0:1] != 0\n",
    "# preds_unpacked = unpack_predictions(preds)\n",
    "# targets_unpacked = unpack_target(y_targets_padded)\n",
    "\n",
    "# loss = mlpf_loss(targets_unpacked, preds_unpacked, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e93a0dd-77cb-4325-9fc1-b9da22b2b508",
   "metadata": {},
   "outputs": [],
   "source": [
    "msk_true_particles = targets_unpacked[\"cls_id\"]!=0\n",
    "\n",
    "pt_target = targets_unpacked[\"pt\"][msk_true_particles].numpy()\n",
    "pt_pred = preds_unpacked[\"pt\"][msk_true_particles].numpy()\n",
    "\n",
    "eta_target = targets_unpacked[\"eta\"][msk_true_particles].numpy()\n",
    "eta_pred = preds_unpacked[\"eta\"][msk_true_particles].numpy()\n",
    "\n",
    "sphi_target = targets_unpacked[\"sin_phi\"][msk_true_particles].numpy()\n",
    "sphi_pred = preds_unpacked[\"sin_phi\"][msk_true_particles].numpy()\n",
    "\n",
    "cphi_target = targets_unpacked[\"cos_phi\"][msk_true_particles].numpy()\n",
    "cphi_pred = preds_unpacked[\"cos_phi\"][msk_true_particles].numpy()\n",
    "\n",
    "energy_target = targets_unpacked[\"energy\"][msk_true_particles].numpy()\n",
    "energy_pred = preds_unpacked[\"energy\"][msk_true_particles].numpy()\n",
    "\n",
    "px = preds_unpacked[\"pt\"] * preds_unpacked[\"cos_phi\"] * msk_true_particles\n",
    "py = preds_unpacked[\"pt\"] * preds_unpacked[\"sin_phi\"] * msk_true_particles\n",
    "pred_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)\n",
    "\n",
    "px = targets_unpacked[\"pt\"] * targets_unpacked[\"cos_phi\"] * msk_true_particles\n",
    "py = targets_unpacked[\"pt\"] * targets_unpacked[\"sin_phi\"] * msk_true_particles\n",
    "true_met = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "864ee0fb-257d-48fc-8c52-d07cb6895395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'pred MET')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(true_met, pred_met, marker=\".\")\n",
    "plt.xlabel(\"true MET\")\n",
    "plt.ylabel(\"pred MET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00ca5eaa-86b6-410a-bd72-3a69134d9e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(pred_met/true_met, bins=np.linspace(0,20,100));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b100969-4eb0-4eff-ae28-4d4f17a559f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'pred pt')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = np.logspace(-1,2,100)\n",
    "plt.hist2d(\n",
    "    pt_target,\n",
    "    pt_pred,\n",
    "    bins=(b,b)\n",
    ")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"true pt\")\n",
    "plt.ylabel(\"pred pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d5ef494-527e-447b-9023-79b1a1245909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'pred eta')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = np.linspace(-4,4,100)\n",
    "plt.hist2d(\n",
    "    eta_target,\n",
    "    eta_pred,\n",
    "    bins=(b,b)\n",
    ")\n",
    "plt.xlabel(\"true eta\")\n",
    "plt.ylabel(\"pred eta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "460019b7-9971-4556-a644-342cc29a94a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'pred sphi')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = np.linspace(-1,1,100)\n",
    "plt.hist2d(\n",
    "    sphi_target,\n",
    "    sphi_pred,\n",
    "    bins=(b,b)\n",
    ")\n",
    "plt.xlabel(\"true sphi\")\n",
    "plt.ylabel(\"pred sphi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4be3453-73ad-49ea-b50f-d1e8286c067e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'pred cphi')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = np.linspace(-1,1,100)\n",
    "plt.hist2d(\n",
    "    cphi_target,\n",
    "    cphi_pred,\n",
    "    bins=(b,b)\n",
    ")\n",
    "plt.xlabel(\"true cphi\")\n",
    "plt.ylabel(\"pred cphi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04008e59-081c-41e7-93db-79ac0bd9da7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'pred energy')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = np.logspace(-1,2,100)\n",
    "plt.hist2d(\n",
    "    energy_target,\n",
    "    energy_pred,\n",
    "    bins=(b,b)\n",
    ")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"true energy\")\n",
    "plt.ylabel(\"pred energy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb59b32-9925-419f-bd15-09923b6d1073",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90148326-4b8c-4d2d-b192-b479b1516633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/sraj/ipykernel_2753893/3765155580.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0m_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0m_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmodel_prepared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel_int8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_prepared\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconvert_custom_config_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_module_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/sraj/ipykernel_2753893/3746077691.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X_features, mask)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mpreds_eta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_eta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_embedding_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mpreds_sin_phi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_sin_phi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_embedding_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0mpreds_cos_phi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_cos_phi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_embedding_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0mpreds_energy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_energy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_embedding_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mpreds_momentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpreds_pt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_eta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_sin_phi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_cos_phi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_energy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/sraj/ipykernel_2753893/3746077691.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, elems, x, orig_value)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mnn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mnn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdequant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0morig_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnn_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m                 for hook_id, hook in (\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_105_cuda/x86_64-el9-gcc11-opt/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "custom_module_config = {\n",
    "        \"float_to_observed_custom_module_class\": {torch.nn.MultiheadAttention: QuantizeableMultiheadAttention},\n",
    "        \"observed_to_quantized_custom_module_class\": {QuantizeableMultiheadAttention: QuantizedMultiheadAttention},\n",
    "}\n",
    "\n",
    "model_prepared = torch.ao.quantization.prepare(model, prepare_custom_config_dict=custom_module_config)\n",
    "\n",
    "#calibrate on data\n",
    "num_events_to_calibrate = 5000\n",
    "for ind in range(max_events_train,max_events_train+num_events_to_calibrate):\n",
    "    _X = torch.unsqueeze(torch.tensor(ds_train[ind][\"X\"]).to(torch.float32), 0)\n",
    "    _mask = _X[:, :, 0]!=0\n",
    "    model_prepared(_X, _mask)\n",
    "\n",
    "model_int8 = torch.ao.quantization.convert(model_prepared,convert_custom_config_dict=custom_module_config,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248b521-f166-4944-95cb-af96b5e66adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_int8.quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8f0258-3225-4c0f-8529-7a3bcbc83659",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_quantized = torch.quantize_per_tensor((X_features_padded[:, :, 0]!=0).to(torch.float32), 1, 0, torch.quint8)\n",
    "preds = model_int8(X_features_padded, mask_quantized)\n",
    "preds = preds[0].detach(), preds[1].detach()\n",
    "preds_unpacked_int8 = unpack_predictions(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1344cde-97f6-4a1c-8526-3586a139efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_int8 = mlpf_loss(targets_unpacked, preds_unpacked_int8, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b2a37d-8aa2-4ae0-be3b-c1d3fe56a2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(2), [loss[\"Total\"].detach().numpy(), loss_int8[\"Total\"].detach().numpy()])\n",
    "plt.xticks(range(2), [\"fp32\", \"int8\"])\n",
    "plt.ylabel(\"Final total loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57979d38-46a9-48bd-a9a2-748cb29d519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_pred_int8 = preds_unpacked_int8[\"pt\"][msk_true_particles].numpy()\n",
    "eta_pred_int8 = preds_unpacked_int8[\"eta\"][msk_true_particles].numpy()\n",
    "sphi_pred_int8 = preds_unpacked_int8[\"sin_phi\"][msk_true_particles].numpy()\n",
    "cphi_pred_int8 = preds_unpacked_int8[\"cos_phi\"][msk_true_particles].numpy()\n",
    "energy_pred_int8 = preds_unpacked_int8[\"energy\"][msk_true_particles].numpy()\n",
    "\n",
    "px = preds_unpacked_int8[\"pt\"] * preds_unpacked_int8[\"cos_phi\"] * msk_true_particles\n",
    "py = preds_unpacked_int8[\"pt\"] * preds_unpacked_int8[\"sin_phi\"] * msk_true_particles\n",
    "pred_met_int8 = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c02b0e-3e92-4aec-8741-6a4a58c1deca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pred_met/true_met, bins=np.linspace(0,5,61), histtype=\"step\", lw=2, label=\"fp32\");\n",
    "plt.hist(pred_met_int8/true_met, bins=np.linspace(0,5,61), histtype=\"step\", lw=2, label=\"int8\");\n",
    "plt.xlabel(\"reco_met / gen_met\")\n",
    "plt.ylabel(\"number of events / bin\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56321366",
   "metadata": {},
   "source": [
    "# Jets\n",
    "\n",
    "https://fastjet.readthedocs.io/en/latest/Awkward.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39f345b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jet 1: pt = 707.9314059897632, eta = -0.22688206060131969, phi = 5.113068680217441, mass = 1231.952231367644#--------------------------------------------------------------------------\n",
      "#                         FastJet release 3.4.1\n",
      "#                 M. Cacciari, G.P. Salam and G. Soyez                  \n",
      "#     A software package for jet finding and analysis at colliders      \n",
      "#                           http://fastjet.fr                           \n",
      "#\t                                                                      \n",
      "# Please cite EPJC72(2012)1896 [arXiv:1111.6097] if you use this package\n",
      "# for scientific work and optionally PLB641(2006)57 [hep-ph/0512210].   \n",
      "#                                                                       \n",
      "# FastJet is provided without warranty under the GNU GPL v2 or higher.  \n",
      "# It uses T. Chan's closest pair algorithm, S. Fortune's Voronoi code,\n",
      "# CGAL and 3rd party plugin jet algorithms. See COPYING file for details.\n",
      "#--------------------------------------------------------------------------\n",
      "\n",
      "Jet 2: pt = 662.287664562741, eta = 0.4998694601530169, phi = 3.6214088127106288, mass = 1084.4891176510894\n",
      "Jet 3: pt = 650.3448414209281, eta = 0.10290552518367481, phi = 2.277502731560636, mass = 1159.7186725080428\n",
      "Jet 4: pt = 436.91384128100793, eta = -0.314101078731973, phi = 0.6295511003407852, mass = 679.6301558707829\n",
      "Jet 5: pt = 20.431037123170483, eta = -1.5281795249223202, phi = 3.1714146761939266, mass = 50.91318464745024\n",
      "Jet 6: pt = 18.36701076200816, eta = 1.8982476314816084, phi = 1.3860006492110384, mass = 63.23286547925353\n",
      "Jet 7: pt = 10.891554670557678, eta = 1.330897110672144, phi = 6.081913919884816, mass = 23.044334555258\n",
      "Jet 8: pt = 8.397956325751851, eta = -1.7336791656422397, phi = 1.5667411118268408, mass = 25.132817896197114\n",
      "Jet 9: pt = 2.5134254834723624, eta = 1.902665506352057, phi = 4.588935013533528, mass = 8.618036917282703\n",
      "Jet 10: pt = 2.477832749018865, eta = -1.722527575753209, phi = 5.953033692199199, mass = 7.380609592342717\n"
     ]
    }
   ],
   "source": [
    "import fastjet as fj\n",
    "import numpy as np\n",
    "\n",
    "def create_particles(X_features):\n",
    "    particles = []\n",
    "    for x_feat in X_features:\n",
    "        pt, eta, phi, mass = x_feat[:, 1], x_feat[:, 2], torch.atan2(x_feat[:, 4], x_feat[:, 3]), x_feat[:, 5]\n",
    "        for pt_, eta_, phi_, mass_ in zip(pt, eta, phi, mass):\n",
    "            px = pt_ * torch.cos(phi_)\n",
    "            py = pt_ * torch.sin(phi_)\n",
    "            pz = pt_ * torch.sinh(eta_)\n",
    "            e = torch.sqrt(px**2 + py**2 + pz**2 + mass_**2)\n",
    "            particles.append(fj.PseudoJet(px.item(), py.item(), pz.item(), e.item()))\n",
    "    return particles\n",
    "\n",
    "# Cluster jets using FastJet\n",
    "def cluster_jets(particles, R=1.0, algorithm=fj.antikt_algorithm):\n",
    "    jet_def = fj.JetDefinition(algorithm, R)\n",
    "    cs = fj.ClusterSequence(particles, jet_def)\n",
    "    jets = fj.sorted_by_pt(cs.inclusive_jets())\n",
    "    return jets\n",
    "\n",
    "\n",
    "# Convert X_features into particles\n",
    "particles = create_particles(X_features)\n",
    "\n",
    "# Cluster jets\n",
    "jets = cluster_jets(particles)\n",
    "\n",
    "# Analyze jets\n",
    "for i, jet in enumerate(jets):\n",
    "    print(f\"Jet {i+1}: pt = {jet.pt()}, eta = {jet.eta()}, phi = {jet.phi()}, mass = {jet.m()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50a759ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jet1: pt= 707.9314059897632\n",
      "Jet2: pt= 662.287664562741\n",
      "Jet3: pt= 650.3448414209281\n",
      "Jet4: pt= 436.91384128100793\n",
      "Jet5: pt= 20.431037123170483\n",
      "Jet6: pt= 18.36701076200816\n",
      "Jet7: pt= 10.891554670557678\n",
      "Jet8: pt= 8.397956325751851\n",
      "Jet9: pt= 2.5134254834723624\n",
      "Jet10: pt= 2.477832749018865\n"
     ]
    }
   ],
   "source": [
    "for i, jet in enumerate(jets):\n",
    "    print(f\"Jet{i+1}: pt= {jet.pt()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9b51b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt_values [707.9314059897632, 662.287664562741, 650.3448414209281, 436.91384128100793, 20.431037123170483, 18.36701076200816, 10.891554670557678, 8.397956325751851, 2.5134254834723624, 2.477832749018865]\n"
     ]
    }
   ],
   "source": [
    "pt_values = [jet.pt() for jet in jets]\n",
    "print(f\"pt_values\",pt_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b27b1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4289b59d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds_eval' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/sraj/ipykernel_2753893/2961230070.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Use the trained model to predict jets on the evaluation dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mpredicted_jets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_jets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents_per_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ds_eval' is not defined"
     ]
    }
   ],
   "source": [
    "# Define a function to predict jets using the trained model\n",
    "def predict_jets(model, dataset, events_per_batch):\n",
    "    predicted_jets = []\n",
    "    inds = range(0, len(dataset), events_per_batch)\n",
    "    for ind in tqdm.tqdm(inds):\n",
    "        # Load the data for one batch\n",
    "        ds_elems = dataset[ind:ind+events_per_batch]\n",
    "        X_features = [torch.tensor(elem[\"X\"]).to(torch.float32) for elem in ds_elems]\n",
    "\n",
    "        # Batch the data into [batch_size, num_elems, num_features]\n",
    "        X_features_padded = pad_sequence(X_features, batch_first=True).to(device=device)\n",
    "        mask = X_features_padded[:, :, 0] != 0\n",
    "\n",
    "        # Run the model to predict jets\n",
    "        with torch.no_grad():\n",
    "            preds = model(X_features_padded, mask)\n",
    "        \n",
    "        # Unpack and process the predicted jets\n",
    "        preds_unpacked = unpack_predictions(preds)\n",
    "        predicted_jets.extend(preds_unpacked)\n",
    "    \n",
    "    return predicted_jets\n",
    "\n",
    "# Use the trained model to predict jets on the evaluation dataset\n",
    "predicted_jets = predict_jets(model, ds_eval, events_per_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72242e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch element 0:\n",
      "tensor([[ 1.0000,  3.6839, -0.2448,  ..., -0.5479, -1.0000,  0.0000],\n",
      "        [ 1.0000, 21.6268,  0.5386,  ...,  0.6570, -1.0000,  0.0000],\n",
      "        [ 1.0000,  3.5427, -0.3341,  ...,  1.2163, -1.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 2.0000,  1.8473, -0.5751,  ..., 25.4729, 19.2184, 18.4121],\n",
      "        [ 2.0000,  0.2759, -0.0767,  ...,  5.8995,  4.7749,  2.4985],\n",
      "        [ 2.0000,  0.8693, -0.6771,  ..., 30.6583, 31.6224, 28.4398]])\n",
      "Batch element 1:\n",
      "tensor([[ 1.0000e+00,  4.7078e+01,  2.7044e-01,  ...,  2.5455e-03,\n",
      "         -1.0000e+00,  0.0000e+00],\n",
      "        [ 1.0000e+00,  2.6926e+01, -6.2637e-02,  ..., -4.3941e-03,\n",
      "         -1.0000e+00,  0.0000e+00],\n",
      "        [ 1.0000e+00,  2.2181e+01, -4.3668e-01,  ..., -4.4045e-03,\n",
      "         -1.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 2.0000e+00,  1.4925e+00, -5.4835e-01,  ...,  1.4042e+01,\n",
      "          2.9127e+01,  2.4526e+01],\n",
      "        [ 2.0000e+00,  2.6105e+00, -9.0993e-01,  ...,  3.6439e+01,\n",
      "          4.4795e+01,  5.2597e+01],\n",
      "        [ 2.0000e+00,  4.7236e+00, -4.4167e-01,  ...,  4.3143e+02,\n",
      "          9.0859e+02,  4.6345e+02]])\n",
      "Batch element 2:\n",
      "tensor([[ 1.0000e+00,  4.7440e+01, -4.6837e-01,  ...,  9.4367e-03,\n",
      "         -1.0000e+00,  0.0000e+00],\n",
      "        [ 1.0000e+00,  5.8608e+00,  4.2135e-01,  ...,  7.2756e-01,\n",
      "         -1.0000e+00,  0.0000e+00],\n",
      "        [ 1.0000e+00,  8.0156e+00,  6.2740e-01,  ..., -4.4983e-02,\n",
      "         -1.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 2.0000e+00,  2.5003e+00,  3.8596e-01,  ...,  2.0533e+01,\n",
      "          3.3211e+01,  1.5568e+01],\n",
      "        [ 2.0000e+00,  3.7204e+00, -1.3960e-01,  ...,  2.8571e+01,\n",
      "          3.8986e+01,  2.9147e+01],\n",
      "        [ 2.0000e+00,  5.8965e+00,  1.4763e+00,  ...,  3.4362e+01,\n",
      "          4.0347e+01,  5.7029e+01]])\n",
      "Batch element 3:\n",
      "tensor([[ 1.0000e+00,  1.6499e+01,  1.2239e+00,  ..., -3.8539e-01,\n",
      "         -1.0000e+00,  0.0000e+00],\n",
      "        [ 1.0000e+00,  1.3658e+01, -8.3923e-01,  ..., -2.0083e-02,\n",
      "         -1.0000e+00,  0.0000e+00],\n",
      "        [ 1.0000e+00,  8.6668e+00, -8.4851e-01,  ..., -5.2766e-03,\n",
      "         -1.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 2.0000e+00,  7.0913e+00,  3.5451e-01,  ...,  1.1936e+02,\n",
      "          9.5356e+01,  7.3544e+01],\n",
      "        [ 2.0000e+00,  2.2577e+01, -1.0222e+00,  ...,  6.3504e+01,\n",
      "          1.2651e+02,  1.6315e+02],\n",
      "        [ 2.0000e+00,  3.3492e+00, -1.4719e+00,  ...,  2.3504e+01,\n",
      "          3.3259e+01,  4.3300e+01]])\n",
      "Batch element 4:\n",
      "tensor([[ 1.0000e+00,  3.0073e+01,  5.4297e-01,  ...,  3.3453e-03,\n",
      "         -1.0000e+00,  0.0000e+00],\n",
      "        [ 1.0000e+00,  7.2090e+01,  5.7680e-01,  ...,  4.1311e-03,\n",
      "         -1.0000e+00,  0.0000e+00],\n",
      "        [ 1.0000e+00,  1.9822e+01,  5.5428e-01,  ...,  1.3948e-02,\n",
      "         -1.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 2.0000e+00,  2.3865e+00, -5.3911e-01,  ...,  3.1241e+01,\n",
      "          2.9918e+01,  3.6526e+01],\n",
      "        [ 2.0000e+00,  4.1794e-01, -7.7783e-01,  ...,  2.9175e+01,\n",
      "          1.4493e+01,  2.5227e+01],\n",
      "        [ 2.0000e+00,  3.9538e+00,  2.9744e-01,  ...,  7.8572e+02,\n",
      "          7.1760e+02,  3.1284e+02]])\n",
      "Batch element 5:\n",
      "tensor([[ 1.0000e+00,  5.0189e+00,  1.4543e-01,  ..., -7.8657e-02,\n",
      "         -1.0000e+00,  0.0000e+00],\n",
      "        [ 1.0000e+00,  1.5650e+01, -1.5809e-01,  ...,  2.0444e-03,\n",
      "         -1.0000e+00,  0.0000e+00],\n",
      "        [ 1.0000e+00,  1.4491e+01, -2.2517e-01,  ...,  1.9644e-03,\n",
      "         -1.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 2.0000e+00,  1.0057e+01,  5.1718e-02,  ...,  1.0597e+02,\n",
      "          1.3967e+02,  4.7799e+01],\n",
      "        [ 2.0000e+00,  1.3383e+01, -8.7564e-01,  ...,  1.9178e+02,\n",
      "          2.6184e+02,  3.1303e+02],\n",
      "        [ 2.0000e+00,  1.1902e+01, -1.1764e+00,  ...,  5.4598e+01,\n",
      "          6.7547e+01,  1.1601e+02]])\n",
      "Batch element 6:\n",
      "tensor([[ 1.0000e+00,  5.9586e+00,  8.1385e-01,  ..., -4.5329e-02,\n",
      "         -1.0000e+00,  0.0000e+00],\n",
      "        [ 1.0000e+00,  9.5201e+00,  2.9853e-01,  ..., -4.3758e-03,\n",
      "         -1.0000e+00,  0.0000e+00],\n",
      "        [ 1.0000e+00,  8.3660e+00, -9.5316e-01,  ...,  1.8778e-01,\n",
      "         -1.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 2.0000e+00,  2.1902e+00,  1.9241e+00,  ...,  7.4655e+01,\n",
      "          4.5092e+01,  2.2886e+02],\n",
      "        [ 2.0000e+00,  3.5284e+00, -9.4639e-01,  ...,  2.7055e+02,\n",
      "          7.2742e+02,  8.5310e+02],\n",
      "        [ 2.0000e+00,  2.7129e+00, -1.3027e+00,  ...,  4.3068e+02,\n",
      "          3.1670e+01,  7.3781e+02]])\n",
      "Batch element 7:\n",
      "tensor([[ 1.0000e+00,  4.3336e+01, -7.8041e-01,  ..., -6.8078e-04,\n",
      "         -1.0000e+00,  0.0000e+00],\n",
      "        [ 1.0000e+00,  2.3925e+01, -5.4545e-01,  ..., -5.0876e-03,\n",
      "         -1.0000e+00,  0.0000e+00],\n",
      "        [ 1.0000e+00,  1.9395e+00, -3.7165e-01,  ..., -1.0744e+00,\n",
      "         -1.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 2.0000e+00,  1.1023e+00, -5.1763e-02,  ...,  1.2018e+01,\n",
      "          4.4677e+01,  1.2371e+01],\n",
      "        [ 2.0000e+00,  5.4299e+00, -1.1663e+00,  ...,  2.9026e+01,\n",
      "          3.5837e+01,  3.8506e+01],\n",
      "        [ 2.0000e+00,  4.1215e+00, -1.3018e+00,  ...,  1.2261e+01,\n",
      "          2.2377e+01,  3.5890e+01]])\n"
     ]
    }
   ],
   "source": [
    "for i, batch_elem in enumerate(X_features):\n",
    "    print(f\"Batch element {i}:\")\n",
    "    print(batch_elem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "518d8193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch element 0 shape: torch.Size([122, 17])\n",
      "Batch element 1 shape: torch.Size([85, 17])\n",
      "Batch element 2 shape: torch.Size([93, 17])\n",
      "Batch element 3 shape: torch.Size([111, 17])\n",
      "Batch element 4 shape: torch.Size([112, 17])\n",
      "Batch element 5 shape: torch.Size([140, 17])\n",
      "Batch element 6 shape: torch.Size([124, 17])\n",
      "Batch element 7 shape: torch.Size([87, 17])\n"
     ]
    }
   ],
   "source": [
    "for i, batch_elem in enumerate(X_features):\n",
    "    print(f\"Batch element {i} shape: {batch_elem.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faae1de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688b47f6-c922-4520-8ba4-5f783a7f4130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(3):\n",
    "#     t0 = time.time()\n",
    "#     for j in range(1):\n",
    "#         model(X_features_padded, X_features_padded[:, :, 0]!=0)\n",
    "#     t1 = time.time()\n",
    "#     print(t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f845b93f-decc-4d8f-be11-d54b7486e24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_quantized = torch.quantize_per_tensor((X_features_padded[:, :, 0]!=0).to(torch.float32), 1, 0, torch.quint8)\n",
    "# for i in range(3):\n",
    "#     t0 = time.time()\n",
    "#     for j in range(1):\n",
    "#         model_int8(X_features_padded, mask_quantized)\n",
    "#     t1 = time.time()\n",
    "#     print(t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c59978-a1ae-44f5-ba0e-ecce9d0d1e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_options = torch.onnx.ExportOptions(dynamic_shapes=True)\n",
    "mask = X_features_padded[:, :, 0]!=0\n",
    "\n",
    "onnx_program = torch.onnx.dynamo_export(model, X_features_padded, mask, export_options=export_options)\n",
    "onnx_program.save(\"mlpf_fp32_dynamo.onnx\")\n",
    "\n",
    "torch.onnx.export(model,                                            # model\n",
    "                  (X_features_padded, mask),                        # model input\n",
    "                  \"mlpf_fp32.onnx\",                                 # path\n",
    "                  export_params=True,                               # store the trained parameter weights inside the model file\n",
    "                  opset_version=17,                                 # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,                         # constant folding for optimization\n",
    "                  input_names = ['input'],                          # input names\n",
    "                  output_names = ['output'],                        # output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size', 1: 'num_elems'},\n",
    "                                'output' : {0 : 'batch_size', 1: 'num_elems'}},\n",
    "                  verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a3cb52-1120-4293-b07c-decaf6c0013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This does not work\n",
    "# onnx_program = torch.onnx.dynamo_export(model_int8, X_features_padded, mask_quantized, export_options=export_options)\n",
    "# onnx_program.save(\"mlpf_int8_dynamo.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfcff62-763d-48b0-99de-4485c160382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model_int8,                                       # model\n",
    "                  (X_features_padded, mask_quantized),              # model input\n",
    "                  \"mlpf_int8.onnx\",                                 # path\n",
    "                  export_params=True,                               # store the trained parameter weights inside the model file\n",
    "                  opset_version=17,                                 # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,                         # constant folding for optimization\n",
    "                  input_names = ['input'],                          # input names\n",
    "                  output_names = ['output'],                        # output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size', 1: 'num_elems'},\n",
    "                                'output' : {0 : 'batch_size', 1: 'num_elems'}},\n",
    "                  verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a36aea-6c1c-4070-a9c1-fa418fe70587",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -csh *.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df9966-0a7a-471a-ab2f-f050581164da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "ort_fp32 = ort.InferenceSession('mlpf_fp32.onnx')\n",
    "outputs = ort_fp32.run(None, {'input': X_features_padded.numpy()})\n",
    "preds_unpacked_ort_fp32 = unpack_predictions((torch.tensor(outputs[0]), torch.tensor(outputs[1])))\n",
    "\n",
    "px = preds_unpacked_ort_fp32[\"pt\"] * preds_unpacked_ort_fp32[\"cos_phi\"] * msk_true_particles\n",
    "py = preds_unpacked_ort_fp32[\"pt\"] * preds_unpacked_ort_fp32[\"sin_phi\"] * msk_true_particles\n",
    "pred_met_ort_fp32 = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)\n",
    "\n",
    "ort_int8 = ort.InferenceSession('mlpf_int8.onnx')\n",
    "outputs = ort_int8.run(None, {'input': X_features_padded.numpy()})\n",
    "preds_unpacked_ort_int8 = unpack_predictions((torch.tensor(outputs[0]), torch.tensor(outputs[1])))\n",
    "\n",
    "px = preds_unpacked_ort_int8[\"pt\"] * preds_unpacked_ort_int8[\"cos_phi\"] * msk_true_particles\n",
    "py = preds_unpacked_ort_int8[\"pt\"] * preds_unpacked_ort_int8[\"sin_phi\"] * msk_true_particles\n",
    "pred_met_ort_int8 = torch.sqrt(torch.sum(px, axis=-2) ** 2 + torch.sum(py, axis=-2) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a8dcba-ade3-4135-a23b-9d9eed6830c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    preds_unpacked[\"pt\"][targets_unpacked[\"cls_id\"]!=0],\n",
    "    preds_unpacked_ort_fp32[\"pt\"][targets_unpacked[\"cls_id\"]!=0],\n",
    "    marker=\".\", label=\"fp32\"\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    preds_unpacked_int8[\"pt\"][targets_unpacked[\"cls_id\"]!=0],\n",
    "    preds_unpacked_ort_int8[\"pt\"][targets_unpacked[\"cls_id\"]!=0],\n",
    "    marker=\".\", label=\"int8\"\n",
    ")\n",
    "plt.xlabel(\"pt, pytorch\")\n",
    "plt.ylabel(\"pt, ONNX\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6919306-9c6e-43ad-9f3a-fa32af4ee102",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    pred_met,\n",
    "    pred_met_ort_fp32,\n",
    "    marker=\".\", label=\"fp32\"\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    pred_met_int8,\n",
    "    pred_met_ort_int8,\n",
    "    marker=\".\", label=\"int8\"\n",
    ")\n",
    "plt.xlabel(\"MET, pytorch\")\n",
    "plt.ylabel(\"MET, ONNX\")\n",
    "plt.legend(loc=\"best\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
